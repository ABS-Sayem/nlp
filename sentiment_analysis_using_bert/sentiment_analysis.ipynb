{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Install Dependencies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Pytorch**: Go to `pytorch.org`, configure your settings (in my case, I choose- stable,windows,pip,python,cpu), copy the run command and run it in a python shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118     # gpu version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Other Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers requests beautifulsoup4 pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `transformers`: transformer is the library of HuggingFace, from where we will load the model \"bert-base-multilingual-uncased-sentiment\" in order to caculate sentiment score.\n",
    "> `bert-base-multilingual-uncased-sentiment`: This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5). [`For more details click here`](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment)\n",
    "* `requests`: is for sending request to a website for extracting data from the website\n",
    "* `beautifulsoup`: to fetch the required data from the site\n",
    "* `pandas`: to represent the data to dataframe for better outlooking\n",
    "* `numpy`: to convert data into numeric for further use in model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Instantiate Model**\n",
    "\n",
    "We will now instantiate the tokenizer and the model and load its weights. For the first time of loading- it will download the model, about 669MB, that will take a few minutes depending your internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 39.0/39.0 [00:00<00:00, 2.05kB/s]\n",
      "C:\\Users\\Abs_Sayem\\AppData\\Roaming\\Python\\Python39\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Abs_Sayem\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 953/953 [00:00<00:00, 95.3kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 872k/872k [00:00<00:00, 981kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 16.0kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 669M/669M [01:15<00:00, 8.90MB/s] \n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "model     = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Encode and Calculate Sentiment Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encode** \n",
    "\n",
    "`Encode` converts the sentence tokens into numeric representation to fit the sentence to the model as models only receive numeric data. And then the model will calculate the sentiment score according to its previous knowledge (as it is a pretrained model). We can also `Decode` the converted tensor to its original sentence form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 10191, 12296, 10497, 10154, 10197, 10114, 10525,   117, 13362,\n",
       "           106,   102]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"He could not do it to me, never!\", return_tensors='pt')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`return_tensors='pt'` is a parameter used in the Hugging Face transformer library. It is used to specify that the tokenizer should return PyTorch tensors instead of a list of Python integers.\n",
    "\n",
    "The sentence has 10 tokens and the tokenizer encoded them into numerical values. The first and last tensor value is for start and end tokens respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check `Decode`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] he could not do it to me, never! [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Sentiment Score**\n",
    "\n",
    "Before that, we have to understand the scoring system to the model. The model scores a sentence from (1 to 5). `1=negetive` and `5=positive` and the values between (1 and 5) refers the intensity of negetivity to positivity. We can consider `3=nutral`. It rates a sentence 5 different values. The highest value is considered as the sentiment score.\n",
    "\n",
    "`Why 5 Scores?` It basically use a softmax classifier to score all the 5 classes. Softmax calculates the probability of classes one by one depending on the conditional probability of other classes.\n",
    "\n",
    "`Conditional Probability`: P(A/B) = Probability of A given that B is already happened. For example: P(fiver/sick) = Probability of fiver given that he is sick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.6982,  1.5826, -0.0619, -1.9971, -1.7371]],\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_scores = model(tokens)\n",
    "probability_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Here, we only need the logits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.6982,  1.5826, -0.0619, -1.9971, -1.7371]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(probability_scores.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`To see which rating has the highest value:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(int(torch.argmax(probability_scores.logits) + 1))\n",
    "# Here,\n",
    "# `torch.argmax()` - returns the index of highest score. In this case it is 0 (index=0, since the indexing of any datastructures starts from 0)\n",
    "# `+ 1` - makes the index start from 1 (since we have the ratings from 1 to 5)\n",
    "# `int` - int converts the tensor to just numeric value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`So, it is a negetive sentiment`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check other Sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilitity Scores: tensor([[-2.0370, -1.4861,  0.1390,  1.3571,  1.5039]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Ratings: 5\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I just loved the documentation\"\n",
    "tokens = tokenizer.encode(sentence, return_tensors='pt')\n",
    "probability_scores = model(tokens)\n",
    "print(f\"Probabilitity Scores: {probability_scores.logits}\")\n",
    "print(f\"Ratings: {int(torch.argmax(probability_scores.logits)+1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilitity Scores: tensor([[-0.1769,  0.8476,  1.2969,  0.1629, -1.7914]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Ratings: 3\n"
     ]
    }
   ],
   "source": [
    "sentence = \"There is a possibility but their attitude might destroy it.\"\n",
    "tokens = tokenizer.encode(sentence, return_tensors='pt')\n",
    "probability_scores = model(tokens)\n",
    "print(f\"Probabilitity Scores: {probability_scores.logits}\")\n",
    "print(f\"Ratings: {int(torch.argmax(probability_scores.logits)+1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Collect Data form Online**\n",
    "Lets collect some reviews from any sites. We will classify these reviews using the model we use earlier. To collect the reviews we need bunch of things, like-\n",
    "* `requests: `to sent a request to the site\n",
    "* `beautifulsoup: `to fetch the informations from the requested site\n",
    "* `regex: `to work with the collected texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www.yelp.com/biz/social-brew-cafe-pyrmont')\n",
    "# It sents a request to the site and get all the contents\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "# Parsing HTML content using Beautiful Soup library. 'r.text' extract the contents as text\n",
    "regex = re.compile('.*comment*.')\n",
    "# It contains the class name 'comment' from where we need the text. We will further sent the class name to find all of them\n",
    "results = soup.find_all('p', {'class': regex})\n",
    "# Here, we pass the class name as regex and find all the classes from the paragraph tag 'p'\n",
    "reviews = [result.text for result in results]\n",
    "# Here, we make a list of texts containing all the result found from paragraph tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Some of the best Milkshakes me and my daughter ever tasted. MMMMMM HMMMMMMMM.',\n",
       " \"Six of us met here for breakfast before our walk to Manly. We were enjoying visiting with each other so much that I apologize for not taking any photos. We all enjoyed our food, as well as our coffee and tea drinks.We were greeted immediately by a friendly server asking if we would like to sit inside or out. We said we would like inside, but weren't exactly sure how many were joining us yet- at least 4. We were told this was no problem, the more the merrier. A few minutes later when 4 more joined our party and we explained to the server we had 6, he just quickly switched our table. I really enjoyed my serenity tea, just what I needed after a long flight in from Sfo that morning. Everyone else were more interested in the lattes for expresso drinks. All said they were hot and delicious. 2 of us ordered the avo on toast. So yummy with the beetroot... I will start adding this to mine now at home, and have fond memories for my trip to Sydney. 2 friends ordered the salmon Benedict- saying it was delicious, and their go to every time they come here. 2 friends had a breakfast sandwich- I'm not sure of the name. It did look delicious. Adorable cafe, friendly staff, clean restroomsVery popular with the locals. I plan to come back the next time I'm in Sydney\",\n",
       " 'Great place with delicious food and friendly staff. It is small but has outdoor seating and a relaxed ambiance. Perfect place to enjoy a cup of coffee. I am visiting Sydney for the first time but this place seems like is a local favorite.',\n",
       " 'Great food amazing coffee and tea. Short walk from the harbor. Staff was very friendly',\n",
       " \"It was ok. Had coffee with my friends. I'm new in the area, still need to discover new places.\",\n",
       " \"Ricotta hot cakes! These were so yummy. I ate them pretty fast and didn't share with anyone because they were that good ;). I ordered a green smoothie to balance it all out. Smoothie was a nice way to end my brekkie at this restaurant. Others with me ordered the salmon Benedict and the smoked salmon flatbread. They were all delicious and all plates were empty. Cheers!\",\n",
       " 'Great staff and food. \\xa0Must try is the pan fried Gnocchi! \\xa0The staff were really friendly and the coffee was good as well',\n",
       " \"We came for brunch twice in our week-long visit to Sydney. Everything on the menu not only sounds delicious, but is really tasty. It really gave us a sour taste of how bad breaky is in America with what's so readily available in Sydney! \\xa0Both days we went were Saturdays and there was a bit of a wait to be seated, the cafe is extremely busy for both dine-in and take-away. Service is fairly quick and servers are all friendly. The location is in Surrey Hills a couple blocks away from the bustling touristy Darling Harbor.The green smoothie is very tasty and refreshing. We tried the smoked salmon salad, the soft shell crab tacos, ricotta hotcakes, and the breaky sandwich. All were delicious, well seasoned, and a solid amount of food for the price. A definite recommend for anyone's trip into Sydney!\",\n",
       " 'I came to Social brew cafe for brunch while exploring the city and on my way to the aquarium. I sat outside. The service was great and the food was good too!I ordered smoked salmon, truffle fries, black coffee and beer.',\n",
       " \"It was ok. The coffee wasn't the best but it was fine. The relish on the breakfast roll was yum which did make it sing. So perhaps I just got a bad coffee but the food was good on my visit.\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load Data into DataFrame**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Calculate Sentiment Score**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
