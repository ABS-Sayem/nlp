{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prerequisite libraries\n",
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LIKE_EMAIL\":True}]\n",
    "matcher.add(\"EMAIL_ADDRESS\",[pattern])\n",
    "doc = nlp(\"This is an email address: abs.alchemy20@gmail.com\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16571425990740197027, 6, 7)]\n"
     ]
    }
   ],
   "source": [
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMAIL_ADDRESS\n"
     ]
    }
   ],
   "source": [
    "print(nlp.vocab[matches[0][0]].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes Taken by Matcher\n",
    "*ORTH-The exact verbatim of a token(str)*\n",
    "\n",
    "*TEXT-The exact verbatim of a token(str)*\n",
    "\n",
    "*LOWER-The lowercase form of the token text(str)*\n",
    "\n",
    "*LENGTH-The length of the token text(int)*\n",
    "\n",
    "*IS_ALPHA*\n",
    "\n",
    "*IS_ASCII*\n",
    "\n",
    "*IS_DIGIT*\n",
    "\n",
    "*IS_LOWER*\n",
    "\n",
    "*IS_UPPER*\n",
    "\n",
    "*IS_TITLE*\n",
    "\n",
    "*IS_PUNCT*\n",
    "\n",
    "*IS_SPACE*\n",
    "\n",
    "*IS_STOP*\n",
    "\n",
    "*IS_SENT_START*\n",
    "\n",
    "*LIKE_NUM*\n",
    "\n",
    "*LIKE_URL*\n",
    "\n",
    "*LIKE_EMAIL*\n",
    "\n",
    "*SPACY*\n",
    "\n",
    "*POS*\n",
    "\n",
    "*TAG*\n",
    "\n",
    "*MORPH*\n",
    "\n",
    "*DEP*\n",
    "\n",
    "*LEMMA*\n",
    "\n",
    "*SHAPE*\n",
    "\n",
    "*ENT_TYPE*\n",
    "\n",
    "*_-Customer extension attributes(Dict[str,Any])*\n",
    "\n",
    "*OP*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applied Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/nlp_wiki.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print text\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grabbing all Proper Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "(3232560085755078826, 11, 12) NLP\n",
      "(3232560085755078826, 21, 22) NLP\n",
      "(3232560085755078826, 23, 24) corpus\n",
      "(3232560085755078826, 62, 63) corpora\n",
      "(3232560085755078826, 73, 74) Corpora\n",
      "(3232560085755078826, 83, 84) Biblical\n",
      "(3232560085755078826, 89, 90) Corpora\n",
      "(3232560085755078826, 143, 144) Wikipedia\n",
      "(3232560085755078826, 201, 202) English\n",
      "(3232560085755078826, 202, 203) Wikipedia\n"
     ]
    }
   ],
   "source": [
    "# Grabbing proper nouns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\":\"PROPN\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern])\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(match,doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Improving with Multi-Word Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "(3232560085755078826, 11, 12) NLP\n",
      "(3232560085755078826, 21, 22) NLP\n",
      "(3232560085755078826, 23, 24) corpus\n",
      "(3232560085755078826, 62, 63) corpora\n",
      "(3232560085755078826, 73, 74) Corpora\n",
      "(3232560085755078826, 83, 84) Biblical\n",
      "(3232560085755078826, 89, 90) Corpora\n",
      "(3232560085755078826, 143, 144) Wikipedia\n",
      "(3232560085755078826, 201, 202) English\n",
      "(3232560085755078826, 201, 203) English Wikipedia\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\":\"PROPN\", \"OP\":\"+\"}]\n",
    "matcher.add(\"PROPER_NOUNS\",[pattern])\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])    # Didn't understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Greedy Keyword Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "(3232560085755078826, 201, 203) English Wikipedia\n",
      "(3232560085755078826, 307, 309) Wikipedia Dump\n",
      "(3232560085755078826, 341, 343) English Wikipedia\n",
      "(3232560085755078826, 11, 12) NLP\n",
      "(3232560085755078826, 21, 22) NLP\n",
      "(3232560085755078826, 23, 24) corpus\n",
      "(3232560085755078826, 62, 63) corpora\n",
      "(3232560085755078826, 73, 74) Corpora\n",
      "(3232560085755078826, 83, 84) Biblical\n",
      "(3232560085755078826, 89, 90) Corpora\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\":\"PROPN\", \"OP\":\"+\"}]\n",
    "matcher.add(\"PROPER_NOUNS\",[pattern], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sorting it to Apperance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "(3232560085755078826, 11, 12) NLP\n",
      "(3232560085755078826, 21, 22) NLP\n",
      "(3232560085755078826, 23, 24) corpus\n",
      "(3232560085755078826, 62, 63) corpora\n",
      "(3232560085755078826, 73, 74) Corpora\n",
      "(3232560085755078826, 83, 84) Biblical\n",
      "(3232560085755078826, 89, 90) Corpora\n",
      "(3232560085755078826, 143, 144) Wikipedia\n",
      "(3232560085755078826, 201, 203) English Wikipedia\n",
      "(3232560085755078826, 214, 215) Install\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\":\"PROPN\", \"OP\":\"+\"}]\n",
    "matcher.add(\"PROPER_NOUNS\",[pattern], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key=lambda x: x[1])\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding in Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(3232560085755078826, 214, 216) Install gensim\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\":\"PROPN\", \"OP\":\"+\"}, {\"POS\":\"VERB\"}]\n",
    "matcher.add(\"PROPER_NOUNS\",[pattern], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key=lambda x: x[1])\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Quotes and Speakers"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88bd64abd146d54e091ff1398670d31add9e50ee4691cdc8ffb9431c01087a21"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
