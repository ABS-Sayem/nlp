{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize\n",
    "*Here, we will talk about word vectors or word embeddings. For this we will working with a larger English medium model- 'en_core_web_md'.* \n",
    "\n",
    "*Lets import spacy and download the model--*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load the model and make a doc object*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "with open(\"dataset/nlp_wiki.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Make a doc object\n",
    "doc = nlp(text)\n",
    "sentence1 = list(doc.sents)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the first things required for natural language processing (NLP) tasks is a corpus.\n"
     ]
    }
   ],
   "source": [
    "print(sentence1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Vectors\n",
    "*Word vectors are numerical representations of words in multidimensional space through matrices. The purpose of word vector is to convert a word into a number to make computer system to understand the word.*\n",
    "\n",
    "*The idea is take all the words in a corpus and convert them into a single unique number. These numbers then stored in a dictionary. This is known as bag of words.*\n",
    "\n",
    "*This approach to represent words numerically. This allows computer to understand words numerically, however, doesn't allow computer to understand its meaning.*\n",
    "\n",
    "*For Example, lets take two sentences-\n",
    "A) Nee loves to eat chocolate and B) Nee likes to eat chocolate. These sentences represented numerically as- A) 1 2 3 4 5 and B) 1 6 3 4 5*\n",
    "\n",
    "*Here, as we can see, both sentences are nearly identical. The only difference is the degreeto which Nee appericiating eating chocolate. If we examine the numerical representation, we see that they are quite close  but their semantical meaning is impossible to know for certain. How similar is 2 to 6?? 6 could represents \"hates\" like it represent \"like\" here.*\n",
    "\n",
    "*This is where word vectors come in.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why use Word Vectors?\n",
    "*Word vectyors takes one dimensional bag-of-words matrix and gives them multidimentional meaning by representing them in higher dimentional space. [This is achived via Python libraries, such a s Gensim.]*\n",
    "\n",
    "*The goal of word vectors is to achieve numerical understanding of language. Lets consider the example above- how do we get a computer to understand 2 and 6 are synonyms or mean something similar?*\n",
    "\n",
    "*One possible ways would be- give computer a synonym dictionary. It can look up the synonyms and know what the words mean. Lets explore that option and see why it cannot possibly work.*\n",
    "\n",
    "*Python has a library named \"PyDictionary\". Lets try it...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nee has no Synonyms in the API\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6640/2211388331.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0msyns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynonym\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{word}:{syns[0:5]}\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from PyDictionary import PyDictionary\n",
    "\n",
    "dictionary = PyDictionary\n",
    "sentence = \"Nee loves to eat Chocolate\"\n",
    "\n",
    "words = sentence.split()\n",
    "#print(words)\n",
    "for word in words:\n",
    "    syns = dictionary.synonym(word)\n",
    "    print(f\"{word}:{syns[0:5]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*However it has some potential to work but not entirely relliable and such a list would be computationally expensive. For bothe the reasons- word vector is prefered.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How Word Vector Looks Like?\n",
    "\n",
    "*Word Vectors has a preset number of dimensions. Models take into account word frequency along with words across a corpus and the appearance of the other words in similar contexts. This allows for the computer to determine the syntactical similarity of words numerically. Also, it then needs to represent these relationships numerically.*\n",
    "\n",
    "*To represent these more concisely, models flatten a matrix (or matrix of matrices) of floats. The number of dimensions represent the number of floats in the matrix.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.4071e-02,  1.1110e-01, -1.4557e-01, -2.4294e-02,  3.8110e-01,\n",
       "       -1.4389e-01, -1.7998e-01, -3.1079e-01, -7.9690e-03,  2.6538e+00,\n",
       "       -1.2772e-01,  2.3885e-02,  7.1284e-02, -1.4264e-01,  1.0939e-01,\n",
       "       -1.0667e-01, -3.8178e-02,  1.1853e+00,  5.2559e-02, -1.7181e-01,\n",
       "       -1.8629e-01, -1.6533e-02, -8.4008e-02,  1.4542e-01,  1.6059e-01,\n",
       "       -6.9163e-02, -7.6812e-02, -2.0658e-01,  1.6025e-01,  2.1405e-01,\n",
       "        5.9209e-02,  4.7891e-01,  8.3374e-02,  1.9994e-01,  9.6225e-02,\n",
       "       -1.0033e-01,  4.2577e-02, -9.3587e-02, -1.3389e-01, -3.2704e-01,\n",
       "        2.3650e-02,  3.4064e-01, -7.5976e-02, -1.0150e-01,  1.2431e-01,\n",
       "       -5.5954e-02, -2.5284e-01, -1.8520e-02,  4.6912e-02, -8.4774e-02,\n",
       "       -1.5884e-01,  2.3751e-01,  7.6109e-02,  7.1753e-02,  3.1405e-02,\n",
       "        3.2656e-02,  1.1271e-01,  2.7839e-01,  3.5233e-01, -1.0844e-01,\n",
       "        7.0183e-02, -4.6891e-02, -1.8825e-01,  4.0518e-01,  2.1180e-01,\n",
       "       -1.4376e-01, -4.8075e-03,  1.3877e-01, -9.1521e-02,  2.5194e-01,\n",
       "        1.7998e-01, -1.5900e-01,  1.4753e-01,  1.1866e-01, -1.5447e-01,\n",
       "        1.0969e-01,  5.7966e-02, -2.5862e-01, -2.3764e-02,  1.9064e-01,\n",
       "       -3.4365e-02,  9.0880e-02, -2.9016e-01, -1.3003e-01,  2.8666e-02,\n",
       "       -2.0623e-01, -2.8586e-01, -2.2934e-01,  1.5382e-01, -2.0373e-01,\n",
       "       -2.5350e-01,  2.0703e-01, -8.6000e-02, -8.4409e-02, -1.2904e-01,\n",
       "        5.9437e-02,  8.5175e-02, -4.1774e-02,  8.6070e-02,  7.8259e-02,\n",
       "       -6.6940e-02, -2.2128e-02, -9.7204e-03, -2.4601e-02,  7.7635e-03,\n",
       "       -9.5970e-01,  4.4450e-02,  6.1008e-02, -1.2404e-01,  7.5514e-02,\n",
       "        3.7011e-01, -1.7903e-01, -7.3509e-02,  1.7854e-01, -1.0738e-01,\n",
       "        5.5760e-02, -3.5650e-01,  4.1038e-01, -1.7730e-01,  1.0413e-01,\n",
       "        7.0496e-02, -4.0687e-01,  2.6068e-01, -2.3406e-02, -4.1537e-02,\n",
       "        9.0448e-02, -3.3967e-01, -3.6952e-01, -1.9355e-02,  1.1835e-01,\n",
       "       -1.5966e-01,  1.1182e-02,  2.3317e-02,  2.3929e-02,  7.4658e-02,\n",
       "       -6.6614e-03,  1.3711e-01,  3.1078e-02,  1.8448e-01, -8.3872e-02,\n",
       "       -1.6817e+00,  2.2500e-01,  1.7538e-01,  3.1784e-01, -5.2595e-02,\n",
       "       -8.4312e-02, -1.3629e-01, -7.4823e-02,  3.9641e-02, -1.3645e-01,\n",
       "       -1.4008e-01,  9.5744e-02,  4.6472e-03,  8.6625e-02,  5.2288e-02,\n",
       "       -5.7616e-03, -2.0944e-01,  7.4924e-02, -1.8494e-01, -1.2465e-01,\n",
       "       -6.0265e-02,  3.1429e-02,  5.8216e-02, -1.0552e-01,  1.7755e-02,\n",
       "       -2.4728e-01, -3.7765e-02, -1.6246e-04,  1.1723e-01,  9.9054e-02,\n",
       "       -6.2556e-03, -2.8509e-01,  1.7956e-01, -1.3748e-01, -4.2976e-01,\n",
       "        4.4895e-02, -1.7917e-01, -8.3175e-02, -7.2140e-02, -1.1763e-01,\n",
       "        1.7213e-01,  2.6727e-02, -1.3591e-01, -2.0115e-01, -2.3381e-01,\n",
       "        2.2689e-02, -8.4114e-02, -2.1102e-01, -1.1765e-01, -2.3708e-02,\n",
       "       -1.6021e-01,  6.1171e-02, -6.8588e-02,  2.2241e-01,  3.1944e-01,\n",
       "       -6.7243e-02, -2.4478e-01, -5.0601e-02, -6.8996e-02, -9.5617e-02,\n",
       "       -1.7759e-01, -1.6193e-01, -1.9167e-01, -5.2516e-02,  2.7398e-01,\n",
       "        1.2593e-02, -1.4691e-02, -1.5302e-01,  2.5296e-02,  2.4122e-01,\n",
       "       -1.5280e-01,  8.7798e-02, -3.7799e-01, -3.7299e-01, -1.1340e-01,\n",
       "        1.1766e-01, -1.6729e-01,  5.6089e-02, -1.6158e-01, -4.6461e-02,\n",
       "       -1.9236e-01, -1.0562e-01,  1.3405e-01,  2.5040e-01,  1.6388e-01,\n",
       "       -2.9802e-01,  1.9515e-01,  3.1131e-01, -2.6026e-01, -2.7901e-02,\n",
       "       -1.4921e-01, -3.1797e-03,  9.7112e-02,  2.7519e-01, -2.9357e-01,\n",
       "       -2.9010e-01,  2.3958e-01, -2.0233e-02, -2.2009e-01,  3.5379e-01,\n",
       "       -1.3837e-02,  3.4645e-01,  1.5853e-01,  4.3607e-01,  2.3632e-01,\n",
       "       -3.3509e-01,  9.2342e-02, -3.0143e-02, -2.1415e-01,  1.0454e-01,\n",
       "        1.6400e-01, -1.0189e-01,  2.4665e-02,  9.4526e-03,  5.1336e-03,\n",
       "        1.7175e-01,  2.2745e-01,  3.4427e-02,  6.2545e-02,  1.0700e-01,\n",
       "        2.8015e-01, -9.9735e-02, -3.2008e-01, -9.4804e-02, -9.8628e-02,\n",
       "        9.1675e-03,  2.8218e-01,  1.9045e-01,  3.9676e-01, -9.9515e-02,\n",
       "        1.4647e-01, -1.2100e-01, -6.9490e-02, -4.4131e-01, -3.6843e-03,\n",
       "        2.6867e-01, -1.1051e-02,  1.3741e-01,  3.4037e-01,  1.2220e-01,\n",
       "        2.5341e-01,  4.6069e-02, -1.7136e-01, -1.5243e-01, -6.5814e-03,\n",
       "       -5.5056e-02,  2.2568e-01, -1.5371e-01,  1.2683e-01,  1.1332e-01,\n",
       "       -2.2479e-01, -3.4352e-02, -8.2647e-02, -1.3485e-01, -1.1371e-01,\n",
       "       -7.2454e-02, -7.3029e-02, -2.7098e-01,  4.4033e-02,  3.7690e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1[0].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Once a word vector is trained, we can matches similarity quickly and reliably. Lets explore some vectors from our model. Specifically, try and find the words most closely related to the word \"nlp\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Langauge', 'language', 'MULTILINGUAL', 'FLASHCARDS', 'UTTERANCE', 'GRAMMER', 'grammar', 'Typological', 'PATOIS', 'LOCALIZATION']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "my_word = \"language\"\n",
    "most_similar_word  = nlp.vocab.vectors.most_similar(\n",
    "    np.asarray([nlp.vocab.vectors[nlp.vocab.strings[my_word]]]), n=10)\n",
    "words = [nlp.vocab.strings[w] for w in most_similar_word[0][0]]\n",
    "distances = most_similar_word[2]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc Similarity\n",
    "*In spaCy we can do the same thing at the document level. Through word vectors we can calculate the similarity between two documents.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like spicy fries and Tea. <=> Fast food tastes good. \n",
      "Similarity: 0.8149152232462087\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I like spicy fries and Tea.\")\n",
    "doc2 = nlp(\"Fast food tastes good.\")\n",
    "\n",
    "# Similarity of two documents\n",
    "print(doc1,\"<=>\",doc2,\"\\nSimilarity:\", doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Similarity\n",
    "*We can also calculate the similarity between two given words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "french_fries <=> burgers\n",
      "Similarity: 0.8149152232462087\n"
     ]
    }
   ],
   "source": [
    "french_fries = doc1[2:4]\n",
    "burgers = doc1[5]\n",
    "print(\"french_fries <=> burgers\")\n",
    "print(\"Similarity:\", doc1.similarity(doc2))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88bd64abd146d54e091ff1398670d31add9e50ee4691cdc8ffb9431c01087a21"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
