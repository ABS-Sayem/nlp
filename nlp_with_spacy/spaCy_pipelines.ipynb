{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here, we will learn about various pipelines in spaCy. spaCy offers both heuristic(rule-based) and machine learning nlp solutions. These solutions are activated by pipes.*\n",
    "\n",
    "*Here, we will learn about pipes and pipelines generally and later we will explore how we can create custom pipes and ass them to a spaCy pipeline.*\n",
    "\n",
    "*Before jump, lets import spaCy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Pipes available from spaCy(Components and Factories)\n",
    "*A pipeline is a sequence of pipes, or actors on data, that make alterations to the data or extract information from it. In some cases, later pipes requires the output from earlier pipes.*\n",
    "\n",
    "> *Sample SpaCy Pipeline for NER: Input_Sentence => Entity_Ruler => Entity_Linker => Output(Sentence with Entities Annotated)*\n",
    "\n",
    "*In the above pipeline, for a sentence input, two pipes are activated on this-*\n",
    "*> 1) Entity Ruler- A rule-based named entity recognizer which finds entities and*\n",
    "*> 2) Entity Linker- identifies what entity is to perform toponym resolution.*\n",
    "\n",
    "*The sentence is then outputted with the sentence and entities annotated.*\n",
    "\n",
    "*At this point, we could use the \"doc.ents\" feature to find the entities in our sentence. To vectorize the input sentence a Tok2Vec input layer can be used. This will allow machine learning pipes to make predictions.*\n",
    "\n",
    "*The complete list of the AttributeRuler pipes available in spaCy and the Matchers---*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attributes Rulers\n",
    "*> Dependency Parser*\n",
    "*> EntityLinker*\n",
    "*> EntityRecognizer*\n",
    "*> EntityRuler*\n",
    "*> Lemmatizer*\n",
    "*> Morphology*\n",
    "*> SentenceRecognizer*\n",
    "*> Sentencizer*\n",
    "*> SpanCategorizer*\n",
    "*> Tagger*\n",
    "*> TextCategorizer*\n",
    "*> Tok2Vec*\n",
    "*> Tokenizer*\n",
    "*> TrainablePipe*\n",
    "*> Transformer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matchers\n",
    "*> DependencyMatcher*\n",
    "*> Matcher*\n",
    "*> PhraseMatcher*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Add Pipes\n",
    "*Generally, we use an off-the-shelf spaCy model, however, sometimes this won't fill the needs or perform a specific task very slowly on big data.*\n",
    "\n",
    "*For instance, suppose we have a document having 1 million sentences. Even if we use a small english model, our model would take hours to process those 1 million sentences. Because, each pipe in a pipeline will be activated (unless specified) and each pipe from Dependency Parser to NER will be performed on the data. This is a serious waste of computational resources and time.*\n",
    "\n",
    "*By creating a blank model and simply adding a Senticizer to it can reduce this time to merely minutes. To demonstrate this process, lets first create a blank model...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank model\n",
    "nlp_blank = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here, we use spacy.blank rather than spacy.load. To create an empty model, we simply pass the two letter for a language. Here, \"en\" for English.*\n",
    "\n",
    "*Now, Lets add a pipe to it. We can simply add a sentencizer to it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x1c0ddc12140>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_blank.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now, lets check using a small model and the blank model on a dataset.*\n",
    "\n",
    "*First, we will apply the blank model(created above) and then a small model to compare the computation time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape data from web\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "source = requests.get(\"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\")\n",
    "soup = BeautifulSoup(source.content).text.replace(\"-\\n\",\"\").replace(\"\\n\",\" \")\n",
    "nlp_blank.max_length = 5278439"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now apply the blank model created above...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94133\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_blank = nlp_blank(soup)\n",
    "print(len(list(doc_blank.sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now load a small model and apply the model to the dataset...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a small model\n",
    "nlp_small = spacy.load(\"en_core_web_sm\")\n",
    "nlp_small.max_length = 5278439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98262\n",
      "Wall time: 1h 26min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_small = nlp_small(soup)\n",
    "print(len(list(doc_small.sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examing a Pipeline\n",
    "*In spaCy, we have a few different ways to study a pipeline. If we want to this in a script, we can do the following command.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': []},\n",
       " 'attrs': {'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []}}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze the small model imported above\n",
    "nlp_small.analyze_pipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note the dictionary staructure- it tells us what is inside the pipeline and its order. Each key after \"summary\" is a pipe.*\n",
    "\n",
    "*> \"assigns:\" corresponds a vlue of what that particular pipe assigns to the token and doc as it passes through the pipeline.*\n",
    "\n",
    "*> In some cases, there wiil be a key \"scores:\" indicates how the machine learning model was evaluated.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_blank.analyze_pipes()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88bd64abd146d54e091ff1398670d31add9e50ee4691cdc8ffb9431c01087a21"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
