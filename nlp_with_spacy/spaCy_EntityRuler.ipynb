{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Concepts\n",
    "*> pipe*\n",
    "\n",
    "*> factory*\n",
    "\n",
    "*> EntityRuler*\n",
    "\n",
    "*> PhraseMatcher*\n",
    "\n",
    "*> Matcher*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to Spacy's ****EntityRuler****\n",
    "*spaCy offers a few different methods for performing rules-based NER. One of them is EntityRuler.*\n",
    "\n",
    "*****EntityRuler**** is a spacy factory that allows to create a set of patterns with corresponding labels. A ****factory**** is a set of classes and functions preloaded in spaCy that perform set tasks. In the case of EntityRuler, user can create an EntityRuler, give it a set of instructions, and use this instructions to find and label entities. Once a EntityRuler is created, user can then add it to the spaCy pipeline as a new pipe.*\n",
    "\n",
    "*A ****pipe**** is an individual component of a pipeline. A ****pipeline-**** take input data, perform some sort of operations on that input data, and then output those operations either as a new data or extracted metadata. In spaCy, there are a few different pipes that perform different tasks. The ****tokenizer-**** tokenizes the text into individual tokens; the ****parser-**** parses the text and the ****NER-**** identifies entities and labels them accordingly.*\n",
    "\n",
    "*It is important to remember that pipelines are sequential. Sometimes the sequences are essential because later pipes might be depend on earlier pipes and sometimes not essential. It should keep in mind while creating custom spaCy model.*\n",
    "\n",
    "*The full documentation of spaCy EntityRuler can be found here: https://spacy.io/api/entityruler*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstration of ******EntityRuler******\n",
    "*Here, we eill introduce a new pipe into spaCy's off-the-shelf small English model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dhaka University ORG\n",
      "Dhaka GPE\n",
      "Bangladesh GPE\n",
      "Chittagong PERSON\n"
     ]
    }
   ],
   "source": [
    "# Import the requisite library\n",
    "import spacy\n",
    "\n",
    "# Build upon the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"The Dhaka University is situated in Dhaka, Bangladesh. And Chittagong is called the port city.\"\n",
    "\n",
    "# Create the Doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Often times the domains in which we wish to deploy models, off-the-shelf models will fail because the have not been trained on domain-specific texts. We can resolve this- either via EntityRuler or via training a new model.*\n",
    "\n",
    "*Now, let fix the problem by giving the model instructions to correctly identify Chittagong. For simplicity, we will use spaCy's GPE label.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dhaka University ORG\n",
      "Dhaka GPE\n",
      "Bangladesh GPE\n",
      "Chittagong PERSON\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries\n",
    "import spacy\n",
    "\n",
    "# Build the spaCy Small Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample Text\n",
    "text = \"The Dhaka University is situated in Dhaka, Bangladesh. And Chittagong is called the port city.\"\n",
    "\n",
    "# Create the EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# List of Entities and Patterns\n",
    "patterns = [{\"label\":\"LOC\", \"pattern\":\"Chittagong\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract Entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you executed the code above and found that you had athe same output, then you did everything correctly. This method has failed. Why?- The answer comes back to the concept of pipelines. We created and added the EntityRuler to the spaCy model's pipelines, but by default, spaCy add's new pipe to the end of the pipeline.*\n",
    "\n",
    "*In order to visualize the pipeline, let's use spaCy's ******analize_pipes()*******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'entity_ruler': {'assigns': ['doc.ents', 'token.ent_type', 'token.ent_iob'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': [],\n",
       "  'entity_ruler': []},\n",
       " 'attrs': {'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This can be a bit difficult to read at first, but what it shows us is the order in which our pipes are set up and a few other key pieces of information about each pipe. If we locate \"ner\", we notice that \"entity_ruler\" sits behind it.*\n",
    "\n",
    "*In order for our EntityRuler to have primacy, we have to assign it to after the \"ner\" pipe.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dhaka University ORG\n",
      "Dhaka GPE\n",
      "Bangladesh GPE\n",
      "Chittagong PERSON\n"
     ]
    }
   ],
   "source": [
    "# Build upon the spaCy Small Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"The Dhaka University is situated in Dhaka, Bangladesh. And Chittagong is called the port city.\"\n",
    "\n",
    "# Create the EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", after=\"ner\")\n",
    "\n",
    "# List of Entities and Patterns\n",
    "patterns = [{\"label\":\"GPE\", \"pattern\":\"Chittagong\"}]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(text)\n",
    "# Extract Entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "'''\n",
    "xxx\n",
    "> Didn't change the Entity Level of Chittagong\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'entity_ruler': {'assigns': ['doc.ents', 'token.ent_type', 'token.ent_iob'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': [],\n",
       "  'entity_ruler': []},\n",
       " 'attrs': {'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complex Rules and Varience to the EntityRuler\n",
    "*In some instances, labels may have a set type of varience that follow a distinct pattern or sets of patterns. One such example is phone number. In Bangladesh, phone numbers have few forms. The standard formal method is (xxx)xxxxx-xxxxxx.*\n",
    "\n",
    "*The spaCy EntityRuler allows the user to introduce a variety of complex rules and variences(via RegEx) by passing the rules to the pattern. For working within a United States domain, you can pass RegEx formulas to pattern matcher to grab all of these issues. There are many arguments that one can pass to the patterns. For a complete list, see: https://spacy.io/usage/rule-based-matching.*\n",
    "\n",
    "*In the example below, we work with one example from spaCy documentation in which we extract a phone nunber from a text. This same task can be done via Regex as well.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the requisite library\n",
    "import spacy\n",
    "\n",
    "# Sample text\n",
    "text = \"Any query about Alchemy Software Limited, contact with the number (+88)01313-406600\"\n",
    "\n",
    "# Build upon the spaCy Small Model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create the Ruler and Add it\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# List of Entities and Patterns (source: https://spacy.io/usage/rule-based-matching)\n",
    "patterns = [{\"label\":\"PHONE_NUMBER\", \"pattern\":[{\"ORTH\":\"(\"}, {\"ORTH\":\"+\"}, {\"SHAPE\":\"dd\"}, {\"ORTH\":\")\"}, {\"SHAPE\":\"ddddd\"}, {\"ORTH\":\"-\", \"OP\":\"?\"}, {\"SHAPE\":\"dddddd\"}]}]\n",
    "\n",
    "# Add patterns to Ruler\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Create the doc\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extarct Entitirs\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "\n",
    "'''\n",
    "xxx\n",
    "> Could not configure the pattern of Mobile Number\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Rules-Based Matching Techniques\n",
    "*There are two other rules-based methods in spaCy: *****Matcher***** and *****PhraseMatcher*****. We habe already met the *****Matcher***** in *****Rules-Based Matching*****. We will be meeting other more complex rules-based matching methods further.*"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88bd64abd146d54e091ff1398670d31add9e50ee4691cdc8ffb9431c01087a21"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
