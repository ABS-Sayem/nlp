{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count Vectorizer\n",
    "*In this portion, we will discuss about count vectorizer. What count vectorizer do- it represents the text as a vector. How it works- suppose you have 5 sentences. it seperates all the unique words from the text and make a 1D matrix of the words. Then for every sentence, it places 1s for the words situated in the sentence and 0s for te remaining words. Thus it creates a 5D matrix ie. a vector. It can be binary or ngrams. Binary count one word only once although it may occur multiple times in a sentence.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Category:\n",
    "    TRAVEL = \"TRAVEL\"\n",
    "    CLOTHING = \"CLOTHING\"\n",
    "\n",
    "trainX = [\"I love train journey\",\"I love reading books on train\",\"Train journey is cheap\",\"I like to wear shirt\",\"T-shirt fits best in summer\"]\n",
    "trainY = [Category.TRAVEL, Category.TRAVEL, Category.TRAVEL, Category.CLOTHING, Category.CLOTHING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best', 'books', 'cheap', 'fits', 'in', 'is', 'journey', 'like', 'love', 'on', 'reading', 'shirt', 'summer', 'to', 'train', 'wear']\n",
      "[[0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0]\n",
      " [0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 0]\n",
      " [0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1]\n",
      " [1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True)       # This is a unigram process. ie- vectorize per word\n",
    "#vectorizer = CountVectorizer(binary=True, ngram_range=(1,2))    # Bigram process\n",
    "trainX_vector = vectorizer.fit_transform(trainX)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "print(trainX_vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a Classifier\n",
    "from sklearn import svm\n",
    "\n",
    "linear_svm = svm.SVC(kernel='linear')\n",
    "linear_svm.fit(trainX_vector,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TRAVEL'], dtype='<U8')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction\n",
    "testX = vectorizer.transform([\"visiting new place is great\"])\n",
    "\n",
    "linear_svm.predict(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Vectors\n",
    "*Word vector is also a simillar type concept that is creating a vector using all the sentences in a text. But here it contains semantic meaning of a word in a vector. In *****Count Vectorizer***** because it place 1 for evry situated word, it cannot represents any significance of any word in the sentence. Word Vectors can seperate the valuable words and their importance in the sentence. It can also measure the relationship among words in sentence. For Word Vectorizer we will use *****word2vec*****.*\n",
    "\n",
    "*Here, we will use spacy library. Spacy has some pretrained model, we can make advantage of them. Brfore using you have to install spacy incase you didn't and download any of the language model.*\n",
    "\n",
    "*> Install spacy: *****pip install spacy***** *\n",
    "\n",
    "*> Download Model: *****!python -m spacy download \"en_core_web_md\"*****, it is a medium model. Is also has small(sm) amd large(lg) models.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the medium model\n",
    "#!python -m spacy download \"en_core_web_md\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*One download the model you can comment out the code because we need not download it again. We can load the model and use it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the spacy library and Load the medium model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now, we need some text on which we can apply the model. We can use the text using before in count vectorizer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the data\n",
    "class Category:\n",
    "    TRAVEL = \"TRAVEL\"\n",
    "    CLOTHING = \"CLOTHING\"\n",
    "\n",
    "trainX = [\"I love train journey\",\"I love reading books on train\",\"Train journey is cheap\",\"I like to wear shirt\",\"T-shirt fits best in summer\"]\n",
    "trainY = [Category.TRAVEL, Category.TRAVEL, Category.TRAVEL, Category.CLOTHING, Category.CLOTHING]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we will represent our trainX in vector format. Before that we have to make the trainx into doc. Doc removes the individual sentence string and make them one.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making docs of data and vectorize\n",
    "docs = [nlp(text) for text in trainX]\n",
    "#print(docs[0].vector)\n",
    "wv_trainX_vector = [x.vector for x in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the classifier\n",
    "from sklearn import svm\n",
    "linear_svm_wv = svm.SVC(kernel='linear')\n",
    "linear_svm_wv.fit(wv_trainX_vector, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CLOTHING'], dtype='<U8')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make Prediction\n",
    "wv_testX = [\"I love tshirt but wear less\"]\n",
    "wv_testX_docs = [nlp(text) for text in wv_testX]\n",
    "wv_testX_vector = [x.vector for x in wv_testX_docs]\n",
    "\n",
    "linear_svm_wv.predict(wv_testX_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regex\n",
    "*Regular expression has various works in natural language processing. One of them is Pattern Matching. Here, we will talk about pattern matching like- phone number, email, password checker etc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expression for various scinario.\n",
    "regexp0 = re.compile(r\"book|accident|concept\")  # Hard words\n",
    "regexp1 = re.compile(r\"\\bbook\\b|\\baccedint\\b|\\bconcept\\b\")\n",
    "regexp2 = re.compile(r\"^ab[^\\s]*cd$\")    # start with 'ab' and end with 'cd', anything in the middle except white space.\n",
    "\n",
    "phrases0 = [\"I have this book\", \"Reebook is a brand name\", \"The concept isn't perfect\", \"Roads are barely good\"]\n",
    "phrases1 = [\"abcd\", \"asdf\", \"xvcd\", \"ab cd\", \"abxxxcd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I have this book', \"The concept isn't perfect\"]\n"
     ]
    }
   ],
   "source": [
    "# Searching\n",
    "searched = []\n",
    "for phrase in phrases0:\n",
    "    if(re.search(regexp1, phrase)):\n",
    "        searched.append(phrase)\n",
    "print(searched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can search specific word or portion of a word by regular expression. Here, we see that in first sentence it found the word book seperately and in 2nd sentence as part of the word reebook.*\n",
    "\n",
    "*But if you want the specific word, not as a part of words, you can use no charecters in the word boundraries (\\b) shown in regexp1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abcd', 'abxxxcd']\n"
     ]
    }
   ],
   "source": [
    "# Find Match\n",
    "matches = []\n",
    "for phrase in phrases1:\n",
    "    if(re.match(regexp2, phrase)):\n",
    "        matches.append(phrase)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming and Lemmatization\n",
    "*Stemming and Lemmatization are two techniques of normalize texts. In vectorization we saw that while training our model it can recognize word but words. While to us it is straight forward to recognize that they are similar word. So, stemming and lemmatization finds the root word.*\n",
    "\n",
    "*Abs Sayem*"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "703b2bc398e15cd74c7a93565a102b3060db76bb218c5bf5b3619260b8a02446"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
