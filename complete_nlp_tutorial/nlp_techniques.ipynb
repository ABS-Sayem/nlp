{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count Vectorizer\n",
    "*In this portion, we will discuss about count vectorizer. What count vectorizer do- it represents the text as a vector. How it works- suppose you have 5 sentences. it seperates all the unique words from the text and make a 1D matrix of the words. Then for every sentence, it places 1s for the words situated in the sentence and 0s for te remaining words. Thus it creates a 5D matrix ie. a vector. It can be binary or ngrams. Binary count one word only once although it may occur multiple times in a sentence.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Category:\n",
    "    TRAVEL = \"TRAVEL\"\n",
    "    CLOTHING = \"CLOTHING\"\n",
    "\n",
    "trainX = [\"I love train journey\",\"I love reading books on train\",\"Train journey is cheap\",\"I like to wear shirt\",\"T-shirt fits best in summer\"]\n",
    "trainY = [Category.TRAVEL, Category.TRAVEL, Category.TRAVEL, Category.CLOTHING, Category.CLOTHING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best', 'books', 'cheap', 'fits', 'in', 'is', 'journey', 'like', 'love', 'on', 'reading', 'shirt', 'summer', 'to', 'train', 'wear']\n",
      "[[0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0]\n",
      " [0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 0]\n",
      " [0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1]\n",
      " [1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True)       # This is a unigram process. ie- vectorize per word\n",
    "#vectorizer = CountVectorizer(binary=True, ngram_range=(1,2))    # Bigram process\n",
    "trainX_vector = vectorizer.fit_transform(trainX)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "print(trainX_vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a Classifier\n",
    "from sklearn import svm\n",
    "\n",
    "linear_svm = svm.SVC(kernel='linear')\n",
    "linear_svm.fit(trainX_vector,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TRAVEL'], dtype='<U8')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction\n",
    "testX = vectorizer.transform([\"visiting new place is great\"])\n",
    "\n",
    "linear_svm.predict(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Vectors\n",
    "*Word vector is also a simillar type concept that is creating a vector using all the sentences in a text. But here it contains semantic meaning of a word in a vector. In *****Count Vectorizer***** because it place 1 for evry situated word, it cannot represents any significance of any word in the sentence. Word Vectors can seperate the valuable words and their importance in the sentence. It can also measure the relationship among words in sentence. For Word Vectorizer we will use *****word2vec*****.*\n",
    "\n",
    "*Here, we will use spacy library. Spacy has some pretrained model, we can make advantage of them. Brfore using you have to install spacy incase you didn't and download any of the language model.*\n",
    "\n",
    "*> Install spacy: *****pip install spacy***** *\n",
    "\n",
    "*> Download Model: *****!python -m spacy download \"en_core_web_md\"*****, it is a medium model. Is also has small(sm) amd large(lg) models.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the medium model\n",
    "#!python -m spacy download \"en_core_web_md\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*One download the model you can comment out the code because we need not download it again. We can load the model and use it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the spacy library and Load the medium model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now, we need some text on which we can apply the model. We can use the text using before in count vectorizer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the data\n",
    "class Category:\n",
    "    TRAVEL = \"TRAVEL\"\n",
    "    CLOTHING = \"CLOTHING\"\n",
    "\n",
    "trainX = [\"I love train journey\",\"I love reading books on train\",\"Train journey is cheap\",\"I like to wear shirt\",\"T-shirt fits best in summer\"]\n",
    "trainY = [Category.TRAVEL, Category.TRAVEL, Category.TRAVEL, Category.CLOTHING, Category.CLOTHING]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we will represent our trainX in vector format. Before that we have to make the trainx into doc. Doc removes the individual sentence string and make them one.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making docs of data and vectorize\n",
    "docs = [nlp(text) for text in trainX]\n",
    "#print(docs[0].vector)\n",
    "wv_trainX_vector = [x.vector for x in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the classifier\n",
    "from sklearn import svm\n",
    "linear_svm_wv = svm.SVC(kernel='linear')\n",
    "linear_svm_wv.fit(wv_trainX_vector, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CLOTHING'], dtype='<U8')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make Prediction\n",
    "wv_testX = [\"I love tshirt but wear less\"]\n",
    "wv_testX_docs = [nlp(text) for text in wv_testX]\n",
    "wv_testX_vector = [x.vector for x in wv_testX_docs]\n",
    "\n",
    "linear_svm_wv.predict(wv_testX_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regex\n",
    "*Regular expression has various works in natural language processing. One of them is Pattern Matching. Here, we will talk about pattern matching like- phone number, email, password checker etc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expression for various scinario.\n",
    "regexp0 = re.compile(r\"book|accident|concept\")  # Hard words\n",
    "regexp1 = re.compile(r\"\\bbook\\b|\\baccedint\\b|\\bconcept\\b\")\n",
    "regexp2 = re.compile(r\"^ab[^\\s]*cd$\")    # start with 'ab' and end with 'cd', anything in the middle except white space.\n",
    "\n",
    "phrases0 = [\"I have this book\", \"Reebook is a brand name\", \"The concept isn't perfect\", \"Roads are barely good\"]\n",
    "phrases1 = [\"abcd\", \"asdf\", \"xvcd\", \"ab cd\", \"abxxxcd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I have this book', \"The concept isn't perfect\"]\n"
     ]
    }
   ],
   "source": [
    "# Searching\n",
    "searched = []\n",
    "for phrase in phrases0:\n",
    "    if(re.search(regexp1, phrase)):\n",
    "        searched.append(phrase)\n",
    "print(searched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can search specific word or portion of a word by regular expression. Here, we see that in first sentence it found the word book seperately and in 2nd sentence as part of the word reebook.*\n",
    "\n",
    "*But if you want the specific word, not as a part of words, you can use no charecters in the word boundraries (\\b) shown in regexp1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abcd', 'abxxxcd']\n"
     ]
    }
   ],
   "source": [
    "# Find Match\n",
    "matches = []\n",
    "for phrase in phrases1:\n",
    "    if(re.match(regexp2, phrase)):\n",
    "        matches.append(phrase)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming and Lemmatization\n",
    "*Stemming and Lemmatization are two techniques of normalize texts. In vectorization we saw that while training our model it can recognize word but words. While to us it is straight forward to recognize that they are similar word. So, tme main goal of stemming and lemmatization is to find the root word.*\n",
    "\n",
    "*But *****Stemming***** do not give the guarantee of stemming the actual root word because it uses an algorithm that follows some certain rules to reduce postfixes and find the root word. For example, writes => write, writing => writ, written => writt*\n",
    "\n",
    "*On the other hand *****Lemmatization***** guarantees to give the actual root word because it uses a dictionary to confirm that the reduced word is an actual valid word.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To perform Stemming and Lemmatization we will use nltk. First we need to import nltk and download some prerequsites. And once downloded the packages, you need not download them again.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now, gets jump into the stemming process- lets find out how it works...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming do not give the guarantee of stemming the actual root word because it uses an algorithm that follows some certain rules to reduce postfixes and finds the root word.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'stem do not give the guarante of stem the actual root word becaus it use an algorithm that follow some certain rule to reduc postfix and find the root word .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmer_phrase = \"Stemming do not give the guarantee of stemming the actual root word because it uses an algorithm that follows some certain rules to reduce postfixes and finds the root word.\"\n",
    "print(stemmer_phrase)\n",
    "stemmed_phrase = []\n",
    "words = word_tokenize(stemmer_phrase)\n",
    "for word in words:\n",
    "    stemmed_phrase.append(stemmer.stem(word))\n",
    "\" \".join(stemmed_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lets try Lemmatization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization guarantees to give the actual root word because it uses a dictionary to confirm that the reduced word is an actual valid word.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Lemmatization guarantee to give the actual root word because it use a dictionary to confirm that the reduce word be an actual valid word .'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatize_phrase = \"Lemmatization guarantees to give the actual root word because it uses a dictionary to confirm that the reduced word is an actual valid word.\"\n",
    "print(lemmatize_phrase)\n",
    "lemmatized_phrase = []\n",
    "words = word_tokenize(lemmatize_phrase)\n",
    "for word in words:\n",
    "    #lemmatized_phrase.append(lemmatizer.lemmatize(word))\n",
    "    # You can also identify the parts-of-speech you want to lemmatize\n",
    "    lemmatized_phrase.append(lemmatizer.lemmatize(word, pos='v'))\n",
    "\" \".join(lemmatized_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords Removal\n",
    "*Stopwords are set of words that are essential part of a sentence but they are not that usefull for meaning. We can remove them to simplify our analysis.*\n",
    "\n",
    "*NLTK has a predefined set of stopwords. We can import them from nltk.corpus*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stemming give guarantee fetch actual root word uses algorithm follows certain rules reduce postfixes finds root word . Lemmatization guarantees give actual root word uses dictionary confirm reduced word actual valid word .'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "#print(stop_words)\n",
    "stop_phrase = \"Stemming do not give the guarantee to fetch the actual root word because it uses an algorithm that follows some certain rules to reduce postfixes and finds the root word. Lemmatization guarantees to give the actual root word because it uses a dictionary to confirm that the reduced word is an actual valid word.\"\n",
    "words = word_tokenize(stop_phrase)\n",
    "stopped_phrase = []\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        stopped_phrase.append(word)\n",
    "\" \".join(stopped_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spell Correction, Sentiment, PoS Tag\n",
    "*In this section we will quick go through some techniques like- spell correction, parts-of-speech tagging, sentiment analysis etc. For this we will use a outsdanding library called *****TextBlob*****.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextBlob is not that incredible for spell correction\n",
      "[('TextBlob', 'NNP'), ('is', 'VBZ'), ('not', 'RB'), ('that', 'IN'), ('incredible', 'JJ'), ('for', 'IN'), ('spel', 'JJ'), ('correction', 'NN')]\n",
      "Sentiment(polarity=0.9, subjectivity=0.9)\n"
     ]
    }
   ],
   "source": [
    "# Import TextBlob\n",
    "from textblob import TextBlob\n",
    "\n",
    "phrase = \"TextBlob is not that incredible for spel correction\"\n",
    "tb_phrase = TextBlob(phrase)\n",
    "\n",
    "# Spell Correct\n",
    "print(tb_phrase.correct())\n",
    "\n",
    "# PoS Tagging\n",
    "print(tb_phrase.tags)\n",
    "\n",
    "# Sentiment\n",
    "print(tb_phrase.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Architecture\n",
    "*Transformer is the most powerfull architecture for language modeling. It overcomes the problems of Recurrent Neural Networks.*\n",
    "\n",
    "*We can easily use this transformer architecture through spacy. For this we need to install the transformer from spacy and this will allow us to download the predefined transformer architecture for language processing.*\n",
    "\n",
    "*Run the bellow code once. You need not install spacy-transformers and download any model again. spacy rename the model from 'en_trf_bertbaseuncased_lg' to 'en_core_web_tfr'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install spacy-transformers\n",
    "#!python -m spacy download en_trf_bertbaseuncased_sm    [not use anymore]\n",
    "#!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "# Here, we will load the transformer model as nlp and will use it to process our data.\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "#doc = nlp(\"Spacy remane the transformer model from en_trf_bertbaseuncased_lg to en_web_core_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Category:\n",
    "    TRAVEL = \"TRAVEL\"\n",
    "    CLOTHING = \"CLOTHING\"\n",
    "\n",
    "trainX = [\"I love train journey\",\"I love reading books on train\",\"Train journey is cheap\",\"I like to wear shirt\",\"T-shirt fits best in summer\"]\n",
    "trainY = [Category.TRAVEL, Category.TRAVEL, Category.TRAVEL, Category.CLOTHING, Category.CLOTHING]\n",
    "#trainY = [0, 0, 0, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-f217813bdbb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtrf_linear_svm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtrf_linear_svm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrf_trainX_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: fit() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "docsX = [nlp(text) for text in trainX]\n",
    "#docsY = [nlp(text) for text in trainY]\n",
    "trf_trainX_vector = [doc.vector for doc in docsX]\n",
    "#trf_trainY_vector = [doc.vector for doc in docsY]\n",
    "trf_linear_svm = svm.SVC()\n",
    "\n",
    "trf_linear_svm.fit(trf_trainX_vector, trainY)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ad1a662050c36948a62a2159e528a078f0bb94ec7adadaee9eccaf8b42424e4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
