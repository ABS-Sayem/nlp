{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Named Entity Recognition(NER)**\n",
    "###### **Indentifying Named Entities in Text Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **What is Named Entity?**\n",
    "###### **Any words that represents a person, organization, location etc. is a Named Entity. Named Entity Recognition is a subtask of Information Extraction and is the process of identifying words which are named entities in a given text. It is also called Entity Identification or Entity Chunking.**\n",
    "##### **Example**\n",
    "\n",
    "###### **\"The Padma Bridge which is situated in Bangladesh is opened by the Prime Minister on Saturday 25th June 2022\"**\n",
    "\n",
    "* Here, named entities are - The Padma Bridge, Bangladesh, Prime Minister, Saturday, 25th June 2022\n",
    "* Named Entity Recognition is the task of identifying these words from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Why it is Important?**\n",
    "###### **In order to understand the meaning from a given text, it is important to identify who did what to whom. Named Entity Recognition is the first task of identifying the words which may represent the who, what and whom in the text. It helps in identifying the major entities the text is talking about - Any NLP task which involves automatically understanding text and acts based on it, needs Named Entity Recognition in its pipeline.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The Approaches**\n",
    "* **Basic NLTK Algorithm**\n",
    "    * with word segmentation\n",
    "    * with sentence segmentation\n",
    "* **Stanforn NLP NER**\n",
    "* **Using Spacy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Let's Start**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Import Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Stanford NER is a Java implementation of a Named Entity Recognizer. Named Entity Recognition (NER) labels sequences of words in a text which are the names of things, such as person and company names, or gene and protein names. It comes with well-engineered feature extractors for Named Entity Recognition, and many options for defining feature extractors. Included with the download are good named entity recognizers for English, particularly for the 3 classes (PERSON, ORGANIZATION, LOCATION), and we also make available on this page various other models for different languages and circumstances, including models trained on just the CoNLL 2003 English training data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Named Entity Tagging using NLTK (Word Based)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stanford', 'NER', 'is', 'a', 'Java', 'implementation', 'of', 'a', 'Named', 'Entity', 'Recognizer', '.', 'Named', 'Entity', 'Recognition', '(', 'NER', ')', 'labels', 'sequences', 'of', 'words', 'in', 'a', 'text', 'which', 'are', 'the', 'names', 'of', 'things', ',', 'such', 'as', 'person', 'and', 'company', 'names', ',', 'or', 'gene', 'and', 'protein', 'names', '.', 'It', 'comes', 'with', 'well-engineered', 'feature', 'extractors', 'for', 'Named', 'Entity', 'Recognition', ',', 'and', 'many', 'options', 'for', 'defining', 'feature', 'extractors', '.', 'Included', 'with', 'the', 'download', 'are', 'good', 'named', 'entity', 'recognizers', 'for', 'English', ',', 'particularly', 'for', 'the', '3', 'classes', '(', 'PERSON', ',', 'ORGANIZATION', ',', 'LOCATION', ')', ',', 'and', 'we', 'also', 'make', 'available', 'on', 'this', 'page', 'various', 'other', 'models', 'for', 'different', 'languages', 'and', 'circumstances', ',', 'including', 'models', 'trained', 'on', 'just', 'the', 'CoNLL', '2003', 'English', 'training', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the words\n",
    "words = nltk.word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Stanford', 'NNP'), ('NER', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('Java', 'NNP'), ('implementation', 'NN'), ('of', 'IN'), ('a', 'DT'), ('Named', 'NNP'), ('Entity', 'NNP'), ('Recognizer', 'NNP'), ('.', '.'), ('Named', 'VBN'), ('Entity', 'NNP'), ('Recognition', 'NNP'), ('(', '('), ('NER', 'NNP'), (')', ')'), ('labels', 'VBZ'), ('sequences', 'NNS'), ('of', 'IN'), ('words', 'NNS'), ('in', 'IN'), ('a', 'DT'), ('text', 'NN'), ('which', 'WDT'), ('are', 'VBP'), ('the', 'DT'), ('names', 'NNS'), ('of', 'IN'), ('things', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('person', 'NN'), ('and', 'CC'), ('company', 'NN'), ('names', 'RB'), (',', ','), ('or', 'CC'), ('gene', 'NN'), ('and', 'CC'), ('protein', 'JJ'), ('names', 'NNS'), ('.', '.'), ('It', 'PRP'), ('comes', 'VBZ'), ('with', 'IN'), ('well-engineered', 'JJ'), ('feature', 'NN'), ('extractors', 'NNS'), ('for', 'IN'), ('Named', 'NNP'), ('Entity', 'NNP'), ('Recognition', 'NNP'), (',', ','), ('and', 'CC'), ('many', 'JJ'), ('options', 'NNS'), ('for', 'IN'), ('defining', 'VBG'), ('feature', 'NN'), ('extractors', 'NNS'), ('.', '.'), ('Included', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('download', 'NN'), ('are', 'VBP'), ('good', 'JJ'), ('named', 'VBN'), ('entity', 'NN'), ('recognizers', 'NNS'), ('for', 'IN'), ('English', 'NNP'), (',', ','), ('particularly', 'RB'), ('for', 'IN'), ('the', 'DT'), ('3', 'CD'), ('classes', 'NNS'), ('(', '('), ('PERSON', 'NNP'), (',', ','), ('ORGANIZATION', 'NNP'), (',', ','), ('LOCATION', 'NNP'), (')', ')'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('also', 'RB'), ('make', 'VBP'), ('available', 'JJ'), ('on', 'IN'), ('this', 'DT'), ('page', 'NN'), ('various', 'JJ'), ('other', 'JJ'), ('models', 'NNS'), ('for', 'IN'), ('different', 'JJ'), ('languages', 'NNS'), ('and', 'CC'), ('circumstances', 'NNS'), (',', ','), ('including', 'VBG'), ('models', 'NNS'), ('trained', 'VBN'), ('on', 'IN'), ('just', 'RB'), ('the', 'DT'), ('CoNLL', 'NNP'), ('2003', 'CD'), ('English', 'NNP'), ('training', 'NN'), ('data', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Parts-of-Speech Tagging\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n"
     ]
    }
   ],
   "source": [
    "# Check NLTK description of the Tags\n",
    "nltk.help.upenn_tagset('NNS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **NE Chunk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NE Stanford/NNP)\n",
      "('NER', 'NNP')\n",
      "('is', 'VBZ')\n",
      "('a', 'DT')\n",
      "('Java', 'NNP')\n",
      "('implementation', 'NN')\n",
      "('of', 'IN')\n",
      "('a', 'DT')\n",
      "(NE Named/NNP Entity/NNP Recognizer/NNP)\n",
      "('.', '.')\n",
      "('Named', 'VBN')\n",
      "(NE Entity/NNP Recognition/NNP)\n",
      "('(', '(')\n",
      "(NE NER/NNP)\n",
      "(')', ')')\n",
      "('labels', 'VBZ')\n",
      "('sequences', 'NNS')\n",
      "('of', 'IN')\n",
      "('words', 'NNS')\n",
      "('in', 'IN')\n",
      "('a', 'DT')\n",
      "('text', 'NN')\n",
      "('which', 'WDT')\n",
      "('are', 'VBP')\n",
      "('the', 'DT')\n",
      "('names', 'NNS')\n",
      "('of', 'IN')\n",
      "('things', 'NNS')\n",
      "(',', ',')\n",
      "('such', 'JJ')\n",
      "('as', 'IN')\n",
      "('person', 'NN')\n",
      "('and', 'CC')\n",
      "('company', 'NN')\n",
      "('names', 'RB')\n",
      "(',', ',')\n",
      "('or', 'CC')\n",
      "('gene', 'NN')\n",
      "('and', 'CC')\n",
      "('protein', 'JJ')\n",
      "('names', 'NNS')\n",
      "('.', '.')\n",
      "('It', 'PRP')\n",
      "('comes', 'VBZ')\n",
      "('with', 'IN')\n",
      "('well-engineered', 'JJ')\n",
      "('feature', 'NN')\n",
      "('extractors', 'NNS')\n",
      "('for', 'IN')\n",
      "(NE Named/NNP Entity/NNP Recognition/NNP)\n",
      "(',', ',')\n",
      "('and', 'CC')\n",
      "('many', 'JJ')\n",
      "('options', 'NNS')\n",
      "('for', 'IN')\n",
      "('defining', 'VBG')\n",
      "('feature', 'NN')\n",
      "('extractors', 'NNS')\n",
      "('.', '.')\n",
      "('Included', 'VBN')\n",
      "('with', 'IN')\n",
      "('the', 'DT')\n",
      "('download', 'NN')\n",
      "('are', 'VBP')\n",
      "('good', 'JJ')\n",
      "('named', 'VBN')\n",
      "('entity', 'NN')\n",
      "('recognizers', 'NNS')\n",
      "('for', 'IN')\n",
      "(NE English/NNP)\n",
      "(',', ',')\n",
      "('particularly', 'RB')\n",
      "('for', 'IN')\n",
      "('the', 'DT')\n",
      "('3', 'CD')\n",
      "('classes', 'NNS')\n",
      "('(', '(')\n",
      "(NE PERSON/NNP)\n",
      "(',', ',')\n",
      "(NE ORGANIZATION/NNP)\n",
      "(',', ',')\n",
      "(NE LOCATION/NNP)\n",
      "(')', ')')\n",
      "(',', ',')\n",
      "('and', 'CC')\n",
      "('we', 'PRP')\n",
      "('also', 'RB')\n",
      "('make', 'VBP')\n",
      "('available', 'JJ')\n",
      "('on', 'IN')\n",
      "('this', 'DT')\n",
      "('page', 'NN')\n",
      "('various', 'JJ')\n",
      "('other', 'JJ')\n",
      "('models', 'NNS')\n",
      "('for', 'IN')\n",
      "('different', 'JJ')\n",
      "('languages', 'NNS')\n",
      "('and', 'CC')\n",
      "('circumstances', 'NNS')\n",
      "(',', ',')\n",
      "('including', 'VBG')\n",
      "('models', 'NNS')\n",
      "('trained', 'VBN')\n",
      "('on', 'IN')\n",
      "('just', 'RB')\n",
      "('the', 'DT')\n",
      "('CoNLL', 'NNP')\n",
      "('2003', 'CD')\n",
      "('English', 'NNP')\n",
      "('training', 'NN')\n",
      "('data', 'NNS')\n",
      "('.', '.')\n"
     ]
    }
   ],
   "source": [
    "chunks = nltk.ne_chunk(pos_tags, binary=True)   # Either NE or not NE\n",
    "for chunk in chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stanford</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Named Entity Recognition</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOCATION</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NER</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Named Entity Recognizer</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PERSON</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>English</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Entity Recognition</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Entities Labels\n",
       "0                  Stanford     NE\n",
       "1  Named Entity Recognition     NE\n",
       "2                  LOCATION     NE\n",
       "3                       NER     NE\n",
       "4   Named Entity Recognizer     NE\n",
       "5                    PERSON     NE\n",
       "6              ORGANIZATION     NE\n",
       "7                   English     NE\n",
       "8        Entity Recognition     NE"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from html import entities\n",
    "\n",
    "entities = []\n",
    "labels = []\n",
    "for chunk in chunks:\n",
    "    if(hasattr(chunk, 'label')):\n",
    "        entities.append(' '.join(c[0] for c in chunk))\n",
    "        labels.append(chunk.label())\n",
    "\n",
    "entities_labels = list(set(zip(entities, labels)))\n",
    "entities_df = pd.DataFrame(entities_labels)\n",
    "entities_df.columns = [\"Entities\", \"Labels\"]\n",
    "entities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PERSON Stanford/NNP)\n",
      "(ORGANIZATION NER/NNP)\n",
      "('is', 'VBZ')\n",
      "('a', 'DT')\n",
      "('Java', 'NNP')\n",
      "('implementation', 'NN')\n",
      "('of', 'IN')\n",
      "('a', 'DT')\n",
      "('Named', 'NNP')\n",
      "('Entity', 'NNP')\n",
      "('Recognizer', 'NNP')\n",
      "('.', '.')\n",
      "('Named', 'VBN')\n",
      "(PERSON Entity/NNP Recognition/NNP)\n",
      "('(', '(')\n",
      "(ORGANIZATION NER/NNP)\n",
      "(')', ')')\n",
      "('labels', 'VBZ')\n",
      "('sequences', 'NNS')\n",
      "('of', 'IN')\n",
      "('words', 'NNS')\n",
      "('in', 'IN')\n",
      "('a', 'DT')\n",
      "('text', 'NN')\n",
      "('which', 'WDT')\n",
      "('are', 'VBP')\n",
      "('the', 'DT')\n",
      "('names', 'NNS')\n",
      "('of', 'IN')\n",
      "('things', 'NNS')\n",
      "(',', ',')\n",
      "('such', 'JJ')\n",
      "('as', 'IN')\n",
      "('person', 'NN')\n",
      "('and', 'CC')\n",
      "('company', 'NN')\n",
      "('names', 'RB')\n",
      "(',', ',')\n",
      "('or', 'CC')\n",
      "('gene', 'NN')\n",
      "('and', 'CC')\n",
      "('protein', 'JJ')\n",
      "('names', 'NNS')\n",
      "('.', '.')\n",
      "('It', 'PRP')\n",
      "('comes', 'VBZ')\n",
      "('with', 'IN')\n",
      "('well-engineered', 'JJ')\n",
      "('feature', 'NN')\n",
      "('extractors', 'NNS')\n",
      "('for', 'IN')\n",
      "(PERSON Named/NNP Entity/NNP Recognition/NNP)\n",
      "(',', ',')\n",
      "('and', 'CC')\n",
      "('many', 'JJ')\n",
      "('options', 'NNS')\n",
      "('for', 'IN')\n",
      "('defining', 'VBG')\n",
      "('feature', 'NN')\n",
      "('extractors', 'NNS')\n",
      "('.', '.')\n",
      "('Included', 'VBN')\n",
      "('with', 'IN')\n",
      "('the', 'DT')\n",
      "('download', 'NN')\n",
      "('are', 'VBP')\n",
      "('good', 'JJ')\n",
      "('named', 'VBN')\n",
      "('entity', 'NN')\n",
      "('recognizers', 'NNS')\n",
      "('for', 'IN')\n",
      "(GPE English/NNP)\n",
      "(',', ',')\n",
      "('particularly', 'RB')\n",
      "('for', 'IN')\n",
      "('the', 'DT')\n",
      "('3', 'CD')\n",
      "('classes', 'NNS')\n",
      "('(', '(')\n",
      "(ORGANIZATION PERSON/NNP)\n",
      "(',', ',')\n",
      "(ORGANIZATION ORGANIZATION/NNP)\n",
      "(',', ',')\n",
      "(ORGANIZATION LOCATION/NNP)\n",
      "(')', ')')\n",
      "(',', ',')\n",
      "('and', 'CC')\n",
      "('we', 'PRP')\n",
      "('also', 'RB')\n",
      "('make', 'VBP')\n",
      "('available', 'JJ')\n",
      "('on', 'IN')\n",
      "('this', 'DT')\n",
      "('page', 'NN')\n",
      "('various', 'JJ')\n",
      "('other', 'JJ')\n",
      "('models', 'NNS')\n",
      "('for', 'IN')\n",
      "('different', 'JJ')\n",
      "('languages', 'NNS')\n",
      "('and', 'CC')\n",
      "('circumstances', 'NNS')\n",
      "(',', ',')\n",
      "('including', 'VBG')\n",
      "('models', 'NNS')\n",
      "('trained', 'VBN')\n",
      "('on', 'IN')\n",
      "('just', 'RB')\n",
      "('the', 'DT')\n",
      "(ORGANIZATION CoNLL/NNP)\n",
      "('2003', 'CD')\n",
      "(GPE English/NNP)\n",
      "('training', 'NN')\n",
      "('data', 'NNS')\n",
      "('.', '.')\n"
     ]
    }
   ],
   "source": [
    "chunks = nltk.ne_chunk(pos_tags, binary=False)   # Either NE or not NE\n",
    "for chunk in chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stanford</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Named Entity Recognition</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CoNLL</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NER</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LOCATION</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PERSON</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Entity Recognition</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>English</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Entities        Labels\n",
       "0                  Stanford        PERSON\n",
       "1  Named Entity Recognition        PERSON\n",
       "2                     CoNLL  ORGANIZATION\n",
       "3                       NER  ORGANIZATION\n",
       "4                  LOCATION  ORGANIZATION\n",
       "5                    PERSON  ORGANIZATION\n",
       "6        Entity Recognition        PERSON\n",
       "7                   English           GPE\n",
       "8              ORGANIZATION  ORGANIZATION"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from html import entities\n",
    "\n",
    "entities = []\n",
    "labels = []\n",
    "for chunk in chunks:\n",
    "    if(hasattr(chunk, 'label')):\n",
    "        entities.append(' '.join(c[0] for c in chunk))\n",
    "        labels.append(chunk.label())\n",
    "\n",
    "entities_labels = list(set(zip(entities, labels)))\n",
    "entities_df = pd.DataFrame(entities_labels)\n",
    "entities_df.columns = [\"Entities\", \"Labels\"]\n",
    "entities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Named Entity Tagging using NLTK (Sentence Based)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stanford</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Named Entity Recognition</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CoNLL</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NER</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LOCATION</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PERSON</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Entity Recognition</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>English</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Entities        Labels\n",
       "0                  Stanford        PERSON\n",
       "1  Named Entity Recognition        PERSON\n",
       "2                     CoNLL  ORGANIZATION\n",
       "3                       NER  ORGANIZATION\n",
       "4                  LOCATION  ORGANIZATION\n",
       "5                    PERSON  ORGANIZATION\n",
       "6        Entity Recognition        PERSON\n",
       "7                   English           GPE\n",
       "8              ORGANIZATION  ORGANIZATION"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from html import entities\n",
    "\n",
    "entities = []\n",
    "labels = []\n",
    "\n",
    "sentence = nltk.sent_tokenize(text)\n",
    "for sent in sentence:\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent)), binary=False):\n",
    "        if(hasattr(chunk, 'label')):\n",
    "            entities.append(' '.join(c[0] for c in chunk))\n",
    "            labels.append(chunk.label())\n",
    "\n",
    "entities_labels = list(set(zip(entities, labels)))\n",
    "entities_df = pd.DataFrame(entities_labels)\n",
    "entities_df.columns = [\"Entities\", \"Labels\"]\n",
    "entities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Stanford NLP NER**\n",
    "###### **Installation and Configuration: https://blog.manash.io/configuring-stanford-parser-and-stanford-ner-tagger-with-nltk-in-python-on-windows-f685483c374a**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"C:/Users/Abs_Sayem/packages/nlp/stanford-ner-2020-11-17/classifiers/english.all.3class.distsim.crf.ser.gz\"\n",
    "jar = \"C:/Users/Abs_Sayem/packages/nlp/stanford-ner-2020-11-17/stanford-ner.jar\"\n",
    "\n",
    "st = StanfordNERTagger(model, jar, encoding='utf-8')\n",
    "\n",
    "java_path = \"C:/Program Files/Java/jdk1.8.0_161/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the java file!\nUse software specific configuration paramaters or set the JAVAHOME environment variable.\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-da8e49023955>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtokenized_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclassified_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclassified_text_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassified_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclassified_text_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'first'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abs_Sayem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tag\\stanford.py\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;31m# This function should return list of tuple rather than list of list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abs_Sayem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tag\\stanford.py\u001b[0m in \u001b[0;36mtag_sents\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mdefault_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_java_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mconfig_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;31m# Create a temporary input file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abs_Sayem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mconfig_java\u001b[1;34m(bin, options, verbose)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \"\"\"\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0m_java_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_java_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     _java_bin = find_binary(\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;34m\"java\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mbin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abs_Sayem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    686\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m ):\n\u001b[1;32m--> 688\u001b[1;33m     return next(\n\u001b[0m\u001b[0;32m    689\u001b[0m         find_binary_iter(\n\u001b[0;32m    690\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abs_Sayem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    671\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mWhether\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mto\u001b[0m \u001b[0mprint\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mfound\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m     \"\"\"\n\u001b[1;32m--> 673\u001b[1;33m     for file in find_file_iter(\n\u001b[0m\u001b[0;32m    674\u001b[0m         \u001b[0mpath_to_bin\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m     ):\n",
      "\u001b[1;32mc:\\Users\\Abs_Sayem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"\\n\\n  For more information on %s, see:\\n    <%s>\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"=\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\\n%s\\n%s\\n%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the java file!\nUse software specific configuration paramaters or set the JAVAHOME environment variable.\n==========================================================================="
     ]
    }
   ],
   "source": [
    "tokenized_text = nltk.word_tokenize(text)\n",
    "classified_text = st.tag(tokenized_text)\n",
    "\n",
    "classified_text_df = pd.DataFrame(classified_text)\n",
    "classified_text_df.drop_duplicates(keep='first', inplace=True)\n",
    "classified_text_df.reset_index(drop=True, inplace=True)\n",
    "classified_text_df.columns = [\"Entities\", \"Labels\"]\n",
    "classified_text_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ad1a662050c36948a62a2159e528a078f0bb94ec7adadaee9eccaf8b42424e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
