{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pypdf2\n",
    "# Open the terminal and run the command \"pip install pypdf2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyPDF2 and its modules\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pdf file\n",
    "my_file = PdfFileReader(\"Bridging_The_Gap_Between_Training_&_Inference_For_Neural_Machine_Translation.pdf\")\n",
    "#print(my_file.getNumPages())\n",
    "str = \"\"\n",
    "for i in range(10):\n",
    "    str += my_file.getPage(i).extractText()\n",
    "\n",
    "# Making a text file\n",
    "with open(\"my_file.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"my_file.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Bridging the Gap between T raining and Infer ence', ' f or Neural Machine T ranslation', ' W en Zhang', ' 1', ' ;', ' 2', ' Y ang F eng', ' 1', ' ;', ' 2', '', ' F andong Meng', ' 3', ' Di Y ou', ' 4', ' Qun Liu', ' 5', ' 1', ' K e y Laboratory of Intelligent Information Processing', ' Institute of Computing T echnology , Chinese Academy of Sciences (ICT/CAS)', ' 2', ' Uni v ersity of Chinese Academy of Sciences, Beijing, China', ' f', ' zhangwen', ' ,', ' fengyang', ' g', ' @', ' ict.ac.cn', ' 3', ' P attern Recognition Center , W eChat AI, T encent Inc, China', ' fandongmeng', ' @', ' tencent.com', ' 4', ' W orcester Polytechnic Institute, W orcester , MA, USA', ' dyou', ' @', ' wpi.edu', ' 5', \" Hua wei Noah' s Ark Lab, Hong K ong, China\", ' qun.liu', ' @', ' huawei.com', ' Abstract', ' Neural Machine T ranslation (NMT) generates', ' tar get w ords sequentially in the w ay of pre-', ' dicting the ne xt w ord conditioned on the con-', ' te xt w ords. At training time, it predicts with', ' the ground truth w ords as conte xt whil e at in-', ' ference it has to generate the entire sequence', ' from scratch. This discrepanc y of the fed con-', ' te xt leads to error accumulation among the', ' w ay . Furthermore, w ord-le v el training re-', ' quires strict matching between the generated', ' sequence and the ground truth sequence which', ' leads to o v ercorrection o v er dif ferent b ut rea-', ' sonable translations. In this paper , we ad-', ' dress these issues by sampling conte xt w ords', ' not only from the ground truth sequence b ut', ' also from the predicted sequence by the model', ' during training, where the predicted sequence', ' is selected with a sentence-le v el optimum.', ' Experiment results on Chi n e se', ' !', ' English and', \" WMT'14 English\", ' !', ' German translation tasks', ' demonstrate that our approach can achie v e sig-', ' impro v ements on multiple datasets.', ' 1 Intr oduction', ' Neural Machine T ranslation has sho wn promising', ' results and dra wn more attention recently . Most', ' NMT models in the encoder -decoder frame-', ' w ork, including the RNN-based (', ' Sutsk e v er et al.', ' ,', ' 2014', ' ;', ' Bahdanau et al.', ' ,', ' 2015', ' ;', ' Meng and Zhang', ' ,', ' 2019', ' ), the CNN-based (', ' Gehring et al.', ' ,', ' 2017', ' ) and', ' the attention-based (', ' V asw ani et al.', ' ,', ' 2017', ' ) mod-', ' els, which predict the ne xt w ord conditioned on', ' the pre vious conte xt w ords, de ri ving a language', ' model o v er tar get w ords. The scenario is at train-', ' ing time the ground truth w ords are used as conte xt', '', ' Corresponding author .', ' while at inferenc e the entire sequence is generated', ' by the resulting model on its o wn and hence the', ' pre vious w ords generated by the model are fed as', ' conte xt. As a result, the predicted w ords at train-', ' ing and inference are dra wn from dif ferent dis-', ' trib utions, namely , from the data distrib ution as', ' opposed to the model distrib ution. This discrep-', ' anc y , called', ' e xposur e bias', ' (', ' Ranzato et al.', ' ,', ' 2015', ' ),', ' leads to a g ap between training and inference. As', ' the tar get sequence gro ws, the errors accumulate', ' among the sequence and the model has to predict', ' under the condition it has ne v er met at training', ' time.', ' Intuiti v ely , to address this problem, the model', ' should be trained to predict under the same con-', ' dition it will f ace at inference. Inspired by D', ' A T A', ' A', ' S', ' D', ' E M O N S T R A T O R', ' ( D', ' A', ' D ) (', ' V enkatraman et al.', ' ,', ' 2015', ' ), feeding as conte xt both ground truth w ords', ' and the predicted w ords during training can be', ' a solution. NMT m odels usually optimize the', ' cross-entrop y loss which requires a strict pairwise', ' matching at the w ord le v el between the predicted', ' sequence and the ground truth sequence. Once', ' the model generates a w ord de viating from the', ' ground truth sequence, the cross-entrop y loss will', ' correct the error immediately and dra w the re-', ' maining generation back to the ground truth se-', ' quence. Ho we v er , this causes a ne w problem. A', ' sentence usually has multiple reasonable transla-', ' tions and it cannot be said that the model mak es a', ' mistak e e v en if it generates a w ord dif ferent from', ' the ground truth w ord. F or e xample,', ' r efer ence', ' : W e should comply with the rule.', ' cand1', ' : W e should abide with the rule.', ' cand2', ' : W e should abide by the la w .', ' cand3', ' : W e should abide by the rule.', 'arXiv:1906.02448v2  [cs.CL]  17 Jun 2019', ' once the model generates ﬁabideﬂ a s the third', ' tar get w ord, the cross-entrop y loss w ould force', ' the model to generate ﬁwithﬂ as the fourth w ord', ' (as', ' cand1', ' ) so as to produce lar ger sentence-le v el', ' lik elihood and be in line with the reference,', ' although ﬁbyﬂ is the right choice. Then, ﬁwithﬂ', ' will be fed as conte xt to generate ﬁthe ruleﬂ, as', ' a result, the model is taught to generate ﬁabide', ' with the ruleﬂ which actually is wrong. The', ' translation', ' cand1', ' can be treated as', ' o ver corr ection', ' phenomenon. Another potential error is that e v en', ' the model predicts the right w ord ﬁbyﬂ follo wing', ' ﬁabideﬂ, when generating subsequent translation,', ' it may produce ﬁthe la wﬂ improperly by feeding', ' ﬁbyﬂ (as', ' cand2', ' ). Assume the references and the', ' training criterion let the model memorize the', ' pattern of the phrase ﬁthe ruleﬂ al w ays follo wing', ' the w ord ﬁw ithﬂ, to help the model reco v er from', ' the tw o kinds of errors and create the correct', ' translation lik e', ' cand3', ' , we should feed ﬁwithﬂ as', ' conte xt rather than ﬁbyﬂ e v en when the pre vious', ' predicted phrase is ﬁabide byﬂ. W e refer to this', ' solution as', ' Over corr ection Reco very', ' (', ' OR', ' ).', ' In this paper , we present a method to bridge the', ' g ap between training and inference and impro v e', ' the o v ercorrection reco v ery capability of NMT .', ' Our method selects', ' or acle', ' w ords from its pre-', ' dicted w ords and then samples as conte xt from the', ' oracle w ords and ground truth w ords. Meanwhile,', ' the oracle w ords are selected not only with a w ord-', ' by-w ord greedy search b ut also with a sentence-', ' le v el e v aluation, e.g. BLEU, which allo ws greater', ' xibility under the pairwise matching restriction', ' of cross-entrop y . At the be ginning of training, the', ' model selects as conte xt ground truth w ords at a', ' greater probability . As the model con v er ges grad-', ' ually , oracle w ords are chos en as conte xt more', ' often. In this w ay , the training process changes', ' from a fully guided scheme to w ards a less guided', ' scheme. Under this mechanism, the model has the', ' chance to learn to handle the mistak es made at in-', ' ference and also has the ability to reco v er from', ' o v ercorrection o v er alternati v e translations. W e', ' v erify our approach on both the RNNsearch model', ' and the stronger T ransformer model. The results', ' sho w that our approach can impro v e', ' the performance on both models.', ' 2 RNN-based NMT Model', ' Our method can be applied in a v ariety of NMT', ' models. W ithout loss of generality , we tak e the', ' RNN-based NMT (', ' Bahdanau et al.', ' ,', ' 2015', ' ) as an', ' e xample to introduce our method. Assume the', ' source sequence and the observ ed translation are', ' x', ' =', ' f', ' x', ' 1', ' ;', '', ' ; x', ' j', ' x', ' j', ' g', ' and', ' y', '', ' =', ' f', ' y', '', ' 1', ' ;', '', ' ; y', '', ' j', ' y', '', ' j', ' g', ' .', ' Encoder .', ' A bidirectional Gated Recurrent Unit', ' (GR U) (', ' Cho et al.', ' ,', ' 2014', ' ) is used to acquire tw o', ' sequences of hidden states, the annotation of', ' x', ' i', ' is', ' h', ' i', ' = [', ' !', ' h', ' i', ' ;', '  ', ' h', ' i', ' ]', ' . Note that', ' e', ' x', ' i', ' is emplo yed to', ' represent the embedding v ector of the w ord', ' x', ' i', ' .', ' !', ' h', ' i', ' =', ' GR U', ' (', ' e', ' x', ' i', ' ;', ' !', ' h', ' i', '', ' 1', ' )', ' (1)', '  ', ' h', ' i', ' =', ' GR U', ' (', ' e', ' x', ' i', ' ;', '  ', ' h', ' i', ' +1', ' )', ' (2)', ' Attention.', ' The attention is designed to e xtract', ' source information (calle d source conte xt v ector).', ' At the', ' j', ' -th step, the rele v ance between the tar get', ' w ord', ' y', '', ' j', ' and the', ' i', ' -th source w ord is e v aluated and', ' normalized o v er the source sequence', ' r', ' ij', ' =', ' v', ' T', ' a', ' tanh (', ' W', ' a', ' s', ' j', '', ' 1', ' +', ' U', ' a', ' h', ' i', ' )', ' (3)', '', ' ij', ' =', ' exp (', ' r', ' ij', ' )', ' P', ' j', ' x', ' j', ' i', ' 0', ' =1', ' exp', '', ' r', ' i', ' 0', ' j', '', ' (4)', ' The source conte xt v ector is the weighted sum of', ' all source annotations and can be calculated by', ' c', ' j', ' =', ' X', ' j', ' x', ' j', ' i', ' =1', '', ' ij', ' h', ' i', ' (5)', ' Decoder .', ' The decoder emplo ys a v ariant of', ' GR U to unroll the tar get information. At the', ' j', ' -th', ' step, the tar get hidden state', ' s', ' j', ' is gi v en by', ' s', ' j', ' =', ' GR U', ' (', ' e', ' y', '', ' j', '', ' 1', ' ; s', ' j', '', ' 1', ' ; c', ' j', ' )', ' (6)', ' The probability distrib ution', ' P', ' j', ' o v er all the w ords', ' in the tar get v ocab ulary is produced conditioned', ' on the embedding of the pre vious ground truth', ' w ord, the source conte xt v ector and the hidden', ' state', ' t', ' j', ' =', ' g', '', ' e', ' y', '', ' j', '', ' 1', ' ; c', ' j', ' ; s', ' j', '', ' (7)', ' o', ' j', ' =', ' W', ' o', ' t', ' j', ' (8)', ' P', ' j', ' = softmax (', ' o', ' j', ' )', ' (9)', ' where', ' g', ' stands for a linear transformation,', ' W', ' o', ' is', ' used to map', ' t', ' j', ' to', ' o', ' j', ' so that each tar get w ord has', ' one corresponding dimension in', ' o', ' j', ' .', ' 3 A ppr oach', ' The main frame w ork (as sho wn in Figure', ' 1', ' ) of our', ' method is to feed as conte xt either the ground truth', ' w ords or the pre vious predicted w ords, i.e.', ' or acle', ' Figure 1: The architecture of our method.', ' wor ds', ' , with a certain probability . This potentially', ' can reduce the g ap between training and inference', ' by tr aining the model to handle the situation which', ' will appear during test time. W e will introduce tw o', ' methods to select the oracle w ords. One method is', ' to select the oracle w ords at the w ord le v el with a', ' greedy search algorithm, and another is to select', ' a oracle sequence at the sentence-le v el optimum.', ' The sentence -le v el oracle pro vides an option of', ' n', ' -', ' gram matching with the ground truth sequence and', ' hence inherently has the ability of reco v ering from', ' o v ercorrection for the alternati v e conte xt. T o pre-', ' dict the', ' j', ' -th tar get w ord', ' y', ' j', ' , the follo wing steps are', ' in v olv ed in our approach:', ' 1.', ' Select an oracle w ord', ' y', ' oracle', ' j', '', ' 1', ' (at w ord le v el or', ' sentence le v el) at the', ' f', ' j', '', ' 1', ' g', ' -th step. (Section', ' Oracle W ord Selection', ' )', ' 2.', ' Sample from the ground truth w ord', ' y', '', ' j', '', ' 1', ' with', ' a probability of', ' p', ' or from the oracle w ord', ' y', ' oracle', ' j', '', ' 1', ' with a probability of', ' 1', '', ' p', ' . (Section', ' Sampling with Decay', ' )', ' 3.', ' Use the sampled w ord as', ' y', ' j', '', ' 1', ' and replace', ' the', ' y', '', ' j', '', ' 1', ' in Equation (', ' 6', ' ) and (', ' 7', ' ) with', ' y', ' j', '', ' 1', ' ,', ' then per form the follo wing prediction of the', ' attention-based NMT .', ' 3.1 Oracle W ord Selection', ' Generally , at the', ' j', ' -th step, the NMT model needs', ' the ground truth w ord', ' y', '', ' j', '', ' 1', ' as the conte xt w ord to', ' predict', ' y', ' j', ' , thus, we could select an oracle w ord', ' y', ' oracle', ' j', '', ' 1', ' to sim ulate the conte xt w ord. The oracle', ' w ord should be a w ord similar to the ground truth', ' or a synon ym. Using dif ferent strate gies will pro-', ' duce a dif ferent oracle w ord', ' y', ' oracle', ' j', '', ' 1', ' . One option is', ' that w ord-le v el greedy search could be emplo yed', ' to output the oracle w ord of each step, which is', ' called', ' W or d-le vel Or acle', ' (called', ' W O', ' ). Besides,', ' we can further optimize the oracle by enlar ging', ' the search space with beam search and then re-', ' ranking the candidate translations with a sentence-', ' le v el metric, e.g. BLEU (', ' P apineni et al.', ' ,', ' 2002', ' ),', ' Figure 2: W ord-le v el oracle without noise.', ' GLEU (', ' W u et al.', ' ,', ' 2016', ' ), R OUGE (', ' Lin', ' ,', ' 2004', ' ), etc,', ' the selected translation is called', ' or acle se ntence', ' ,', ' the w ords in the translation are', ' Sentence-le vel Or -', ' acle', ' (denoted as', ' SO', ' ).', ' W ord-Le v el Oracle', ' F or the', ' f', ' j', '', ' 1', ' g', ' -th decoding step, the direct w ay to', ' select the w ord-le v el oracle is to pick the w ord', ' with the highest probability from the w ord dis-', ' trib ution', ' P', ' j', '', ' 1', ' dra wn by Equation (', ' 9', ' ), which is', ' sho wn in Figure', ' 2', ' . The predicted score in', ' o', ' j', '', ' 1', ' is', ' the v alue before the', ' softmax', ' operation. In prac-', ' tice, we can acquire more rob ust w ord-le v el or -', ' acles by introducing the', ' Gumbel-Max', ' technique', ' (', ' Gumbel', ' ,', ' 1954', ' ;', ' Maddison et al.', ' ,', ' 2014', ' ), which', ' pro vides a simple and ef w ay to sample from', ' a cate gorical distrib ution.', ' The G umbel noise, treated as a form of re gular -', ' ization, is added to', ' o', ' j', '', ' 1', ' in Equation (', ' 8', ' ), as sho wn', ' in Figure', ' 3', ' , then', ' softmax', ' function is performed,', ' the w ord distrib ution of', ' y', ' j', '', ' 1', ' is approximated by', '', ' =', '', ' log (', '', ' log', ' u', ' )', ' (10)', ' ~', ' o', ' j', '', ' 1', ' = (', ' o', ' j', '', ' 1', ' +', '', ' )', ' =˝', ' (11)', ' ~', ' P', ' j', '', ' 1', ' = softmax ( ~', ' o', ' j', '', ' 1', ' )', ' (12)', ' where', '', ' is the Gumbel noise calculated from a uni-', ' form random v ariable', ' u', ' ˘ U', ' (0', ' ;', ' 1)', ' ,', ' ˝', ' is tempera-', ' ture. As', ' ˝', ' approaches 0, the', ' softmax', ' function is', ' similar to the', ' argmax', ' operation, and it becomes', ' uniform distrib ution gradually when', ' ˝', ' ! 1', ' .', ' Similarly , according to', ' ~', ' P', ' j', '', ' 1', ' , the', ' 1', ' -best w ord is', ' selected as the w ord-le v el oracle w ord', ' y', ' oracle', ' j', '', ' 1', ' =', ' y', ' W O', ' j', '', ' 1', ' = argmax', '', ' ~', ' P', ' j', '', ' 1', '', ' (13)', ' Note that the Gumbel noise is just used to select', ' the oracle and it does not af fect the loss function', ' for training.', ' Sentence-Le v el Oracle', ' The sentence-le v el oracle is emplo yed to allo w for', ' more xible translati on with', ' n', ' -gram matching re-', ' quired by a sentence-le v el metric. In this paper ,', ' Figure 3: W ord-le v el oracle with Gumbel noise.', ' we emplo y BLEU as the sentence-le v el metric. T o', ' select the sentence-le v el oracles, we perform', ' beam search for all sentences in each batch, as-', ' suming bea m size is', ' k', ' , and get', ' k', ' -best candi date', ' translations. In the process of beam search, we', ' also could apply the Gumbel noise for each w ord', ' generation. W e then e v aluate each translation by', ' calculating its BLEU score with the ground truth', ' sequence, and use the translation with the highest', ' BLEU score as the', ' or acle sentence', ' . W e denote it', ' as', ' y', ' S', ' = (', ' y', ' S', ' 1', ' ; :::; y', ' S', ' j', ' y', ' S', ' j', ' )', ' , then at the', ' j', ' -th decoding', ' step, we the sentence-le v el oracle w ord as', ' y', ' oracle', ' j', '', ' 1', ' =', ' y', ' SO', ' j', '', ' 1', ' =', ' y', ' S', ' j', '', ' 1', ' (14)', ' But a problem comes with sentence-le v el oracle.', ' As the model samples from ground truth w ord and', ' the sentence-le v el oracle w ord at each step, the', ' tw o sequences should ha v e the same number of', ' w ords. Ho we v er we can not assure this with the', ' nai v e beam search decoding algorithm. Based on', ' the abo v e problem, we introduce', ' for ce decoding', ' to', ' mak e sure the tw o sequences ha v e the same length.', ' F or ce Decoding .', ' As the length of the ground', ' truth sequence is', ' j', ' y', '', ' j', ' , the goal of force decod-', ' ing is to generate a sequence with', ' j', ' y', '', ' j', ' w ords fol-', ' lo wed by a special end-of-sentence (EOS) symbol.', ' Therefore, in beam search, once a candidate trans-', ' lation tends to end with EOS when it is shorter or', ' longer than', ' j', ' y', '', ' j', ' , we will force it to generate', ' j', ' y', '', ' j', ' w ords, that is,', '', ' If the candidate translation gets a w ord distri-', ' b ution', ' P', ' j', ' at the', ' j', ' -th step where', ' j', ' 6', ' j', ' y', '', ' j', ' and', ' EOS is the top w ord in', ' P', ' j', ' , t hen we select', ' the top second w ord in', ' P', ' j', ' as the', ' j', ' -th w ord of', ' this candidate translation.', '', ' If the candidate translation gets a w ord distri-', ' b ution', ' P', ' j', ' y', '', ' j', ' +1', ' at the', ' fj', ' y', '', ' j', ' +1', ' g', ' -th step where', ' EOS is not the top w ord in', ' P', ' j', ' y', '', ' j', ' +1', ' , then', ' we select EOS as the', ' fj', ' y', '', ' j', ' +1', ' g', ' -th w ord of', ' this candidate translation.', ' In this w ay , we can mak e sure that all the', ' k', ' can-', ' didate translations ha v e', ' j', ' y', '', ' j', ' w ords, then re-rank', ' the', ' k', ' candidates according to BLEU score a nd se-', ' lect the top as the oracle sentence. F or adding', ' Gumbel noise into the sentence-le v el oracle selec-', ' tion, we replace the', ' P', ' j', ' with', ' ~', ' P', ' j', ' at the', ' j', ' -th decod-', ' ing step during force decoding.', ' 3.2 Sampling with Decay', ' In our method, we emplo y a sampling mechanism', ' to randomly select the ground truth w ord', ' y', '', ' j', '', ' 1', ' or', ' the oracle w ord', ' y', ' oracle', ' j', '', ' 1', ' as', ' y', ' j', '', ' 1', ' . At the be ginning', ' of training, as the model is not well trained, us-', ' ing', ' y', ' oracle', ' j', '', ' 1', ' as', ' y', ' j', '', ' 1', ' too often w ould lead to v ery', ' slo w con v er gence, e v en being trapped into local', ' optimum. On the other hand, at the end of train-', ' ing, if the conte xt', ' y', ' j', '', ' 1', ' is still s elected from the', ' ground truth w ord', ' y', '', ' j', '', ' 1', ' at a lar ge probability , the', ' model is not fully e xposed to the circumstance', ' which it has to confront at inference and hence can', ' not kno w ho w to act in the situation at inference.', ' In this sense, the probability', ' p', ' of selecting from', ' the ground truth w ord can not be ed, b ut has', ' to decrease progressi v ely as the training adv ances.', ' At t he be ginning,', ' p', ' =1', ' , which means the model is', ' trained entirely based on the ground truth w ords.', ' As the model con v er ges gradually , the model se-', ' lects from the oracle w ords more often.', ' Borro wing ideas from b ut being dif ferent', ' from', ' Bengio et al.', ' (', ' 2015', ' ) which used a schedule to', ' decrease', ' p', ' as a function of the inde x of mini-batch,', ' we', ' p', ' with a decay function dependent on', ' the inde x of training epochs', ' e', ' (starting from', ' 0', ' )', ' p', ' =', '', '', ' + exp (', '', ' )', ' (15)', ' where', '', ' is a h yper -parameter . The function is', ' strictly monotone decreasing. As the training pro-', ' ceeds, the probability', ' p', ' of feeding ground truth', ' w ords decreases gradually .', ' 3.3 T raining', ' After selecting', ' y', ' j', '', ' 1', ' by using the abo v e method,', ' we can get the w ord distrib ution of', ' y', ' j', ' according', ' to Equation (', ' 6', ' ), (', ' 7', ' ), (', ' 8', ' ) and (', ' 9', ' ). W e do not add', ' the Gumbel noise to the di strib ution when calcu-', ' lating loss for training. The objecti v e is to maxi-', ' mize the probabi lity of the ground truth sequence', ' based on maximum lik elihood estimation (MLE).', ' Thus follo wing loss function is minimized:', ' L', ' (', '', ' ) =', '', ' X', ' N', ' n', ' =1', ' X', ' j', ' y', ' n', ' j', ' j', ' =1', ' log', ' P', ' n', ' j', '', ' y', ' n', ' j', '', ' (16)', ' where', ' N', ' is the number of sentence pairs in the', ' training data,', ' j', ' y', ' n', ' j', ' indicates the length of the', ' n', ' -th', ' ground truth sentence,', ' P', ' n', ' j', ' refers to the predicted', ' probability distrib ution at the', ' j', ' -th step for the', ' n', ' -th', ' sentence, hence', ' P', ' n', ' j', ' h', ' y', ' n', ' j', ' i', ' is the probability of gen-', ' erating the ground truth w ord', ' y', ' n', ' j', ' at the', ' j', ' -th step.', ' 4 Related W ork', ' Some other researchers ha v e noticed the prob-', ' lem of e xposure bias in NMT and tried to solv e', ' it.', ' V enkatraman et al.', ' (', ' 2015', ' ) proposed D', ' A T A', ' A', ' S', ' D', ' E M O N S T R A T O R', ' (D AD) which initialized', ' the training e xamples as the paired tw o adjacent', ' ground truth w ords and at each step added the pre-', ' dicted w ord paired with the ne xt ground truth w ord', ' as a ne w training e xample.', ' Bengio et al.', ' (', ' 2015', ' )', ' further de v eloped the method by sampling as con-', ' te xt from the pre vious ground truth w ord and the', ' pre vious predicted w ord with a changing probabil-', ' ity , not treating them equally in the whole training', ' process. This is similar to our method, b ut the y', ' do not include the sentence-le v el oracle to relie v e', ' the o v ercorrection problem and neither the noise', ' perturbations on the predicted distrib ution.', ' Another di rection of attempts is the sentence-', ' le v el training with the thinking that the sentence-', ' le v el metric, e.g., BLEU, brings a certain de-', ' gree of xibility for generation and hence is', ' more rob ust to mitig ate the e xposure bias problem.', ' T o a v oid the problem of e xposure bias,', ' Ranzato', ' et al.', ' (', ' 2015', ' ) presented a no v el algorithm Mix ed', ' Incremental Cross-Entrop y Reinforce (MIXER)', ' for sequence-le v el training, which directly op-', ' timized t he sentence-le v el BLEU used at infer -', ' ence.', ' Shen et al.', ' (', ' 2016', ' ) introduced the Minimum', ' Risk T raining (MR T) into the end-to-end NMT', ' model, which optimized model param eters by', ' minimizing directly the e xpected loss with respect', ' to arbitrary e v aluation metrics, e.g., sentence-le v el', ' BLEU.', ' Shao et al.', ' (', ' 2018', ' ) proposed to eliminate', ' the e xposure bias through a probabilistic n-gram', ' matching objecti v e, which trains NMT NMT un-', ' der the greedy decoding strate gy .', ' 5 Experiments', ' W e carry out e xperiments on the NIST', ' Chinese', ' !', ' English (Zh', ' !', \" En) and the WMT'14\", ' English', ' !', ' German (En', ' !', ' De) translation tasks.', ' 5.1 Settings', ' F or Zh', ' !', ' En, the training dataset consists of 1.25M', ' sentence pairs e xtracted from LDC corpora', ' 1', ' . W e', ' choose the NIST 2002 (MT02) dataset as the v al-', ' idation set, which has', ' 878', ' sentences, and the', ' NIST 2003 (MT03), NIST 2004 (MT04), NIST', ' 2005 (MT05) and NIST 2006 (MT06) datasets', ' as the test sets, which contain 919, 1788, 1082', ' and 1664 sentences respecti v ely . F or En', ' !', ' De,', ' we perform our e xperiments on the corpus pro-', \" vided by WMT'14, which contains 4.5M sentence\", ' pairs', ' 2', ' . W e use the', ' newstest2013', ' as the v alidation', ' set, and the', ' newstest2014', ' as the test sets, which', ' containing', ' 3003', ' and', ' 2737', ' sentences respe cti v ely .', ' W e measure the translation quality with BLEU', ' scores (', ' P apineni et al.', ' ,', ' 2002', ' ). F or Zh', ' !', ' En, case-', ' insensiti v e BLEU score is calculated by using the', ' mte val-v11b .pl', ' script. F or En', ' !', ' De, we tok enize', ' the references and e v aluate the performance with', ' case-sensiti v e BLEU score by the', ' multi-bleu.pl', ' script. The metrics are e xactly the same as in pre-', ' vious w ork. Besides, we mak e statistical', ' cance test according to the method of', ' Collins et al.', ' (', ' 2005', ' ).', ' In training the NMT model, we limit the source', ' and tar get v ocab ulary to the most frequent', ' 30', ' K', ' w ords for both sides in the Zh', ' !', ' En translation', ' task, co v ering approximately', ' 97', ' :', ' 7', ' % and', ' 99', ' :', ' 3', ' %', ' w ords of tw o corpus respecti v ely . F or the En', ' !', ' De', ' translation task, sentences are encoded using byte-', ' pair encoding (BPE) (', ' Se nnrich et al.', ' ,', ' 2016', ' ) with', ' 37', ' k', ' mer ging operations for both source and tar -', ' get languages, which ha v e v ocab ularies of', ' 39418', ' and', ' 40274', ' tok ens r especti v ely . W e limit the length', ' of sentences in the training datasets to', ' 50', ' w ords', ' for Zh', ' !', ' En and', ' 128', ' subw ords for En', ' !', ' De. F or', ' RNNSearch model, the dimension of w ord em-', ' bedding and hidden layer is', ' 512', ' , and the beam', ' size in testing is', ' 10', ' . All parameters are initialized', ' by the uniform distrib ution o v er', ' [', '', ' 0', ' :', ' 1', ' ;', ' 0', ' :', ' 1]', ' . The', ' mini-batch stochastic gradient descent (SGD) al-', ' gorithm is emplo yed to train the model parameters', ' with batch size setting to', ' 80', ' . Moreo v er , the learn-', ' ing rate is adjusted by adadelta optimizer (', ' Zeiler', ' ,', ' 2012', ' ) with', ' ˆ', ' =', ' 0', ' :', ' 95', ' and', '', ' =', ' 1', ' e', ' -', ' 6', ' . Dropout is applied', ' on the output layer with dropout rate being', ' 0', ' :', ' 5', ' .', ' F or T ransformer model, we train base model with', ' 1', ' These se ntence pairs are mainly e xtracted from', ' LDC2002E18, LDC2003E07, LDC2003E14, Hansards por -', ' tion of LDC2004T07, LDC2004T08 and LDC2005T06', ' 2', ' http://www.statmt.org/wmt14/', ' translation- task.html', ' Systems', ' Ar chitectur e', ' MT03', ' MT04', ' MT05', ' MT06', ' A v erage', ' Existing end-to-end NMT systems', ' T u et al.', ' (', ' 2016', ' )', ' Co v erage', ' 33', ' :', ' 69', ' 38', ' :', ' 05', ' 35', ' :', ' 01', ' 34', ' :', ' 83', ' 35', ' :', ' 40', ' Shen et al.', ' (', ' 2016', ' )', ' MR T', ' 37', ' :', ' 41', ' 39', ' :', ' 87', ' 37', ' :', ' 45', ' 36', ' :', ' 80', ' 37', ' :', ' 88', ' Zhang et al.', ' (', ' 2017', ' )', ' Distortion', ' 37', ' :', ' 93', ' 40', ' :', ' 40', ' 36', ' :', ' 81', ' 35', ' :', ' 77', ' 37', ' :', ' 73', ' Our end-to-end NMT systems', ' this w ork', ' RNNsearch', ' 37', ' :', ' 93', ' 40', ' :', ' 53', ' 36', ' :', ' 65', ' 35', ' :', ' 80', ' 37', ' :', ' 73', ' + SS-NMT', ' 38', ' :', ' 82', ' 41', ' :', ' 68', ' 37', ' :', ' 28', ' 37', ' :', ' 98', ' 38', ' :', ' 94', ' + MIXER', ' 38', ' :', ' 70', ' 40', ' :', ' 81', ' 37', ' :', ' 59', ' 38', ' :', ' 38', ' 38', ' :', ' 87', ' + OR-NMT', ' 40.40', ' zy', ' ?', ' 42.63', ' zy', ' ?', ' 38.87', ' zy', ' ?', ' 38.44', ' z', ' 40.09', ' T ransformer', ' 46', ' :', ' 89', ' 47', ' :', ' 88', ' 47', ' :', ' 40', ' 46', ' :', ' 66', ' 47', ' :', ' 21', ' + w ord oracle', ' 47', ' :', ' 42', ' 48', ' :', ' 34', ' 47', ' :', ' 89', ' 47', ' :', ' 34', ' 47', ' :', ' 75', ' + sentence oracle', ' 48.31', '', ' 49.40', '', ' 48.72', '', ' 48.45', '', ' 48.72', ' T able 1: Case-insensiti v e BLEU scores (%) on Zh', ' !', ' En translation task. ﬁ', ' z', ' ﬂ, ﬁ', ' y', ' ﬂ, ﬁ', ' ?', ' ﬂ and ﬁ', '', ' ﬂ indicate statistically', ' dif ference (p', ' <', ' 0.01) from RNNsearch, SS-NMT , MIXER and T ransformer , respecti v ely .', ' def ault settings (f airseq', ' 3', ' ).', ' 5.2 Systems', ' The follo wing systems are in v olv ed:', ' RNNsear ch:', ' Our implementation of an im-', ' pro v ed model as described in Section', ' 2', ' , where', ' the decoder emplo ys tw o GR Us and an attention.', ' , Equation', ' 6', ' is substituted with:', ' ~', ' s', ' j', ' =', ' GR U', ' 1', ' (', ' e', ' y', '', ' j', '', ' 1', ' ; s', ' j', '', ' 1', ' )', ' (17)', ' s', ' j', ' =', ' GR U', ' 2', ' (', ' c', ' j', ' ;', ' ~', ' s', ' j', ' )', ' (18)', ' Besides, in Equation', ' 3', ' ,', ' s', ' j', '', ' 1', ' is replaced with', ' ~', ' s', ' j', '', ' 1', ' .', ' SS-NMT :', ' Our implementation of the scheduled', ' sampling (SS) method (', ' Bengio et al.', ' ,', ' 2015', ' ) on the', ' basis of the RNNsearch. The decay scheme is the', ' same as Equation', ' 15', ' in our approach.', ' MIXER:', ' Our implementation of the mix ed in-', ' cremental cross-entrop y reinforce (', ' Ranzato et al.', ' ,', ' 2015', ' ), where the sentence-le v el metric is BLEU', ' and the a v erage re w ard is acquired according to', ' its of method with a', ' 1', ' -layer linear re gressor .', ' OR-NMT :', ' Based on the RNNsearch, we intro-', ' duced the w ord-le v el oracles, sentence-le v el ora-', ' cles and the Gumbel noises to enhance the o v er -', ' correction reco v ery capacity . F or the sentence-', ' le v el oracle selection, we set the beam size to be', ' 3', ' ,', ' set', ' ˝', ' =', ' 0', ' :', ' 5', ' in Equa tion (', ' 11', ' ) and', '', ' =', ' 12', ' for the decay', ' function in Equation (', ' 15', ' ). OR-NMT is the abbre-', ' viation of NMT with Ov ercorrection Reco v ery .', ' 3', ' https://github.com/pytorch/fairseq', ' 5.3 Results on Zh', ' !', ' En T ranslation', ' W e v erify our method on tw o baseline models with', ' the NIST Zh', ' !', ' En datasets in this section.', ' Results on the RNNsear ch', ' As sho wn in T able', ' 1', ' ,', ' T u et al.', ' (', ' 2016', ' ) propose to', ' model co v erage in RNN-based NMT to impro v e', ' the adequac y of translations.', ' Shen et al.', ' (', ' 2016', ' )', ' propose minimum risk training (MR T) for NMT', ' to directly optimize model parameters with respect', ' to BLEU scores.', ' Zhang et al.', ' (', ' 2017', ' ) model dis-', ' tortion to enhance the attention model . Compared', ' with them, our baseline system RNNsearch 1) out-', ' performs pre vious shallo w RNN-based NMT sys-', ' tem equipped with the co v erage model (', ' T u et al.', ' ,', ' 2016', ' ); and 2) achie v es competiti v e performance', ' with the MR T (', ' Shen et al.', ' ,', ' 2016', ' ) and the Distor -', ' tion (', ' Zhang et al.', ' ,', ' 2017', ' ) on the same datasets. W e', ' hope that the strong shallo w baseline system used', ' in this w ork mak es the e v aluation con vincing.', ' W e also compare with the other tw o related', ' methods that aim at solving the e xposure bias', ' problem, including the scheduled sampling (', ' Ben-', ' gio et al.', ' ,', ' 2015', ' ) (SS-NMT) and the sentence-', ' le v el training (', ' Ranzato et al.', ' ,', ' 2015', ' ) (MIXER).', ' From T able', ' 1', ' , we can see that both SS-NMT and', ' MIXER can achie v e impro v ements by taking mea-', ' sures to mitig ate the e xposure bias. While our', ' approach OR-NMT can outperform the baseline', ' system RNNsearch and the competiti v e compar -', ' ison systems by directly incorporate the sentence-', ' le v el oracle and noise perturbations for relie ving', ' the o v ercorrection problem. P articular ly , our OR-', ' NMT outperforms the RNNsearch', ' by +', ' 2', ' :', ' 36', ' BLEU points a v eragely on four test', ' datasets. Comparing with the tw o related models,', ' Systems', ' A v erage', ' RNNsearch', ' 37', ' :', ' 73', ' + w ord oracle', ' 38', ' :', ' 94', ' + noise', ' 39', ' :', ' 50', ' + sentence oracle', ' 39', ' :', ' 56', ' + noise', ' 40.09', ' T able 2: F actor analysis on Zh', ' !', ' En translation, the re-', ' sults are a v erage BLEU scores on MT03', ' ˘', ' 06 datasets.', ' our a pproach further gi v es a impro v e-', ' ments on most test sets and achie v es impro v ement', ' by about +', ' 1', ' :', ' 2', ' BLEU points on a v erage.', ' Results on the T ransf ormer', ' The methods we propose can also be adapted', ' to the stronger T ransformer model. The e v alu-', ' ated results are listed in T able', ' 1', ' . Our w ord-le v el', ' method can impro v e the base model by +', ' 0', ' :', ' 54', ' BLEU points on a v erage, and the sentence-le v el', ' method can further bring in +', ' 1', ' :', ' 0', ' BLEU points im-', ' pro v ement.', ' 5.4 F actor Analysis', ' W e propose se v eral strate gies to impro v e the per -', ' formance of approach on relie ving the o v ercorrec-', ' tion problem, including utilizing the w ord-le v el', ' oracle, the sentence-le v el oracle, and incorporat-', ' ing the Gumbel noise for oracle selection. T o in-', ' v estig ate the of these f actors , we conduct', ' the e xperiments and list the results in T able', ' 2', ' .', ' When only emplo ying the w ord-le v el oracle , the', ' translation performance w as impro v ed by +', ' 1', ' :', ' 21', ' BLEU points, this indicates that feeding pre-', ' dicted w ords as conte xt can mitig ate e xposure', ' bias. When emplo ying the sentence-le v el oracle,', ' we can further achie v e +', ' 0', ' :', ' 62', ' BLEU points im-', ' pro v ement. It sho ws t hat the sentence-le v el oracle', ' performs better than the w ord-le v el oracle i n terms', ' of BLEU. W e conjecture that the superiority may', ' come from a greater xibility for w ord genera-', ' tion which can mitig ate the problem of o v ercor -', ' rection. By incorporating the Gumbel noise dur -', ' ing the generation of the w ord-le v el and sentence-', ' le v el oracle w ords, the BLEU score are further im-', ' pro v ed by', ' 0', ' :', ' 56', ' and', ' 0', ' :', ' 53', ' respecti v ely . This indi-', ' cates Gumbel noise can help the selection of each', ' oracle w ord, which is consistent with our claim', ' that Gumbel-Max pro vides a ef and rob ust', ' w ay to sample from a cate gorical distrib ution.', ' Figure 4: T raining loss curv es on Zh', ' !', ' En translation', ' with dif ferent f actors. The black, blue and red colors', ' represent the RNNsearch, RNNs earch with w ord-le v el', ' oracle and RNNsearch with sentence-le v el oracle sys-', ' tems respecti v ely .', ' Figure 5: T rends of BLEU scores on the v alidation set', ' with dif ferent f actors on the Zh', ' !', ' En translation task.', ' 5.5 About Con v er gence', ' In this section, we analyze the of dif fer -', ' ent f actors for the con v er gence. Figure', ' 4', ' gi v es the', ' training loss curv es of the RNNsearch, w ord-le v el', ' oracle (W O) without noise and sentence-le v el or -', ' acle (SO) with noise. In training, BLEU score', ' on the v alidation set is used to select the best', ' model, a detailed comparison among the BLEU', ' score curv es under dif ferent f actors is sho wn in', ' Figure', ' 5', ' . RNNsearch con v er ges f ast and achie v es', ' the best result at the', ' 7', ' -th epoch, while the train-', ' ing loss continues to decline after the', ' 7', ' -th epoch', ' until the end. Thus, the training of RNNsearch', ' may encounter the o v problem. Figure', ' 4', ' and', ' 5', ' also re v eal that, inte grating t he oracle sam-', ' pling and the Gumbel noise leads to a little slo wer', ' con v er gence and the training loss does not k eep', ' decreasing after the best results appear on the v al-', ' idation set. This is consistent with our intuition', ' that oracle sampling and noises can a v oid o v', ' Figure 6: T rends of BLEU scores on the MT03 test set', ' with dif ferent f actors on the Zh', ' !', ' En translation task.', ' ting despite needs a longer time to con v er ge.', ' Figure', ' 6', ' sho ws the BLEU scores curv es on the', ' MT03 test set under dif ferent f actors', ' 4', ' . When sam-', ' pling oracles with noise (', ' ˝', ' =', ' 0', ' :', ' 5', ' ) on the sentence', ' le v el, we obtain the best model. W ithout noise,', ' our system con v er ges to a lo wer BLEU score. This', ' can be understood easily that using its o wn re-', ' sults repeatedly during training without an y re g-', ' ularization will l ead to o v and quick con-', ' v er gence. In this sense, our method from', ' the sentence-le v el sampling and Gumbel noise.', ' 5.6 About Length', ' Figure', ' 7', ' sho ws the BLEU scores of generated', ' translations on the MT03 test set with respect to', ' the lengths of the source sentences. In partic-', ' ular , we split the translations for the MT03 test', ' set into dif ferent bins according to the length of', ' source sentences, then test the BLEU scores for', ' translations in each bin separately with the results', ' reported in Figure', ' 7', ' . Our approach can achie v e', ' big impro v ements o v er the baseline system in all', ' bins, especially in the bins (', ' 10', ' ,', ' 20', ' ], (', ' 40', ' ,', ' 50', ' ] and', ' (', ' 70', ' ,', ' 80', ' ] of the super -long sentences. The cross-', ' entrop y loss requires that the predicted sequence', ' is e xactly the same as the ground truth sequence', ' which is more dif to achie v e for long sen-', ' tences, while our sentence-le v el oracle can help', ' reco v er from this kind of o v ercorrection.', ' 5.7 Effect on Exposur e Bias', ' T o v alidate whether the impro v ements is mainly', ' obtained by addressing the e xposure bias prob-', ' lem, we randomly select', ' 1', ' K sentence pairs from', ' 4', ' Note that the ﬁSOﬂ model without noise is trained based', ' on the pre-trained RNNsearch model (as sho wn by the red', ' dashed lines in Figure', ' 5', ' and', ' 6', ' ).', ' Figure 7: Performance comparison on the MT03 test', ' set with respect to the dif ferent lengths of source sen-', ' tences on the Zh', ' !', ' En translation task.', ' the Zh', ' !', ' En training data, and use the pre-trained', ' RNNSearch model and proposed model to de-', ' code the source sentences. The BLEU score of', ' RNNSearch model w as', ' 24', ' :', ' 87', ' , while our model', ' produced +', ' 2', ' :', ' 18', ' points. W e then count the ground', ' truth w ords whose probabilities in the predicted', ' distrib utions produced by our model are greater', ' than those produced by the baseline model, and', ' mark the number as', ' N', ' . There are totally', ' 28', ' ;', ' 266', ' gold w ords in the references, and', ' N', ' =', ' 18', ' ;', ' 391', ' .', ' The proportion is', ' 18', ' ;', ' 391', ' =', ' 28', ' ;', ' 266', ' =', ' 65', ' :', ' 06%', ' , which', ' could v erify the impro v ements are mainly ob-', ' tained by addressing the e xposure bias problem.', ' 5.8 Results on En', ' !', ' De T ranslation', ' Systems', ' newstest2014', ' RNNsearch', ' 25', ' :', ' 82', ' + SS-NMT', ' 26', ' :', ' 50', ' + MIXER', ' 26', ' :', ' 76', ' + OR-NMT', ' 27.41', ' z', ' T ransformer (base)', ' 27', ' :', ' 34', ' + SS-NMT', ' 28', ' :', ' 05', ' + MIXER', ' 27', ' :', ' 98', ' + OR-NMT', ' 28.65', ' z', ' T able 3: Case-sensiti v e BLEU scores (%) on En', ' !', ' De', ' task. The ﬁ', ' z', ' ﬂ indicates the r esults are bet-', ' ter (p', ' <', ' 0.01) than RNNsearch and T ransformer .', \" W e also e v aluate our approach on the WMT'14\", ' benchmarks on the En', ' !', ' De translation task. From', ' the results listed in T able', ' 3', ' , we conclude that', ' the proposed method outperforms the', ' competiti v e baseline model as well as related ap-', ' proaches. Similar with results on the Zh', ' !', ' En task,', ' both scheduled sampling and MIXER could im-', ' pro v e the tw o baseline systems. Our method im-', ' pro v es the RNNSearch and T ransformer baseline', ' models by +', ' 1', ' :', ' 59', ' and +', ' 1', ' :', ' 31', ' BLEU points respec-', ' ti v ely . These res ults demonstrate that our model', ' w orks well across dif ferent language pairs.', ' 6 Conclusion', ' The end-to-end NMT model generates a transla-', ' tion w ord by w ord with the ground truth w ords', ' as conte xt at training time as opposed to the pre-', ' vious w ords generated by the model as conte xt', ' at inference. T o mitig ate the discrepanc y be-', ' tween training and inference, when predicting one', ' w ord, we feed as conte xt either the ground truth', ' w ord or the pre vious predicted w ord with a sam-', ' pling scheme. The predicted w ords, ref erred to', ' as oracle w ords, can be generated with the w ord-', ' le v el or sentence-le v el optimization. C ompared to', ' w ord-le v el oracle, sentence-le v el oracle can fur -', ' ther equip the model with the ability of o v ercor -', ' rection reco v ery . T o mak e the model fully e x-', ' posed to the circumstance at reference, we sam-', ' ple the conte xt w ord with decay from the ground', ' truth w ords. W e v the ef fecti v eness of our', ' method with tw o strong baseline models and re-', ' lated w orks on the real translation tasks, achie v ed', ' impro v ement on all the datasets. W e', ' also conclude that the sentence-le v el oracle sho w', ' superiority o v er the w ord-le v el oracle.', ' Ackno wledgments', ' W e thank the three anon ymous re vie wers for', ' their v aluable suggestions. This w ork w as sup-', ' ported by National Natural Science F oundation', ' of China (NO. 61662077, NO. 61876174) and', ' National K e y R&D Program of China (NO.', ' YS2017YFGH001428).', ' Refer ences', ' Dzmitry Bahdanau, K yungh yun Cho, and Y oshua Ben-', ' gio. 2015. Ne u r al machine translation by jointly', ' learning to align and translate.', ' ICLR 2015', ' .', ' Samy Bengi o , Oriol V in yals, Na vdeep Jaitly , and', ' Noam Shazeer . 2015.', ' Scheduled sampling for', ' sequence prediction with recurrent neural net-', ' w orks', ' . In C. Cortes, N. D. La wrence, D. D. Lee,', ' M. Sugiyama, and R. Garnett, editors,', ' Advances in', ' Neur al Information Pr ocessing Systems 28', ' , pages', ' 1171Œ1179. Curran Associates, Inc.', ' K yungh yun Cho, Bart v an Merrienboer , Caglar Gul-', ' cehre, Dzmitry Bahdanau, Fethi Boug ares, Holger', ' Schwenk, and Y oshua Bengio. 2014.', ' Learning', ' phrase representations using rnn encoder Œdecoder', ' for statistical machine translation', ' . In', ' Pr oceedings of', ' the 2014 Confer ence on Empirical Methods in Nat-', ' ur al Langua g e Pr ocessing (EMNLP)', ' , pages 1724Œ', ' 1734, Doha, Qatar . Association for Computational', ' Linguistics.', ' Michael Collins, Philipp K oehn, and Iv ona K ucero v a.', ' 2005.', ' Clause restructuring for statistical machine', ' translation', ' . In', ' Pr oceedings of the 43r d Annual', ' Meeting of the Association for Computational Lin-', \" guistics (A CL '05)\", ' , pages 531Œ540, Ann Arbor ,', ' Michig an. Association for Computational Linguis-', ' tics.', ' Jonas Gehring, Michael Auli, Da vid Grangier , Denis', ' Y arats, and Y ann N. Dauphin. 2017.', ' Con v olutional', ' sequence to sequence learni ng', ' . In', ' Pr oceedings', ' of the 34th International Confer ence on Mac hine', ' Learning', ' , v olum e 70 of', ' Pr oceedings of Mac hine', ' Learning Resear c h', ' , pages 1243Œ1252, International', ' Con v ention Centre, Sydne y , Australia. PMLR.', ' Emil Julius Gumbel. 1954. Statistical theory of e x-', ' treme v aluse and some practical applications.', ' Nat.', ' Bur . Standar ds Appl. Math. Ser . 33', ' .', ' Chin-Y e w Lin. 2004. Rouge: A package for automatic', ' e v aluation of summaries. In', ' T e xt Summarization', ' Br anc hes Out: Pr oceedings of the A CL-04 W ork-', ' shop', ' , pages 74Œ81, Barcelona, Spain. Association', ' for Computational Linguistics.', ' Chris J Maddison, Daniel T arlo w , and T om Minka.', ' 2014.', ' A* sampling', ' . In Z. Ghahramani, M. W elling,', ' C. Cortes, N. D. La wrence, and K. Q. W einber ger ,', ' editors,', ' Advances in Neur al Information Pr ocessing', ' Systems 27', ' , pages 3086Œ3094. Curran Associates,', ' Inc.', ' F andong Meng and Jinchao Zhang. 2019. Dtmt: A', ' no v el deep transition architecture for neural ma-', ' chine translation. In', ' Pr oceedings of the Thirty-', ' Thir d AAAI Confer ence on Intellig ence', ' ,', \" AAAI'19. AAAI Press.\", ' Kishore P apineni, Salim Rouk os, T odd W ard, and W ei-', ' Jing Zhu. 2002. Bleu: a method for automatic e v al-', ' uation of machine translation. In', ' Pr oceedings of', ' the 40th annual meeting on association for compu-', ' tational linguistics', ' , pages 311Œ318. Association for', ' Computational Linguistics.', \" Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli,\", ' and W ojciech Zaremba. 2015. Sequence le v el train-', ' ing with recurrent neural netw orks.', ' arXiv pr eprint', ' arXiv:1511.06732', ' .', ' Rico Sennrich, Barry Haddo w , and Ale xandra Birch.', ' 2016.', ' Neural machine translation of rare w ords', ' with subw ord units', ' . In', ' Pr oceedings of the 54th An-', ' nual Meeting of the Ass ociation for Computational', ' Linguistics (V olume 1: Long P aper s)', ' , pages 1715Œ', ' 1725, Berlin, German y . Association for Computa-', ' tional Linguistics.', ' Chenze Shao, Xilin Chen, and Y ang Feng. 2018.', ' Greedy search with probabilistic n-gram matching', ' for neural machine translation. In', ' Pr oceedings of', ' the 2018 Confer ence on Empirical Methods in Nat-', ' ur al Langua g e Pr ocessing', ' , pages 4778Œ4784.', ' Shiqi Shen, Y ong Cheng, Zhongjun He, W ei He, Hua', ' W u, Maosong Sun, and Y ang Liu. 2016. Minimum', ' risk training for neural machine translation. In', ' Pr o-', ' ceedings of the 54th Annual Meeting of the Associa-', ' tion for Computational Linguistics (V olume 1: Long', ' P aper s)', ' , v olume 1, pages 1683Œ1692.', ' Ilya Sutsk e v er , Oriol V in yals, and Quoc V Le. 2014.', ' Sequence to sequence learning with neural net-', ' w orks', ' . In Z. Ghahramani, M. W elling, C. Cortes,', ' N. D. La wrence, and K. Q. W einber ger , editors,', ' Ad-', ' vances in Neur al Information Pr ocessing Systems', ' 27', ' , pages 3104Œ3112. Curran Associates, Inc.', ' Zhaopeng T u, Zhengdong Lu, Y ang Liu, Xiaohua Liu,', ' and Hang Li. 2016. Modeling co v erage for neural', ' machine translation. In', ' Pr oceedings of A CL', ' .', ' Ashish V asw ani, Noam Shazeer , Niki P armar , Jak ob', ' Uszk oreit, Llion Jones, Aidan N Gomez, ukasz', ' Kaiser , and Illia Polosukhin. 2017.', ' Attention is all', ' you need', ' . In I. Guyon, U. V . Luxb ur g, S. Bengio,', ' H. W allach, R. Fer gus, S. V ishw anathan, and R. Gar -', ' nett, editors ,', ' Advances in Neur al Information Pr o-', ' cessing Systems 30', ' , pages 5998Œ6008. Curran As-', ' sociates, Inc.', ' Arun V enkatraman, Mar tial Hebert, and J. Andre w', ' Bagnell. 2015.', ' Impro ving multi-step prediction of', ' learned time series models', ' . In', ' Pr oceedings of the', ' T wenty-Ninth AAAI Confer ence on Intelli-', ' g ence', \" , AAAI'15, pages 3024Œ3030. AAAI Press.\", ' Y onghui W u, Mik e Schuster , Zhifeng C h e n, Quoc V', ' Le, Mohammad Norouzi, W olfg ang Machere y ,', ' Maxim Krikun, Y uan Cao, Qin Gao, Klaus', \" Machere y , et al. 2016. Google' s neural ma-\", ' chine translation system: Bridging the g ap between', ' human and machine translation.', ' arXiv pr eprint', ' arXiv:1609.08144', ' .', ' Matthe w D Zeiler . 2012. Adadelta: an adapti v e learn-', ' ing rate method.', ' arXiv pr eprint arXiv:1212.5701', ' .', ' Jinchao Zhang, Mi ngxu a n W ang, Qun Liu, and Jie', ' Zhou. 2017. Incorporating w ord reordering kno wl-', ' edge into atte n t ion-based neural machine transla-', ' tion. In', ' Pr oceedings of A CL', ' .', '']\n"
     ]
    }
   ],
   "source": [
    "replaced_data = text.replace('\\t','\\n').split('\\n')\n",
    "print(replaced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AbsSayem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\AbsSayem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\AbsSayem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\AbsSayem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12452/2277344633.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplaced_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mne_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'label'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m    106\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1274\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1275\u001b[0m         \"\"\"\n\u001b[1;32m-> 1276\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1278\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m         \"\"\"\n\u001b[1;32m-> 1332\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_match_potential_end_contexts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m         \"\"\"\n\u001b[1;32m-> 1332\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_match_potential_end_contexts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m             \u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1419\u001b[0m         \"\"\"\n\u001b[0;32m   1420\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1421\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msentence1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1422\u001b[0m             \u001b[0msentence1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msentence2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1395\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_match_potential_end_contexts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1397\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1373\u001b[0m         \u001b[0mbefore_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m         \u001b[0mmatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1376\u001b[0m             \u001b[1;31m# Ignore matches that have already been captured by matches to the right of this match\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmatches\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbefore_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "for sent in nltk.sent_tokenize(replaced_data[2]):\n",
    "      for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                  print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88bd64abd146d54e091ff1398670d31add9e50ee4691cdc8ffb9431c01087a21"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
