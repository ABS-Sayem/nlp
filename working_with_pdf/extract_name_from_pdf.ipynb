{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exract All Names from a PDF File\n",
    "*In this tutorial we will build a pipeline of extracting all the names from a pdf file. Here we will go through the following steps-*\n",
    "\n",
    "*> Load the PDF File*\n",
    "\n",
    "*> Extract the Text*\n",
    "\n",
    "*> Make a Text File (Optional)*\n",
    "\n",
    "*> Data Preprocessing*\n",
    "\n",
    "*> Extact Names*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the PDF File and Extarct Text\n",
    "*To load the pdf file we will use *****PdfFileReader*****, is a default module of *****PyPDF2*****. After load the pdf file we will extract the text from the pdf file. We can do the both load_the_pdf and extract_text together.*\n",
    "\n",
    "*To do so, we will first import the libraries for loading pdf and extracting text form the file. If you don't install the pypdf2, open the terminal and use the command-*\n",
    "\n",
    "*> \"pip install pypdf2\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyPDF2 and its module\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To extract the text we need to know how many pages are there in the pdf file. Then for every pages we can extract the text data and make a string adding all the extracted text.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pdf file\n",
    "my_file = PdfFileReader(\"Bridging_The_Gap_Between_Training_&_Inference_For_Neural_Machine_Translation.pdf\")\n",
    "#print(my_file.getNumPages())\n",
    "str = \"\"\n",
    "for i in range(10):\n",
    "    str += my_file.getPage(i).extractText()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make a Text File\n",
    "*Text file will help us in further processing. We already created a text file namely *****my_file.txt*****. Once created, you can just open the file whenever you want. Usually, the text file will saved in the current directory.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a text file\n",
    "#with open(\"my_file.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#    f.write(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the text file\n",
    "with open(\"my_file.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing\n",
    "*Before start preprocessing, we split the data in tokens and replaced all the tabs with a new line. Python *****split()***** method seperate the text by tokens and make a list of them.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Abs_Sayem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the libraries\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing tab(\\t) into newline(\\n)\n",
    "replaced_data = text.replace('\\t','\\n').split('\\n')\n",
    "# Convert list to string\n",
    "string_data = \" \".join(map(str, replaced_data))\n",
    "# Remove numbers\n",
    "nonnumbered_text = re.sub(r'\\d+', '', string_data)\n",
    "# Remove Punctuation\n",
    "s = set(string.punctuation)\n",
    "tokenized_text = word_tokenize(nonnumbered_text)\n",
    "#print(tokenized_text)\n",
    "filtered_text = []\n",
    "for i in tokenized_text:\n",
    "    if i not in s:\n",
    "        filtered_text.append(i)\n",
    "# Remove Stopwords\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "stopped_text = [word.lower() for word in filtered_text if word.lower() not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parsing Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Using nameparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nameparser.parser import HumanName\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_list = []\n",
    "person_names=person_list\n",
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #avoid grabbing lone surnames\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "#     print (person_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to string\n",
    "new_string_data = \" \".join(map(str, stopped_text))\n",
    "#print(string_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "names = get_human_names(new_string_data)\n",
    "for person in person_list:\n",
    "    person_split = person.split(\" \")\n",
    "    for name in person_split:\n",
    "        if wordnet.synsets(name):\n",
    "            if(name in person):\n",
    "                person_names.remove(person)\n",
    "                break\n",
    "\n",
    "print(person_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Using NERTragger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NERTagger' from 'nltk.tag.stanford' (C:\\Users\\Abs_Sayem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tag\\stanford.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-fbe08837beaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstanford\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNERTagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'NERTagger' from 'nltk.tag.stanford' (C:\\Users\\Abs_Sayem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tag\\stanford.py)"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import NERTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in nltk.sent_tokenize(new_string_data):\n",
    "      for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                  print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = NERTagger('stanford-ner/all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')\n",
    "#text = \"\"\"YOUR TEXT GOES HERE\"\"\"\n",
    "\n",
    "for sent in nltk.sent_tokenize(new_string_data):\n",
    "    tokens = nltk.tokenize.word_tokenize(sent)\n",
    "    tags = st.tag(tokens)\n",
    "    for tag in tags:\n",
    "        if tag[1]=='PERSON':\n",
    "            print(tag)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "703b2bc398e15cd74c7a93565a102b3060db76bb218c5bf5b3619260b8a02446"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
