{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pypdf2\n",
    "# Open the terminal and run the command \"pip install pypdf2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyPDF2 and its modules\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pdf file\n",
    "# It will return an object\n",
    "def load_pdf(file_name):\n",
    "    import PyPDF2\n",
    "    from PyPDF2 import PdfFileReader\n",
    "    \n",
    "    my_file = PdfFileReader(f\"{file_name}\")\n",
    "    return(my_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Text from pdf\n",
    "def getPdfText(file_name, page_range):\n",
    "    import PyPDF2\n",
    "    from PyPDF2 import PdfFileReader\n",
    "    \n",
    "    my_file = PdfFileReader(f\"{file_name}\")\n",
    "    no_page = my_file.getNumPages()\n",
    "    if(page_range <= no_page):\n",
    "        page_num = page_range\n",
    "    else:\n",
    "        page_num = no_page\n",
    "    file_text = \"\"\n",
    "    for i in range(page_num):\n",
    "        file_text += my_file.getPage(i).extractText()\n",
    "    return(file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a text file from pdf file\n",
    "def makeTxt2Pdf(file_name, page_range):\n",
    "    import PyPDF2\n",
    "    from PyPDF2 import PdfFileReader\n",
    "    \n",
    "    my_file = PdfFileReader(f\"{file_name}\")\n",
    "    no_page = my_file.getNumPages()\n",
    "    if(page_range <= no_page):\n",
    "        page_num = page_range\n",
    "    else:\n",
    "        page_num = no_page\n",
    "    file_text = \"\"\n",
    "    for i in range(page_num):\n",
    "        file_text += my_file.getPage(i).extractText()\n",
    "    # making txt file\n",
    "    with open(\"text_file.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the text file\n",
    "with open(\"my_file.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_data = text.replace('\\t','\\n').split('\\n')\n",
    "#replaced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to string\n",
    "string_data = \" \".join(map(str, replaced_data))\n",
    "#print(string_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numbers\n",
    "import re\n",
    "nonnumbered_text = re.sub(r'\\d+', '', string_data)\n",
    "#print(nonnumbered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = set(string.punctuation)\n",
    "tokenized_text = word_tokenize(nonnumbered_text)\n",
    "#print(tokenized_text)\n",
    "filtered_text = []\n",
    "for i in tokenized_text:\n",
    "    if i not in s:\n",
    "        filtered_text.append(i)\n",
    "#print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download(\"stopwords\")\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "stopped_text = [word.lower() for word in filtered_text if word.lower() not in stop]\n",
    "#print(stopped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nameparser.parser import HumanName\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_list = []\n",
    "person_names=person_list\n",
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #avoid grabbing lone surnames\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "#     print (person_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to string\n",
    "string_data1 = \" \".join(map(str, stopped_text))\n",
    "#print(string_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D A', 'Systems Ar', 'De T', 'Dzmitry Bahdanau', 'Samy Bengi', 'Noam Shazeer', 'Fethi Boug', 'Holger Schwenk', 'Ann Arbor', 'Jonas Gehring', 'Emil Julius Gumbel', 'Jinchao Zhang', 'Sumit Chopra', 'Barry Haddo', 'Chenze Shao', 'Shiqi Shen', 'Ilya Sutsk', 'Xiaohua Liu', 'Llion Jones', 'Illia Polosukhin', 'Qin Gao', 'Klaus Machere', 'Qun Liu']\n"
     ]
    }
   ],
   "source": [
    "names = get_human_names(string_data1)\n",
    "for person in person_list:\n",
    "    person_split = person.split(\" \")\n",
    "    for name in person_split:\n",
    "        if wordnet.synsets(name):\n",
    "            if(name in person):\n",
    "                person_names.remove(person)\n",
    "                break\n",
    "\n",
    "print(person_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "import nltk\n",
    "#nltk.download()\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in nltk.sent_tokenize(string_data1):\n",
    "      for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                  print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NERTagger' from 'nltk.tag.stanford' (C:\\Users\\Abs_Sayem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tag\\stanford.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-34d6059754dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstanford\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNERTagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNERTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stanford-ner/all.3class.distsim.crf.ser.gz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stanford-ner/stanford-ner.jar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#text = \"\"\"YOUR TEXT GOES HERE\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'NERTagger' from 'nltk.tag.stanford' (C:\\Users\\Abs_Sayem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tag\\stanford.py)"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import NERTagger\n",
    "st = NERTagger('stanford-ner/all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')\n",
    "#text = \"\"\"YOUR TEXT GOES HERE\"\"\"\n",
    "\n",
    "for sent in nltk.sent_tokenize(string_data1):\n",
    "    tokens = nltk.tokenize.word_tokenize(sent)\n",
    "    tags = st.tag(tokens)\n",
    "    for tag in tags:\n",
    "        if tag[1]=='PERSON':\n",
    "            print(tag)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ad1a662050c36948a62a2159e528a078f0bb94ec7adadaee9eccaf8b42424e4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
