{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Info from PDF\n",
    "*PDFs are a proprietory format of adobe and it is not easy when it comes to automating the process of extracting information from pdf file. Our aim is to extract required information from any pdf file.*\n",
    "\n",
    "*In this tutorial we will use some outstanding python libraries to do our work.*\n",
    "\n",
    "*> *****PyPDF2:***** to convert simple, text-based pdf files into text readable*\n",
    "\n",
    "*> *****textract:***** to convert non-trival, scanned pdf files into text readable*\n",
    "\n",
    "*> *****NLTK:***** to clean and convert phrases into keywords*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pypdf2 - \"pip install pypdf2\"\n",
    "# Install textract - \"pip install textract\"\n",
    "# Install NLTK - \"pip install nltk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyPDF2 and its modules\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pdf file\n",
    "# It will return an object\n",
    "def loadPdf(file_name):\n",
    "    #import PyPDF2\n",
    "    #from PyPDF2 import PdfFileReader\n",
    "    \n",
    "    my_file = PdfFileReader(f\"{file_name}\")\n",
    "    return(my_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Text from pdf\n",
    "def getPdfText(file_name, page_range):\n",
    "    #import PyPDF2\n",
    "    #from PyPDF2 import PdfFileReader\n",
    "    \n",
    "    my_file = PdfFileReader(f\"{file_name}\")\n",
    "    no_page = my_file.getNumPages()\n",
    "    if(page_range <= no_page):\n",
    "        page_num = page_range\n",
    "    else:\n",
    "        page_num = no_page\n",
    "    file_text = \"\"\n",
    "    for i in range(page_num):\n",
    "        file_text += my_file.getPage(i).extractText()\n",
    "    return(file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a text file from pdf file\n",
    "def makeTxt2Pdf(file_name, page_range):\n",
    "    #import PyPDF2\n",
    "    #from PyPDF2 import PdfFileReader\n",
    "    \n",
    "    my_file = PdfFileReader(f\"{file_name}\")\n",
    "    no_page = my_file.getNumPages()\n",
    "    if(page_range <= no_page):\n",
    "        page_num = page_range\n",
    "    else:\n",
    "        page_num = no_page\n",
    "    file_text = \"\"\n",
    "    for i in range(page_num):\n",
    "        file_text += my_file.getPage(i).extractText()\n",
    "    # making txt file\n",
    "    with open(\"text_file.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bridging the Gap between T raining and Infer ence\n",
      " f or Neural Machine T ranslation\n",
      " W en Zhang\n",
      " 1\n",
      " ;\n",
      " 2\n",
      " Y ang F eng\n",
      " 1\n",
      " ;\n",
      " 2\n",
      "\n",
      " F andong Meng\n",
      " 3\n",
      " Di Y ou\n",
      " 4\n",
      " Qun Liu\n",
      " 5\n",
      " 1\n",
      " K e y Laboratory of Intelligent Information Processing\n",
      " Institute of Computing T echnology , Chinese Academy of Sciences (ICT/CAS)\n",
      " 2\n",
      " Uni v ersity of Chinese Academy of Sciences, Beijing, China\n",
      " f\n",
      " zhangwen\n",
      " ,\n",
      " fengyang\n",
      " g\n",
      " @\n",
      " ict.ac.cn\n",
      " 3\n",
      " P attern Recognition Center , W eChat AI, T encent Inc, China\n",
      " fandongmeng\n",
      " @\n",
      " tencent.com\n",
      " 4\n",
      " W orcester Polytechnic Institute, W orcester , MA, USA\n",
      " dyou\n",
      " @\n",
      " wpi.edu\n",
      " 5\n",
      " Hua wei Noah' s Ark Lab, Hong K ong, China\n",
      " qun.liu\n",
      " @\n",
      " huawei.com\n",
      " Abstract\n",
      " Neural Machine T ranslation (NMT) generates\n",
      " tar get w ords sequentially in the w ay of pre-\n",
      " dicting the ne xt w ord conditioned on the con-\n",
      " te xt w ords. At training time, it predicts with\n",
      " the ground truth w ords as conte xt whil e at in-\n",
      " ference it has to generate the entire sequence\n",
      " from scratch. This discrepanc y of the fed con-\n",
      " te xt leads to error accumulation among the\n",
      " w ay . Furthermore, w ord-le v el training re-\n",
      " quires strict matching between the generated\n",
      " sequence and the ground truth sequence which\n",
      " leads to o v ercorrection o v er dif ferent b ut rea-\n",
      " sonable translations. In this paper , we ad-\n",
      " dress these issues by sampling conte xt w ords\n",
      " not only from the ground truth sequence b ut\n",
      " also from the predicted sequence by the model\n",
      " during training, where the predicted sequence\n",
      " is selected with a sentence-le v el optimum.\n",
      " Experiment results on Chi n e se\n",
      " !\n",
      " English and\n",
      " WMT'14 English\n",
      " !\n",
      " German translation tasks\n",
      " demonstrate that our approach can achie v e sig-\n",
      " impro v ements on multiple datasets.\n",
      " 1 Intr oduction\n",
      " Neural Machine T ranslation has sho wn promising\n",
      " results and dra wn more attention recently . Most\n",
      " NMT models in the encoder -decoder frame-\n",
      " w ork, including the RNN-based (\n",
      " Sutsk e v er et al.\n",
      " ,\n",
      " 2014\n",
      " ;\n",
      " Bahdanau et al.\n",
      " ,\n",
      " 2015\n",
      " ;\n",
      " Meng and Zhang\n",
      " ,\n",
      " 2019\n",
      " ), the CNN-based (\n",
      " Gehring et al.\n",
      " ,\n",
      " 2017\n",
      " ) and\n",
      " the attention-based (\n",
      " V asw ani et al.\n",
      " ,\n",
      " 2017\n",
      " ) mod-\n",
      " els, which predict the ne xt w ord conditioned on\n",
      " the pre vious conte xt w ords, de ri ving a language\n",
      " model o v er tar get w ords. The scenario is at train-\n",
      " ing time the ground truth w ords are used as conte xt\n",
      "\n",
      " Corresponding author .\n",
      " while at inferenc e the entire sequence is generated\n",
      " by the resulting model on its o wn and hence the\n",
      " pre vious w ords generated by the model are fed as\n",
      " conte xt. As a result, the predicted w ords at train-\n",
      " ing and inference are dra wn from dif ferent dis-\n",
      " trib utions, namely , from the data distrib ution as\n",
      " opposed to the model distrib ution. This discrep-\n",
      " anc y , called\n",
      " e xposur e bias\n",
      " (\n",
      " Ranzato et al.\n",
      " ,\n",
      " 2015\n",
      " ),\n",
      " leads to a g ap between training and inference. As\n",
      " the tar get sequence gro ws, the errors accumulate\n",
      " among the sequence and the model has to predict\n",
      " under the condition it has ne v er met at training\n",
      " time.\n",
      " Intuiti v ely , to address this problem, the model\n",
      " should be trained to predict under the same con-\n",
      " dition it will f ace at inference. Inspired by D\n",
      " A T A\n",
      " A\n",
      " S\n",
      " D\n",
      " E M O N S T R A T O R\n",
      " ( D\n",
      " A\n",
      " D ) (\n",
      " V enkatraman et al.\n",
      " ,\n",
      " 2015\n",
      " ), feeding as conte xt both ground truth w ords\n",
      " and the predicted w ords during training can be\n",
      " a solution. NMT m odels usually optimize the\n",
      " cross-entrop y loss which requires a strict pairwise\n",
      " matching at the w ord le v el between the predicted\n",
      " sequence and the ground truth sequence. Once\n",
      " the model generates a w ord de viating from the\n",
      " ground truth sequence, the cross-entrop y loss will\n",
      " correct the error immediately and dra w the re-\n",
      " maining generation back to the ground truth se-\n",
      " quence. Ho we v er , this causes a ne w problem. A\n",
      " sentence usually has multiple reasonable transla-\n",
      " tions and it cannot be said that the model mak es a\n",
      " mistak e e v en if it generates a w ord dif ferent from\n",
      " the ground truth w ord. F or e xample,\n",
      " r efer ence\n",
      " : W e should comply with the rule.\n",
      " cand1\n",
      " : W e should abide with the rule.\n",
      " cand2\n",
      " : W e should abide by the la w .\n",
      " cand3\n",
      " : W e should abide by the rule.\n",
      "arXiv:1906.02448v2  [cs.CL]  17 Jun 2019\n",
      " once the model generates ﬁabideﬂ a s the third\n",
      " tar get w ord, the cross-entrop y loss w ould force\n",
      " the model to generate ﬁwithﬂ as the fourth w ord\n",
      " (as\n",
      " cand1\n",
      " ) so as to produce lar ger sentence-le v el\n",
      " lik elihood and be in line with the reference,\n",
      " although ﬁbyﬂ is the right choice. Then, ﬁwithﬂ\n",
      " will be fed as conte xt to generate ﬁthe ruleﬂ, as\n",
      " a result, the model is taught to generate ﬁabide\n",
      " with the ruleﬂ which actually is wrong. The\n",
      " translation\n",
      " cand1\n",
      " can be treated as\n",
      " o ver corr ection\n",
      " phenomenon. Another potential error is that e v en\n",
      " the model predicts the right w ord ﬁbyﬂ follo wing\n",
      " ﬁabideﬂ, when generating subsequent translation,\n",
      " it may produce ﬁthe la wﬂ improperly by feeding\n",
      " ﬁbyﬂ (as\n",
      " cand2\n",
      " ). Assume the references and the\n",
      " training criterion let the model memorize the\n",
      " pattern of the phrase ﬁthe ruleﬂ al w ays follo wing\n",
      " the w ord ﬁw ithﬂ, to help the model reco v er from\n",
      " the tw o kinds of errors and create the correct\n",
      " translation lik e\n",
      " cand3\n",
      " , we should feed ﬁwithﬂ as\n",
      " conte xt rather than ﬁbyﬂ e v en when the pre vious\n",
      " predicted phrase is ﬁabide byﬂ. W e refer to this\n",
      " solution as\n",
      " Over corr ection Reco very\n",
      " (\n",
      " OR\n",
      " ).\n",
      " In this paper , we present a method to bridge the\n",
      " g ap between training and inference and impro v e\n",
      " the o v ercorrection reco v ery capability of NMT .\n",
      " Our method selects\n",
      " or acle\n",
      " w ords from its pre-\n",
      " dicted w ords and then samples as conte xt from the\n",
      " oracle w ords and ground truth w ords. Meanwhile,\n",
      " the oracle w ords are selected not only with a w ord-\n",
      " by-w ord greedy search b ut also with a sentence-\n",
      " le v el e v aluation, e.g. BLEU, which allo ws greater\n",
      " xibility under the pairwise matching restriction\n",
      " of cross-entrop y . At the be ginning of training, the\n",
      " model selects as conte xt ground truth w ords at a\n",
      " greater probability . As the model con v er ges grad-\n",
      " ually , oracle w ords are chos en as conte xt more\n",
      " often. In this w ay , the training process changes\n",
      " from a fully guided scheme to w ards a less guided\n",
      " scheme. Under this mechanism, the model has the\n",
      " chance to learn to handle the mistak es made at in-\n",
      " ference and also has the ability to reco v er from\n",
      " o v ercorrection o v er alternati v e translations. W e\n",
      " v erify our approach on both the RNNsearch model\n",
      " and the stronger T ransformer model. The results\n",
      " sho w that our approach can impro v e\n",
      " the performance on both models.\n",
      " 2 RNN-based NMT Model\n",
      " Our method can be applied in a v ariety of NMT\n",
      " models. W ithout loss of generality , we tak e the\n",
      " RNN-based NMT (\n",
      " Bahdanau et al.\n",
      " ,\n",
      " 2015\n",
      " ) as an\n",
      " e xample to introduce our method. Assume the\n",
      " source sequence and the observ ed translation are\n",
      " x\n",
      " =\n",
      " f\n",
      " x\n",
      " 1\n",
      " ;\n",
      "\n",
      " ; x\n",
      " j\n",
      " x\n",
      " j\n",
      " g\n",
      " and\n",
      " y\n",
      "\n",
      " =\n",
      " f\n",
      " y\n",
      "\n",
      " 1\n",
      " ;\n",
      "\n",
      " ; y\n",
      "\n",
      " j\n",
      " y\n",
      "\n",
      " j\n",
      " g\n",
      " .\n",
      " Encoder .\n",
      " A bidirectional Gated Recurrent Unit\n",
      " (GR U) (\n",
      " Cho et al.\n",
      " ,\n",
      " 2014\n",
      " ) is used to acquire tw o\n",
      " sequences of hidden states, the annotation of\n",
      " x\n",
      " i\n",
      " is\n",
      " h\n",
      " i\n",
      " = [\n",
      " !\n",
      " h\n",
      " i\n",
      " ;\n",
      "  \n",
      " h\n",
      " i\n",
      " ]\n",
      " . Note that\n",
      " e\n",
      " x\n",
      " i\n",
      " is emplo yed to\n",
      " represent the embedding v ector of the w ord\n",
      " x\n",
      " i\n",
      " .\n",
      " !\n",
      " h\n",
      " i\n",
      " =\n",
      " GR U\n",
      " (\n",
      " e\n",
      " x\n",
      " i\n",
      " ;\n",
      " !\n",
      " h\n",
      " i\n",
      "\n",
      " 1\n",
      " )\n",
      " (1)\n",
      "  \n",
      " h\n",
      " i\n",
      " =\n",
      " GR U\n",
      " (\n",
      " e\n",
      " x\n",
      " i\n",
      " ;\n",
      "  \n",
      " h\n",
      " i\n",
      " +1\n",
      " )\n",
      " (2)\n",
      " Attention.\n",
      " The attention is designed to e xtract\n",
      " source information (calle d source conte xt v ector).\n",
      " At the\n",
      " j\n",
      " -th step, the rele v ance between the tar get\n",
      " w ord\n",
      " y\n",
      "\n",
      " j\n",
      " and the\n",
      " i\n",
      " -th source w ord is e v aluated and\n",
      " normalized o v er the source sequence\n",
      " r\n",
      " ij\n",
      " =\n",
      " v\n",
      " T\n",
      " a\n",
      " tanh (\n",
      " W\n",
      " a\n",
      " s\n",
      " j\n",
      "\n",
      " 1\n",
      " +\n",
      " U\n",
      " a\n",
      " h\n",
      " i\n",
      " )\n",
      " (3)\n",
      "\n",
      " ij\n",
      " =\n",
      " exp (\n",
      " r\n",
      " ij\n",
      " )\n",
      " P\n",
      " j\n",
      " x\n",
      " j\n",
      " i\n",
      " 0\n",
      " =1\n",
      " exp\n",
      "\n",
      " r\n",
      " i\n",
      " 0\n",
      " j\n",
      "\n",
      " (4)\n",
      " The source conte xt v ector is the weighted sum of\n",
      " all source annotations and can be calculated by\n",
      " c\n",
      " j\n",
      " =\n",
      " X\n",
      " j\n",
      " x\n",
      " j\n",
      " i\n",
      " =1\n",
      "\n",
      " ij\n",
      " h\n",
      " i\n",
      " (5)\n",
      " Decoder .\n",
      " The decoder emplo ys a v ariant of\n",
      " GR U to unroll the tar get information. At the\n",
      " j\n",
      " -th\n",
      " step, the tar get hidden state\n",
      " s\n",
      " j\n",
      " is gi v en by\n",
      " s\n",
      " j\n",
      " =\n",
      " GR U\n",
      " (\n",
      " e\n",
      " y\n",
      "\n",
      " j\n",
      "\n",
      " 1\n",
      " ; s\n",
      " j\n",
      "\n",
      " 1\n",
      " ; c\n",
      " j\n",
      " )\n",
      " (6)\n",
      " The probability distrib ution\n",
      " P\n",
      " j\n",
      " o v er all the w ords\n",
      " in the tar get v ocab ulary is produced conditioned\n",
      " on the embedding of the pre vious ground truth\n",
      " w ord, the source conte xt v ector and the hidden\n",
      " state\n",
      " t\n",
      " j\n",
      " =\n",
      " g\n",
      "\n",
      " e\n",
      " y\n",
      "\n",
      " j\n",
      "\n",
      " 1\n",
      " ; c\n",
      " j\n",
      " ; s\n",
      " j\n",
      "\n",
      " (7)\n",
      " o\n",
      " j\n",
      " =\n",
      " W\n",
      " o\n",
      " t\n",
      " j\n",
      " (8)\n",
      " P\n",
      " j\n",
      " = softmax (\n",
      " o\n",
      " j\n",
      " )\n",
      " (9)\n",
      " where\n",
      " g\n",
      " stands for a linear transformation,\n",
      " W\n",
      " o\n",
      " is\n",
      " used to map\n",
      " t\n",
      " j\n",
      " to\n",
      " o\n",
      " j\n",
      " so that each tar get w ord has\n",
      " one corresponding dimension in\n",
      " o\n",
      " j\n",
      " .\n",
      " 3 A ppr oach\n",
      " The main frame w ork (as sho wn in Figure\n",
      " 1\n",
      " ) of our\n",
      " method is to feed as conte xt either the ground truth\n",
      " w ords or the pre vious predicted w ords, i.e.\n",
      " or acle\n",
      " Figure 1: The architecture of our method.\n",
      " wor ds\n",
      " , with a certain probability . This potentially\n",
      " can reduce the g ap between training and inference\n",
      " by tr aining the model to handle the situation which\n",
      " will appear during test time. W e will introduce tw o\n",
      " methods to select the oracle w ords. One method is\n",
      " to select the oracle w ords at the w ord le v el with a\n",
      " greedy search algorithm, and another is to select\n",
      " a oracle sequence at the sentence-le v el optimum.\n",
      " The sentence -le v el oracle pro vides an option of\n",
      " n\n",
      " -\n",
      " gram matching with the ground truth sequence and\n",
      " hence inherently has the ability of reco v ering from\n",
      " o v ercorrection for the alternati v e conte xt. T o pre-\n",
      " dict the\n",
      " j\n",
      " -th tar get w ord\n",
      " y\n",
      " j\n",
      " , the follo wing steps are\n",
      " in v olv ed in our approach:\n",
      " 1.\n",
      " Select an oracle w ord\n",
      " y\n",
      " oracle\n",
      " j\n",
      "\n",
      " 1\n",
      " (at w ord le v el or\n",
      " sentence le v el) at the\n",
      " f\n",
      " j\n",
      "\n",
      " 1\n",
      " g\n",
      " -th step. (Section\n",
      " Oracle W ord Selection\n",
      " )\n",
      " 2.\n",
      " Sample from the ground truth w ord\n",
      " y\n",
      "\n",
      " j\n",
      "\n",
      " 1\n",
      " with\n",
      " a probability of\n",
      " p\n",
      " or from the oracle w ord\n",
      " y\n",
      " oracle\n",
      " j\n",
      "\n",
      " 1\n",
      " with a probability of\n",
      " 1\n",
      "\n",
      " p\n",
      " . (Section\n",
      " Sampling with Decay\n",
      " )\n",
      " 3.\n",
      " Use the sampled w ord as\n",
      " y\n",
      " j\n",
      "\n",
      " 1\n",
      " and replace\n",
      " the\n",
      " y\n",
      "\n",
      " j\n",
      "\n",
      " 1\n",
      " in Equation (\n",
      " 6\n",
      " ) and (\n",
      " 7\n",
      " ) with\n",
      " y\n",
      " j\n",
      "\n",
      " 1\n",
      " ,\n",
      " then per form the follo wing prediction of the\n",
      " attention-based NMT .\n",
      " 3.1 Oracle W ord Selection\n",
      " Generally , at the\n",
      " j\n",
      " -th step, the NMT model needs\n",
      " the ground truth w ord\n",
      " y\n",
      "\n",
      " j\n",
      "\n",
      " 1\n",
      " as the conte xt w ord to\n",
      " predict\n",
      " y\n",
      " j\n",
      " , thus, we could select an oracle w ord\n",
      " y\n",
      " oracle\n",
      " j\n",
      "\n",
      " 1\n",
      " to sim ulate the conte xt w ord. The oracle\n",
      " w ord should be a w ord similar to the ground truth\n",
      " or a synon ym. Using dif ferent strate gies will pro-\n",
      " duce a dif ferent oracle w ord\n",
      " y\n",
      " oracle\n",
      " j\n",
      "\n",
      " 1\n",
      " . One option is\n",
      " that w ord-le v el greedy search could be emplo yed\n",
      " to output the oracle w ord of each step, which is\n",
      " called\n",
      " W or d-le vel Or acle\n",
      " (called\n",
      " W O\n",
      " ). Besides,\n",
      " we can further optimize the oracle by enlar ging\n",
      " the search space with beam search and then re-\n",
      " ranking the candidate translations with a sentence-\n",
      " le v el metric, e.g. BLEU (\n",
      " P apineni et al.\n",
      " ,\n",
      " 2002\n",
      " ),\n",
      " Figure 2: W ord-le v el oracle without noise.\n",
      " GLEU (\n",
      " W u et al.\n",
      " ,\n",
      " 2016\n",
      " ), R OUGE (\n",
      " Lin\n",
      " ,\n",
      " 2004\n",
      " ), etc,\n",
      " the selected translation is called\n",
      " or acle se ntence\n",
      " ,\n",
      " the w ords in the translation are\n",
      " Sentence-le vel Or -\n",
      " acle\n",
      " (denoted as\n",
      " SO\n",
      " ).\n",
      " W ord-Le v el Oracle\n",
      " F or the\n",
      " f\n",
      " j\n",
      "\n",
      " 1\n",
      " g\n",
      " -th decoding step, the direct w ay to\n",
      " select the w ord-le v el oracle is to pick the w ord\n",
      " with the highest probability from the w ord dis-\n",
      " trib ution\n",
      " P\n",
      " j\n",
      "\n",
      " 1\n",
      " dra wn by Equation (\n",
      " 9\n",
      " ), which is\n",
      " sho wn in Figure\n",
      " 2\n",
      " . The predicted score in\n",
      " o\n",
      " j\n",
      "\n",
      " 1\n",
      " is\n",
      " the v alue before the\n",
      " softmax\n",
      " operation. In prac-\n",
      " tice, we can acquire more rob ust w ord-le v el or -\n",
      " acles by introducing the\n",
      " Gumbel-Max\n",
      " technique\n",
      " (\n",
      " Gumbel\n",
      " ,\n",
      " 1954\n",
      " ;\n",
      " Maddison et al.\n",
      " ,\n",
      " 2014\n",
      " ), which\n",
      " pro vides a simple and ef w ay to sample from\n",
      " a cate gorical distrib ution.\n",
      " The G umbel noise, treated as a form of re gular -\n",
      " ization, is added to\n",
      " o\n",
      " j\n",
      "\n",
      " 1\n",
      " in Equation (\n",
      " 8\n",
      " ), as sho wn\n",
      " in Figure\n",
      " 3\n",
      " , then\n",
      " softmax\n",
      " function is performed,\n",
      " the w ord distrib ution of\n",
      " y\n",
      " j\n",
      "\n",
      " 1\n",
      " is approximated by\n",
      "\n",
      " =\n",
      "\n",
      " log (\n",
      "\n",
      " log\n",
      " u\n",
      " )\n",
      " (10)\n",
      " ~\n",
      " o\n",
      " j\n",
      "\n",
      " 1\n",
      " = (\n",
      " o\n",
      " j\n",
      "\n",
      " 1\n",
      " +\n",
      "\n",
      " )\n",
      " =˝\n",
      " (11)\n",
      " ~\n",
      " P\n",
      " j\n",
      "\n",
      " 1\n",
      " = softmax ( ~\n",
      " o\n",
      " j\n",
      "\n",
      " 1\n",
      " )\n",
      " (12)\n",
      " where\n",
      "\n",
      " is the Gumbel noise calculated from a uni-\n",
      " form random v ariable\n",
      " u\n",
      " ˘ U\n",
      " (0\n",
      " ;\n",
      " 1)\n",
      " ,\n",
      " ˝\n",
      " is tempera-\n",
      " ture. As\n",
      " ˝\n",
      " approaches 0, the\n",
      " softmax\n",
      " function is\n",
      " similar to the\n",
      " argmax\n",
      " operation, and it becomes\n",
      " uniform distrib ution gradually when\n",
      " ˝\n",
      " ! 1\n",
      " .\n",
      " Similarly , according to\n",
      " ~\n",
      " P\n",
      " j\n",
      "\n",
      " 1\n",
      " , the\n",
      " 1\n",
      " -best w ord is\n",
      " selected as the w ord-le v el oracle w ord\n",
      " y\n",
      " oracle\n",
      " j\n",
      "\n",
      " 1\n",
      " =\n",
      " y\n",
      " W O\n",
      " j\n",
      "\n",
      " 1\n",
      " = argmax\n",
      "\n",
      " ~\n",
      " P\n",
      " j\n",
      "\n",
      " 1\n",
      "\n",
      " (13)\n",
      " Note that the Gumbel noise is just used to select\n",
      " the oracle and it does not af fect the loss function\n",
      " for training.\n",
      " Sentence-Le v el Oracle\n",
      " The sentence-le v el oracle is emplo yed to allo w for\n",
      " more xible translati on with\n",
      " n\n",
      " -gram matching re-\n",
      " quired by a sentence-le v el metric. In this paper ,\n",
      " Figure 3: W ord-le v el oracle with Gumbel noise.\n",
      " we emplo y BLEU as the sentence-le v el metric. T o\n",
      " select the sentence-le v el oracles, we perform\n",
      " beam search for all sentences in each batch, as-\n",
      " suming bea m size is\n",
      " k\n",
      " , and get\n",
      " k\n",
      " -best candi date\n",
      " translations. In the process of beam search, we\n",
      " also could apply the Gumbel noise for each w ord\n",
      " generation. W e then e v aluate each translation by\n",
      " calculating its BLEU score with the ground truth\n",
      " sequence, and use the translation with the highest\n",
      " BLEU score as the\n",
      " or acle sentence\n",
      " . W e denote it\n",
      " as\n",
      " y\n",
      " S\n",
      " = (\n",
      " y\n",
      " S\n",
      " 1\n",
      " ; :::; y\n",
      " S\n",
      " j\n",
      " y\n",
      " S\n",
      " j\n",
      " )\n",
      " , then at the\n",
      " j\n",
      " -th decoding\n",
      " step, we the sentence-le v el oracle w ord as\n",
      " y\n",
      " oracle\n",
      " j\n",
      "\n",
      " 1\n",
      " =\n",
      " y\n",
      " SO\n",
      " j\n",
      "\n",
      " 1\n",
      " =\n",
      " y\n",
      " S\n",
      " j\n",
      "\n",
      " 1\n",
      " (14)\n",
      " But a problem comes with sentence-le v el oracle.\n",
      " As the model samples from ground truth w ord and\n",
      " the sentence-le v el oracle w ord at each step, the\n",
      " tw o sequences should ha v e the same number of\n",
      " w ords. Ho we v er we can not assure this with the\n",
      " nai v e beam search decoding algorithm. Based on\n",
      " the abo v e problem, we introduce\n",
      " for ce decoding\n",
      " to\n",
      " mak e sure the tw o sequences ha v e the same length.\n",
      " F or ce Decoding .\n",
      " As the length of the ground\n",
      " truth sequence is\n",
      " j\n",
      " y\n",
      "\n",
      " j\n",
      " , the goal of force decod-\n",
      " ing is to generate a sequence with\n",
      " j\n",
      " y\n",
      "\n",
      " j\n",
      " w ords fol-\n",
      " lo wed by a special end-of-sentence (EOS) symbol.\n",
      " Therefore, in beam search, once a candidate trans-\n",
      " lation tends to end with EOS when it is shorter or\n",
      " longer than\n",
      " j\n",
      " y\n",
      "\n",
      " j\n",
      " , we will force it to generate\n",
      " j\n",
      " y\n",
      "\n",
      " j\n",
      " w ords, that is,\n",
      "\n",
      " If the candidate translation gets a w ord distri-\n",
      " b ution\n",
      " P\n",
      " j\n",
      " at the\n",
      " j\n",
      " -th step where\n",
      " j\n",
      " 6\n",
      " j\n",
      " y\n",
      "\n",
      " j\n",
      " and\n",
      " EOS is the top w ord in\n",
      " P\n",
      " j\n",
      " , t hen we select\n",
      " the top second w ord in\n",
      " P\n",
      " j\n",
      " as the\n",
      " j\n",
      " -th w ord of\n",
      " this candidate translation.\n",
      "\n",
      " If the candidate translation gets a w ord distri-\n",
      " b ution\n",
      " P\n",
      " j\n",
      " y\n",
      "\n",
      " j\n",
      " +1\n",
      " at the\n",
      " fj\n",
      " y\n",
      "\n",
      " j\n",
      " +1\n",
      " g\n",
      " -th step where\n",
      " EOS is not the top w ord in\n",
      " P\n",
      " j\n",
      " y\n",
      "\n",
      " j\n",
      " +1\n",
      " , then\n",
      " we select EOS as the\n",
      " fj\n",
      " y\n",
      "\n",
      " j\n",
      " +1\n",
      " g\n",
      " -th w ord of\n",
      " this candidate translation.\n",
      " In this w ay , we can mak e sure that all the\n",
      " k\n",
      " can-\n",
      " didate translations ha v e\n",
      " j\n",
      " y\n",
      "\n",
      " j\n",
      " w ords, then re-rank\n",
      " the\n",
      " k\n",
      " candidates according to BLEU score a nd se-\n",
      " lect the top as the oracle sentence. F or adding\n",
      " Gumbel noise into the sentence-le v el oracle selec-\n",
      " tion, we replace the\n",
      " P\n",
      " j\n",
      " with\n",
      " ~\n",
      " P\n",
      " j\n",
      " at the\n",
      " j\n",
      " -th decod-\n",
      " ing step during force decoding.\n",
      " 3.2 Sampling with Decay\n",
      " In our method, we emplo y a sampling mechanism\n",
      " to randomly select the ground truth w ord\n",
      " y\n",
      "\n",
      " j\n",
      "\n",
      " 1\n",
      " or\n",
      " the oracle w ord\n",
      " y\n",
      " oracle\n",
      " j\n",
      "\n",
      " 1\n",
      " as\n",
      " y\n",
      " j\n",
      "\n",
      " 1\n",
      " . At the be ginning\n",
      " of training, as the model is not well trained, us-\n",
      " ing\n",
      " y\n",
      " oracle\n",
      " j\n",
      "\n",
      " 1\n",
      " as\n",
      " y\n",
      " j\n",
      "\n",
      " 1\n",
      " too often w ould lead to v ery\n",
      " slo w con v er gence, e v en being trapped into local\n",
      " optimum. On the other hand, at the end of train-\n",
      " ing, if the conte xt\n",
      " y\n",
      " j\n",
      "\n",
      " 1\n",
      " is still s elected from the\n",
      " ground truth w ord\n",
      " y\n",
      "\n",
      " j\n",
      "\n",
      " 1\n",
      " at a lar ge probability , the\n",
      " model is not fully e xposed to the circumstance\n",
      " which it has to confront at inference and hence can\n",
      " not kno w ho w to act in the situation at inference.\n",
      " In this sense, the probability\n",
      " p\n",
      " of selecting from\n",
      " the ground truth w ord can not be ed, b ut has\n",
      " to decrease progressi v ely as the training adv ances.\n",
      " At t he be ginning,\n",
      " p\n",
      " =1\n",
      " , which means the model is\n",
      " trained entirely based on the ground truth w ords.\n",
      " As the model con v er ges gradually , the model se-\n",
      " lects from the oracle w ords more often.\n",
      " Borro wing ideas from b ut being dif ferent\n",
      " from\n",
      " Bengio et al.\n",
      " (\n",
      " 2015\n",
      " ) which used a schedule to\n",
      " decrease\n",
      " p\n",
      " as a function of the inde x of mini-batch,\n",
      " we\n",
      " p\n",
      " with a decay function dependent on\n",
      " the inde x of training epochs\n",
      " e\n",
      " (starting from\n",
      " 0\n",
      " )\n",
      " p\n",
      " =\n",
      "\n",
      "\n",
      " + exp (\n",
      "\n",
      " )\n",
      " (15)\n",
      " where\n",
      "\n",
      " is a h yper -parameter . The function is\n",
      " strictly monotone decreasing. As the training pro-\n",
      " ceeds, the probability\n",
      " p\n",
      " of feeding ground truth\n",
      " w ords decreases gradually .\n",
      " 3.3 T raining\n",
      " After selecting\n",
      " y\n",
      " j\n",
      "\n",
      " 1\n",
      " by using the abo v e method,\n",
      " we can get the w ord distrib ution of\n",
      " y\n",
      " j\n",
      " according\n",
      " to Equation (\n",
      " 6\n",
      " ), (\n",
      " 7\n",
      " ), (\n",
      " 8\n",
      " ) and (\n",
      " 9\n",
      " ). W e do not add\n",
      " the Gumbel noise to the di strib ution when calcu-\n",
      " lating loss for training. The objecti v e is to maxi-\n",
      " mize the probabi lity of the ground truth sequence\n",
      " based on maximum lik elihood estimation (MLE).\n",
      " Thus follo wing loss function is minimized:\n",
      " L\n",
      " (\n",
      "\n",
      " ) =\n",
      "\n",
      " X\n",
      " N\n",
      " n\n",
      " =1\n",
      " X\n",
      " j\n",
      " y\n",
      " n\n",
      " j\n",
      " j\n",
      " =1\n",
      " log\n",
      " P\n",
      " n\n",
      " j\n",
      "\n",
      " y\n",
      " n\n",
      " j\n",
      "\n",
      " (16)\n",
      " where\n",
      " N\n",
      " is the number of sentence pairs in the\n",
      " training data,\n",
      " j\n",
      " y\n",
      " n\n",
      " j\n",
      " indicates the length of the\n",
      " n\n",
      " -th\n",
      " ground truth sentence,\n",
      " P\n",
      " n\n",
      " j\n",
      " refers to the predicted\n",
      " probability distrib ution at the\n",
      " j\n",
      " -th step for the\n",
      " n\n",
      " -th\n",
      " sentence, hence\n",
      " P\n",
      " n\n",
      " j\n",
      " h\n",
      " y\n",
      " n\n",
      " j\n",
      " i\n",
      " is the probability of gen-\n",
      " erating the ground truth w ord\n",
      " y\n",
      " n\n",
      " j\n",
      " at the\n",
      " j\n",
      " -th step.\n",
      " 4 Related W ork\n",
      " Some other researchers ha v e noticed the prob-\n",
      " lem of e xposure bias in NMT and tried to solv e\n",
      " it.\n",
      " V enkatraman et al.\n",
      " (\n",
      " 2015\n",
      " ) proposed D\n",
      " A T A\n",
      " A\n",
      " S\n",
      " D\n",
      " E M O N S T R A T O R\n",
      " (D AD) which initialized\n",
      " the training e xamples as the paired tw o adjacent\n",
      " ground truth w ords and at each step added the pre-\n",
      " dicted w ord paired with the ne xt ground truth w ord\n",
      " as a ne w training e xample.\n",
      " Bengio et al.\n",
      " (\n",
      " 2015\n",
      " )\n",
      " further de v eloped the method by sampling as con-\n",
      " te xt from the pre vious ground truth w ord and the\n",
      " pre vious predicted w ord with a changing probabil-\n",
      " ity , not treating them equally in the whole training\n",
      " process. This is similar to our method, b ut the y\n",
      " do not include the sentence-le v el oracle to relie v e\n",
      " the o v ercorrection problem and neither the noise\n",
      " perturbations on the predicted distrib ution.\n",
      " Another di rection of attempts is the sentence-\n",
      " le v el training with the thinking that the sentence-\n",
      " le v el metric, e.g., BLEU, brings a certain de-\n",
      " gree of xibility for generation and hence is\n",
      " more rob ust to mitig ate the e xposure bias problem.\n",
      " T o a v oid the problem of e xposure bias,\n",
      " Ranzato\n",
      " et al.\n",
      " (\n",
      " 2015\n",
      " ) presented a no v el algorithm Mix ed\n",
      " Incremental Cross-Entrop y Reinforce (MIXER)\n",
      " for sequence-le v el training, which directly op-\n",
      " timized t he sentence-le v el BLEU used at infer -\n",
      " ence.\n",
      " Shen et al.\n",
      " (\n",
      " 2016\n",
      " ) introduced the Minimum\n",
      " Risk T raining (MR T) into the end-to-end NMT\n",
      " model, which optimized model param eters by\n",
      " minimizing directly the e xpected loss with respect\n",
      " to arbitrary e v aluation metrics, e.g., sentence-le v el\n",
      " BLEU.\n",
      " Shao et al.\n",
      " (\n",
      " 2018\n",
      " ) proposed to eliminate\n",
      " the e xposure bias through a probabilistic n-gram\n",
      " matching objecti v e, which trains NMT NMT un-\n",
      " der the greedy decoding strate gy .\n",
      " 5 Experiments\n",
      " W e carry out e xperiments on the NIST\n",
      " Chinese\n",
      " !\n",
      " English (Zh\n",
      " !\n",
      " En) and the WMT'14\n",
      " English\n",
      " !\n",
      " German (En\n",
      " !\n",
      " De) translation tasks.\n",
      " 5.1 Settings\n",
      " F or Zh\n",
      " !\n",
      " En, the training dataset consists of 1.25M\n",
      " sentence pairs e xtracted from LDC corpora\n",
      " 1\n",
      " . W e\n",
      " choose the NIST 2002 (MT02) dataset as the v al-\n",
      " idation set, which has\n",
      " 878\n",
      " sentences, and the\n",
      " NIST 2003 (MT03), NIST 2004 (MT04), NIST\n",
      " 2005 (MT05) and NIST 2006 (MT06) datasets\n",
      " as the test sets, which contain 919, 1788, 1082\n",
      " and 1664 sentences respecti v ely . F or En\n",
      " !\n",
      " De,\n",
      " we perform our e xperiments on the corpus pro-\n",
      " vided by WMT'14, which contains 4.5M sentence\n",
      " pairs\n",
      " 2\n",
      " . W e use the\n",
      " newstest2013\n",
      " as the v alidation\n",
      " set, and the\n",
      " newstest2014\n",
      " as the test sets, which\n",
      " containing\n",
      " 3003\n",
      " and\n",
      " 2737\n",
      " sentences respe cti v ely .\n",
      " W e measure the translation quality with BLEU\n",
      " scores (\n",
      " P apineni et al.\n",
      " ,\n",
      " 2002\n",
      " ). F or Zh\n",
      " !\n",
      " En, case-\n",
      " insensiti v e BLEU score is calculated by using the\n",
      " mte val-v11b .pl\n",
      " script. F or En\n",
      " !\n",
      " De, we tok enize\n",
      " the references and e v aluate the performance with\n",
      " case-sensiti v e BLEU score by the\n",
      " multi-bleu.pl\n",
      " script. The metrics are e xactly the same as in pre-\n",
      " vious w ork. Besides, we mak e statistical\n",
      " cance test according to the method of\n",
      " Collins et al.\n",
      " (\n",
      " 2005\n",
      " ).\n",
      " In training the NMT model, we limit the source\n",
      " and tar get v ocab ulary to the most frequent\n",
      " 30\n",
      " K\n",
      " w ords for both sides in the Zh\n",
      " !\n",
      " En translation\n",
      " task, co v ering approximately\n",
      " 97\n",
      " :\n",
      " 7\n",
      " % and\n",
      " 99\n",
      " :\n",
      " 3\n",
      " %\n",
      " w ords of tw o corpus respecti v ely . F or the En\n",
      " !\n",
      " De\n",
      " translation task, sentences are encoded using byte-\n",
      " pair encoding (BPE) (\n",
      " Se nnrich et al.\n",
      " ,\n",
      " 2016\n",
      " ) with\n",
      " 37\n",
      " k\n",
      " mer ging operations for both source and tar -\n",
      " get languages, which ha v e v ocab ularies of\n",
      " 39418\n",
      " and\n",
      " 40274\n",
      " tok ens r especti v ely . W e limit the length\n",
      " of sentences in the training datasets to\n",
      " 50\n",
      " w ords\n",
      " for Zh\n",
      " !\n",
      " En and\n",
      " 128\n",
      " subw ords for En\n",
      " !\n",
      " De. F or\n",
      " RNNSearch model, the dimension of w ord em-\n",
      " bedding and hidden layer is\n",
      " 512\n",
      " , and the beam\n",
      " size in testing is\n",
      " 10\n",
      " . All parameters are initialized\n",
      " by the uniform distrib ution o v er\n",
      " [\n",
      "\n",
      " 0\n",
      " :\n",
      " 1\n",
      " ;\n",
      " 0\n",
      " :\n",
      " 1]\n",
      " . The\n",
      " mini-batch stochastic gradient descent (SGD) al-\n",
      " gorithm is emplo yed to train the model parameters\n",
      " with batch size setting to\n",
      " 80\n",
      " . Moreo v er , the learn-\n",
      " ing rate is adjusted by adadelta optimizer (\n",
      " Zeiler\n",
      " ,\n",
      " 2012\n",
      " ) with\n",
      " ˆ\n",
      " =\n",
      " 0\n",
      " :\n",
      " 95\n",
      " and\n",
      "\n",
      " =\n",
      " 1\n",
      " e\n",
      " -\n",
      " 6\n",
      " . Dropout is applied\n",
      " on the output layer with dropout rate being\n",
      " 0\n",
      " :\n",
      " 5\n",
      " .\n",
      " F or T ransformer model, we train base model with\n",
      " 1\n",
      " These se ntence pairs are mainly e xtracted from\n",
      " LDC2002E18, LDC2003E07, LDC2003E14, Hansards por -\n",
      " tion of LDC2004T07, LDC2004T08 and LDC2005T06\n",
      " 2\n",
      " http://www.statmt.org/wmt14/\n",
      " translation- task.html\n",
      " Systems\n",
      " Ar chitectur e\n",
      " MT03\n",
      " MT04\n",
      " MT05\n",
      " MT06\n",
      " A v erage\n",
      " Existing end-to-end NMT systems\n",
      " T u et al.\n",
      " (\n",
      " 2016\n",
      " )\n",
      " Co v erage\n",
      " 33\n",
      " :\n",
      " 69\n",
      " 38\n",
      " :\n",
      " 05\n",
      " 35\n",
      " :\n",
      " 01\n",
      " 34\n",
      " :\n",
      " 83\n",
      " 35\n",
      " :\n",
      " 40\n",
      " Shen et al.\n",
      " (\n",
      " 2016\n",
      " )\n",
      " MR T\n",
      " 37\n",
      " :\n",
      " 41\n",
      " 39\n",
      " :\n",
      " 87\n",
      " 37\n",
      " :\n",
      " 45\n",
      " 36\n",
      " :\n",
      " 80\n",
      " 37\n",
      " :\n",
      " 88\n",
      " Zhang et al.\n",
      " (\n",
      " 2017\n",
      " )\n",
      " Distortion\n",
      " 37\n",
      " :\n",
      " 93\n",
      " 40\n",
      " :\n",
      " 40\n",
      " 36\n",
      " :\n",
      " 81\n",
      " 35\n",
      " :\n",
      " 77\n",
      " 37\n",
      " :\n",
      " 73\n",
      " Our end-to-end NMT systems\n",
      " this w ork\n",
      " RNNsearch\n",
      " 37\n",
      " :\n",
      " 93\n",
      " 40\n",
      " :\n",
      " 53\n",
      " 36\n",
      " :\n",
      " 65\n",
      " 35\n",
      " :\n",
      " 80\n",
      " 37\n",
      " :\n",
      " 73\n",
      " + SS-NMT\n",
      " 38\n",
      " :\n",
      " 82\n",
      " 41\n",
      " :\n",
      " 68\n",
      " 37\n",
      " :\n",
      " 28\n",
      " 37\n",
      " :\n",
      " 98\n",
      " 38\n",
      " :\n",
      " 94\n",
      " + MIXER\n",
      " 38\n",
      " :\n",
      " 70\n",
      " 40\n",
      " :\n",
      " 81\n",
      " 37\n",
      " :\n",
      " 59\n",
      " 38\n",
      " :\n",
      " 38\n",
      " 38\n",
      " :\n",
      " 87\n",
      " + OR-NMT\n",
      " 40.40\n",
      " zy\n",
      " ?\n",
      " 42.63\n",
      " zy\n",
      " ?\n",
      " 38.87\n",
      " zy\n",
      " ?\n",
      " 38.44\n",
      " z\n",
      " 40.09\n",
      " T ransformer\n",
      " 46\n",
      " :\n",
      " 89\n",
      " 47\n",
      " :\n",
      " 88\n",
      " 47\n",
      " :\n",
      " 40\n",
      " 46\n",
      " :\n",
      " 66\n",
      " 47\n",
      " :\n",
      " 21\n",
      " + w ord oracle\n",
      " 47\n",
      " :\n",
      " 42\n",
      " 48\n",
      " :\n",
      " 34\n",
      " 47\n",
      " :\n",
      " 89\n",
      " 47\n",
      " :\n",
      " 34\n",
      " 47\n",
      " :\n",
      " 75\n",
      " + sentence oracle\n",
      " 48.31\n",
      "\n",
      " 49.40\n",
      "\n",
      " 48.72\n",
      "\n",
      " 48.45\n",
      "\n",
      " 48.72\n",
      " T able 1: Case-insensiti v e BLEU scores (%) on Zh\n",
      " !\n",
      " En translation task. ﬁ\n",
      " z\n",
      " ﬂ, ﬁ\n",
      " y\n",
      " ﬂ, ﬁ\n",
      " ?\n",
      " ﬂ and ﬁ\n",
      "\n",
      " ﬂ indicate statistically\n",
      " dif ference (p\n",
      " <\n",
      " 0.01) from RNNsearch, SS-NMT , MIXER and T ransformer , respecti v ely .\n",
      " def ault settings (f airseq\n",
      " 3\n",
      " ).\n",
      " 5.2 Systems\n",
      " The follo wing systems are in v olv ed:\n",
      " RNNsear ch:\n",
      " Our implementation of an im-\n",
      " pro v ed model as described in Section\n",
      " 2\n",
      " , where\n",
      " the decoder emplo ys tw o GR Us and an attention.\n",
      " , Equation\n",
      " 6\n",
      " is substituted with:\n",
      " ~\n",
      " s\n",
      " j\n",
      " =\n",
      " GR U\n",
      " 1\n",
      " (\n",
      " e\n",
      " y\n",
      "\n",
      " j\n",
      "\n",
      " 1\n",
      " ; s\n",
      " j\n",
      "\n",
      " 1\n",
      " )\n",
      " (17)\n",
      " s\n",
      " j\n",
      " =\n",
      " GR U\n",
      " 2\n",
      " (\n",
      " c\n",
      " j\n",
      " ;\n",
      " ~\n",
      " s\n",
      " j\n",
      " )\n",
      " (18)\n",
      " Besides, in Equation\n",
      " 3\n",
      " ,\n",
      " s\n",
      " j\n",
      "\n",
      " 1\n",
      " is replaced with\n",
      " ~\n",
      " s\n",
      " j\n",
      "\n",
      " 1\n",
      " .\n",
      " SS-NMT :\n",
      " Our implementation of the scheduled\n",
      " sampling (SS) method (\n",
      " Bengio et al.\n",
      " ,\n",
      " 2015\n",
      " ) on the\n",
      " basis of the RNNsearch. The decay scheme is the\n",
      " same as Equation\n",
      " 15\n",
      " in our approach.\n",
      " MIXER:\n",
      " Our implementation of the mix ed in-\n",
      " cremental cross-entrop y reinforce (\n",
      " Ranzato et al.\n",
      " ,\n",
      " 2015\n",
      " ), where the sentence-le v el metric is BLEU\n",
      " and the a v erage re w ard is acquired according to\n",
      " its of method with a\n",
      " 1\n",
      " -layer linear re gressor .\n",
      " OR-NMT :\n",
      " Based on the RNNsearch, we intro-\n",
      " duced the w ord-le v el oracles, sentence-le v el ora-\n",
      " cles and the Gumbel noises to enhance the o v er -\n",
      " correction reco v ery capacity . F or the sentence-\n",
      " le v el oracle selection, we set the beam size to be\n",
      " 3\n",
      " ,\n",
      " set\n",
      " ˝\n",
      " =\n",
      " 0\n",
      " :\n",
      " 5\n",
      " in Equa tion (\n",
      " 11\n",
      " ) and\n",
      "\n",
      " =\n",
      " 12\n",
      " for the decay\n",
      " function in Equation (\n",
      " 15\n",
      " ). OR-NMT is the abbre-\n",
      " viation of NMT with Ov ercorrection Reco v ery .\n",
      " 3\n",
      " https://github.com/pytorch/fairseq\n",
      " 5.3 Results on Zh\n",
      " !\n",
      " En T ranslation\n",
      " W e v erify our method on tw o baseline models with\n",
      " the NIST Zh\n",
      " !\n",
      " En datasets in this section.\n",
      " Results on the RNNsear ch\n",
      " As sho wn in T able\n",
      " 1\n",
      " ,\n",
      " T u et al.\n",
      " (\n",
      " 2016\n",
      " ) propose to\n",
      " model co v erage in RNN-based NMT to impro v e\n",
      " the adequac y of translations.\n",
      " Shen et al.\n",
      " (\n",
      " 2016\n",
      " )\n",
      " propose minimum risk training (MR T) for NMT\n",
      " to directly optimize model parameters with respect\n",
      " to BLEU scores.\n",
      " Zhang et al.\n",
      " (\n",
      " 2017\n",
      " ) model dis-\n",
      " tortion to enhance the attention model . Compared\n",
      " with them, our baseline system RNNsearch 1) out-\n",
      " performs pre vious shallo w RNN-based NMT sys-\n",
      " tem equipped with the co v erage model (\n",
      " T u et al.\n",
      " ,\n",
      " 2016\n",
      " ); and 2) achie v es competiti v e performance\n",
      " with the MR T (\n",
      " Shen et al.\n",
      " ,\n",
      " 2016\n",
      " ) and the Distor -\n",
      " tion (\n",
      " Zhang et al.\n",
      " ,\n",
      " 2017\n",
      " ) on the same datasets. W e\n",
      " hope that the strong shallo w baseline system used\n",
      " in this w ork mak es the e v aluation con vincing.\n",
      " W e also compare with the other tw o related\n",
      " methods that aim at solving the e xposure bias\n",
      " problem, including the scheduled sampling (\n",
      " Ben-\n",
      " gio et al.\n",
      " ,\n",
      " 2015\n",
      " ) (SS-NMT) and the sentence-\n",
      " le v el training (\n",
      " Ranzato et al.\n",
      " ,\n",
      " 2015\n",
      " ) (MIXER).\n",
      " From T able\n",
      " 1\n",
      " , we can see that both SS-NMT and\n",
      " MIXER can achie v e impro v ements by taking mea-\n",
      " sures to mitig ate the e xposure bias. While our\n",
      " approach OR-NMT can outperform the baseline\n",
      " system RNNsearch and the competiti v e compar -\n",
      " ison systems by directly incorporate the sentence-\n",
      " le v el oracle and noise perturbations for relie ving\n",
      " the o v ercorrection problem. P articular ly , our OR-\n",
      " NMT outperforms the RNNsearch\n",
      " by +\n",
      " 2\n",
      " :\n",
      " 36\n",
      " BLEU points a v eragely on four test\n",
      " datasets. Comparing with the tw o related models,\n",
      " Systems\n",
      " A v erage\n",
      " RNNsearch\n",
      " 37\n",
      " :\n",
      " 73\n",
      " + w ord oracle\n",
      " 38\n",
      " :\n",
      " 94\n",
      " + noise\n",
      " 39\n",
      " :\n",
      " 50\n",
      " + sentence oracle\n",
      " 39\n",
      " :\n",
      " 56\n",
      " + noise\n",
      " 40.09\n",
      " T able 2: F actor analysis on Zh\n",
      " !\n",
      " En translation, the re-\n",
      " sults are a v erage BLEU scores on MT03\n",
      " ˘\n",
      " 06 datasets.\n",
      " our a pproach further gi v es a impro v e-\n",
      " ments on most test sets and achie v es impro v ement\n",
      " by about +\n",
      " 1\n",
      " :\n",
      " 2\n",
      " BLEU points on a v erage.\n",
      " Results on the T ransf ormer\n",
      " The methods we propose can also be adapted\n",
      " to the stronger T ransformer model. The e v alu-\n",
      " ated results are listed in T able\n",
      " 1\n",
      " . Our w ord-le v el\n",
      " method can impro v e the base model by +\n",
      " 0\n",
      " :\n",
      " 54\n",
      " BLEU points on a v erage, and the sentence-le v el\n",
      " method can further bring in +\n",
      " 1\n",
      " :\n",
      " 0\n",
      " BLEU points im-\n",
      " pro v ement.\n",
      " 5.4 F actor Analysis\n",
      " W e propose se v eral strate gies to impro v e the per -\n",
      " formance of approach on relie ving the o v ercorrec-\n",
      " tion problem, including utilizing the w ord-le v el\n",
      " oracle, the sentence-le v el oracle, and incorporat-\n",
      " ing the Gumbel noise for oracle selection. T o in-\n",
      " v estig ate the of these f actors , we conduct\n",
      " the e xperiments and list the results in T able\n",
      " 2\n",
      " .\n",
      " When only emplo ying the w ord-le v el oracle , the\n",
      " translation performance w as impro v ed by +\n",
      " 1\n",
      " :\n",
      " 21\n",
      " BLEU points, this indicates that feeding pre-\n",
      " dicted w ords as conte xt can mitig ate e xposure\n",
      " bias. When emplo ying the sentence-le v el oracle,\n",
      " we can further achie v e +\n",
      " 0\n",
      " :\n",
      " 62\n",
      " BLEU points im-\n",
      " pro v ement. It sho ws t hat the sentence-le v el oracle\n",
      " performs better than the w ord-le v el oracle i n terms\n",
      " of BLEU. W e conjecture that the superiority may\n",
      " come from a greater xibility for w ord genera-\n",
      " tion which can mitig ate the problem of o v ercor -\n",
      " rection. By incorporating the Gumbel noise dur -\n",
      " ing the generation of the w ord-le v el and sentence-\n",
      " le v el oracle w ords, the BLEU score are further im-\n",
      " pro v ed by\n",
      " 0\n",
      " :\n",
      " 56\n",
      " and\n",
      " 0\n",
      " :\n",
      " 53\n",
      " respecti v ely . This indi-\n",
      " cates Gumbel noise can help the selection of each\n",
      " oracle w ord, which is consistent with our claim\n",
      " that Gumbel-Max pro vides a ef and rob ust\n",
      " w ay to sample from a cate gorical distrib ution.\n",
      " Figure 4: T raining loss curv es on Zh\n",
      " !\n",
      " En translation\n",
      " with dif ferent f actors. The black, blue and red colors\n",
      " represent the RNNsearch, RNNs earch with w ord-le v el\n",
      " oracle and RNNsearch with sentence-le v el oracle sys-\n",
      " tems respecti v ely .\n",
      " Figure 5: T rends of BLEU scores on the v alidation set\n",
      " with dif ferent f actors on the Zh\n",
      " !\n",
      " En translation task.\n",
      " 5.5 About Con v er gence\n",
      " In this section, we analyze the of dif fer -\n",
      " ent f actors for the con v er gence. Figure\n",
      " 4\n",
      " gi v es the\n",
      " training loss curv es of the RNNsearch, w ord-le v el\n",
      " oracle (W O) without noise and sentence-le v el or -\n",
      " acle (SO) with noise. In training, BLEU score\n",
      " on the v alidation set is used to select the best\n",
      " model, a detailed comparison among the BLEU\n",
      " score curv es under dif ferent f actors is sho wn in\n",
      " Figure\n",
      " 5\n",
      " . RNNsearch con v er ges f ast and achie v es\n",
      " the best result at the\n",
      " 7\n",
      " -th epoch, while the train-\n",
      " ing loss continues to decline after the\n",
      " 7\n",
      " -th epoch\n",
      " until the end. Thus, the training of RNNsearch\n",
      " may encounter the o v problem. Figure\n",
      " 4\n",
      " and\n",
      " 5\n",
      " also re v eal that, inte grating t he oracle sam-\n",
      " pling and the Gumbel noise leads to a little slo wer\n",
      " con v er gence and the training loss does not k eep\n",
      " decreasing after the best results appear on the v al-\n",
      " idation set. This is consistent with our intuition\n",
      " that oracle sampling and noises can a v oid o v\n",
      " Figure 6: T rends of BLEU scores on the MT03 test set\n",
      " with dif ferent f actors on the Zh\n",
      " !\n",
      " En translation task.\n",
      " ting despite needs a longer time to con v er ge.\n",
      " Figure\n",
      " 6\n",
      " sho ws the BLEU scores curv es on the\n",
      " MT03 test set under dif ferent f actors\n",
      " 4\n",
      " . When sam-\n",
      " pling oracles with noise (\n",
      " ˝\n",
      " =\n",
      " 0\n",
      " :\n",
      " 5\n",
      " ) on the sentence\n",
      " le v el, we obtain the best model. W ithout noise,\n",
      " our system con v er ges to a lo wer BLEU score. This\n",
      " can be understood easily that using its o wn re-\n",
      " sults repeatedly during training without an y re g-\n",
      " ularization will l ead to o v and quick con-\n",
      " v er gence. In this sense, our method from\n",
      " the sentence-le v el sampling and Gumbel noise.\n",
      " 5.6 About Length\n",
      " Figure\n",
      " 7\n",
      " sho ws the BLEU scores of generated\n",
      " translations on the MT03 test set with respect to\n",
      " the lengths of the source sentences. In partic-\n",
      " ular , we split the translations for the MT03 test\n",
      " set into dif ferent bins according to the length of\n",
      " source sentences, then test the BLEU scores for\n",
      " translations in each bin separately with the results\n",
      " reported in Figure\n",
      " 7\n",
      " . Our approach can achie v e\n",
      " big impro v ements o v er the baseline system in all\n",
      " bins, especially in the bins (\n",
      " 10\n",
      " ,\n",
      " 20\n",
      " ], (\n",
      " 40\n",
      " ,\n",
      " 50\n",
      " ] and\n",
      " (\n",
      " 70\n",
      " ,\n",
      " 80\n",
      " ] of the super -long sentences. The cross-\n",
      " entrop y loss requires that the predicted sequence\n",
      " is e xactly the same as the ground truth sequence\n",
      " which is more dif to achie v e for long sen-\n",
      " tences, while our sentence-le v el oracle can help\n",
      " reco v er from this kind of o v ercorrection.\n",
      " 5.7 Effect on Exposur e Bias\n",
      " T o v alidate whether the impro v ements is mainly\n",
      " obtained by addressing the e xposure bias prob-\n",
      " lem, we randomly select\n",
      " 1\n",
      " K sentence pairs from\n",
      " 4\n",
      " Note that the ﬁSOﬂ model without noise is trained based\n",
      " on the pre-trained RNNsearch model (as sho wn by the red\n",
      " dashed lines in Figure\n",
      " 5\n",
      " and\n",
      " 6\n",
      " ).\n",
      " Figure 7: Performance comparison on the MT03 test\n",
      " set with respect to the dif ferent lengths of source sen-\n",
      " tences on the Zh\n",
      " !\n",
      " En translation task.\n",
      " the Zh\n",
      " !\n",
      " En training data, and use the pre-trained\n",
      " RNNSearch model and proposed model to de-\n",
      " code the source sentences. The BLEU score of\n",
      " RNNSearch model w as\n",
      " 24\n",
      " :\n",
      " 87\n",
      " , while our model\n",
      " produced +\n",
      " 2\n",
      " :\n",
      " 18\n",
      " points. W e then count the ground\n",
      " truth w ords whose probabilities in the predicted\n",
      " distrib utions produced by our model are greater\n",
      " than those produced by the baseline model, and\n",
      " mark the number as\n",
      " N\n",
      " . There are totally\n",
      " 28\n",
      " ;\n",
      " 266\n",
      " gold w ords in the references, and\n",
      " N\n",
      " =\n",
      " 18\n",
      " ;\n",
      " 391\n",
      " .\n",
      " The proportion is\n",
      " 18\n",
      " ;\n",
      " 391\n",
      " =\n",
      " 28\n",
      " ;\n",
      " 266\n",
      " =\n",
      " 65\n",
      " :\n",
      " 06%\n",
      " , which\n",
      " could v erify the impro v ements are mainly ob-\n",
      " tained by addressing the e xposure bias problem.\n",
      " 5.8 Results on En\n",
      " !\n",
      " De T ranslation\n",
      " Systems\n",
      " newstest2014\n",
      " RNNsearch\n",
      " 25\n",
      " :\n",
      " 82\n",
      " + SS-NMT\n",
      " 26\n",
      " :\n",
      " 50\n",
      " + MIXER\n",
      " 26\n",
      " :\n",
      " 76\n",
      " + OR-NMT\n",
      " 27.41\n",
      " z\n",
      " T ransformer (base)\n",
      " 27\n",
      " :\n",
      " 34\n",
      " + SS-NMT\n",
      " 28\n",
      " :\n",
      " 05\n",
      " + MIXER\n",
      " 27\n",
      " :\n",
      " 98\n",
      " + OR-NMT\n",
      " 28.65\n",
      " z\n",
      " T able 3: Case-sensiti v e BLEU scores (%) on En\n",
      " !\n",
      " De\n",
      " task. The ﬁ\n",
      " z\n",
      " ﬂ indicates the r esults are bet-\n",
      " ter (p\n",
      " <\n",
      " 0.01) than RNNsearch and T ransformer .\n",
      " W e also e v aluate our approach on the WMT'14\n",
      " benchmarks on the En\n",
      " !\n",
      " De translation task. From\n",
      " the results listed in T able\n",
      " 3\n",
      " , we conclude that\n",
      " the proposed method outperforms the\n",
      " competiti v e baseline model as well as related ap-\n",
      " proaches. Similar with results on the Zh\n",
      " !\n",
      " En task,\n",
      " both scheduled sampling and MIXER could im-\n",
      " pro v e the tw o baseline systems. Our method im-\n",
      " pro v es the RNNSearch and T ransformer baseline\n",
      " models by +\n",
      " 1\n",
      " :\n",
      " 59\n",
      " and +\n",
      " 1\n",
      " :\n",
      " 31\n",
      " BLEU points respec-\n",
      " ti v ely . These res ults demonstrate that our model\n",
      " w orks well across dif ferent language pairs.\n",
      " 6 Conclusion\n",
      " The end-to-end NMT model generates a transla-\n",
      " tion w ord by w ord with the ground truth w ords\n",
      " as conte xt at training time as opposed to the pre-\n",
      " vious w ords generated by the model as conte xt\n",
      " at inference. T o mitig ate the discrepanc y be-\n",
      " tween training and inference, when predicting one\n",
      " w ord, we feed as conte xt either the ground truth\n",
      " w ord or the pre vious predicted w ord with a sam-\n",
      " pling scheme. The predicted w ords, ref erred to\n",
      " as oracle w ords, can be generated with the w ord-\n",
      " le v el or sentence-le v el optimization. C ompared to\n",
      " w ord-le v el oracle, sentence-le v el oracle can fur -\n",
      " ther equip the model with the ability of o v ercor -\n",
      " rection reco v ery . T o mak e the model fully e x-\n",
      " posed to the circumstance at reference, we sam-\n",
      " ple the conte xt w ord with decay from the ground\n",
      " truth w ords. W e v the ef fecti v eness of our\n",
      " method with tw o strong baseline models and re-\n",
      " lated w orks on the real translation tasks, achie v ed\n",
      " impro v ement on all the datasets. W e\n",
      " also conclude that the sentence-le v el oracle sho w\n",
      " superiority o v er the w ord-le v el oracle.\n",
      " Ackno wledgments\n",
      " W e thank the three anon ymous re vie wers for\n",
      " their v aluable suggestions. This w ork w as sup-\n",
      " ported by National Natural Science F oundation\n",
      " of China (NO. 61662077, NO. 61876174) and\n",
      " National K e y R&D Program of China (NO.\n",
      " YS2017YFGH001428).\n",
      " Refer ences\n",
      " Dzmitry Bahdanau, K yungh yun Cho, and Y oshua Ben-\n",
      " gio. 2015. Ne u r al machine translation by jointly\n",
      " learning to align and translate.\n",
      " ICLR 2015\n",
      " .\n",
      " Samy Bengi o , Oriol V in yals, Na vdeep Jaitly , and\n",
      " Noam Shazeer . 2015.\n",
      " Scheduled sampling for\n",
      " sequence prediction with recurrent neural net-\n",
      " w orks\n",
      " . In C. Cortes, N. D. La wrence, D. D. Lee,\n",
      " M. Sugiyama, and R. Garnett, editors,\n",
      " Advances in\n",
      " Neur al Information Pr ocessing Systems 28\n",
      " , pages\n",
      " 1171Œ1179. Curran Associates, Inc.\n",
      " K yungh yun Cho, Bart v an Merrienboer , Caglar Gul-\n",
      " cehre, Dzmitry Bahdanau, Fethi Boug ares, Holger\n",
      " Schwenk, and Y oshua Bengio. 2014.\n",
      " Learning\n",
      " phrase representations using rnn encoder Œdecoder\n",
      " for statistical machine translation\n",
      " . In\n",
      " Pr oceedings of\n",
      " the 2014 Confer ence on Empirical Methods in Nat-\n",
      " ur al Langua g e Pr ocessing (EMNLP)\n",
      " , pages 1724Œ\n",
      " 1734, Doha, Qatar . Association for Computational\n",
      " Linguistics.\n",
      " Michael Collins, Philipp K oehn, and Iv ona K ucero v a.\n",
      " 2005.\n",
      " Clause restructuring for statistical machine\n",
      " translation\n",
      " . In\n",
      " Pr oceedings of the 43r d Annual\n",
      " Meeting of the Association for Computational Lin-\n",
      " guistics (A CL '05)\n",
      " , pages 531Œ540, Ann Arbor ,\n",
      " Michig an. Association for Computational Linguis-\n",
      " tics.\n",
      " Jonas Gehring, Michael Auli, Da vid Grangier , Denis\n",
      " Y arats, and Y ann N. Dauphin. 2017.\n",
      " Con v olutional\n",
      " sequence to sequence learni ng\n",
      " . In\n",
      " Pr oceedings\n",
      " of the 34th International Confer ence on Mac hine\n",
      " Learning\n",
      " , v olum e 70 of\n",
      " Pr oceedings of Mac hine\n",
      " Learning Resear c h\n",
      " , pages 1243Œ1252, International\n",
      " Con v ention Centre, Sydne y , Australia. PMLR.\n",
      " Emil Julius Gumbel. 1954. Statistical theory of e x-\n",
      " treme v aluse and some practical applications.\n",
      " Nat.\n",
      " Bur . Standar ds Appl. Math. Ser . 33\n",
      " .\n",
      " Chin-Y e w Lin. 2004. Rouge: A package for automatic\n",
      " e v aluation of summaries. In\n",
      " T e xt Summarization\n",
      " Br anc hes Out: Pr oceedings of the A CL-04 W ork-\n",
      " shop\n",
      " , pages 74Œ81, Barcelona, Spain. Association\n",
      " for Computational Linguistics.\n",
      " Chris J Maddison, Daniel T arlo w , and T om Minka.\n",
      " 2014.\n",
      " A* sampling\n",
      " . In Z. Ghahramani, M. W elling,\n",
      " C. Cortes, N. D. La wrence, and K. Q. W einber ger ,\n",
      " editors,\n",
      " Advances in Neur al Information Pr ocessing\n",
      " Systems 27\n",
      " , pages 3086Œ3094. Curran Associates,\n",
      " Inc.\n",
      " F andong Meng and Jinchao Zhang. 2019. Dtmt: A\n",
      " no v el deep transition architecture for neural ma-\n",
      " chine translation. In\n",
      " Pr oceedings of the Thirty-\n",
      " Thir d AAAI Confer ence on Intellig ence\n",
      " ,\n",
      " AAAI'19. AAAI Press.\n",
      " Kishore P apineni, Salim Rouk os, T odd W ard, and W ei-\n",
      " Jing Zhu. 2002. Bleu: a method for automatic e v al-\n",
      " uation of machine translation. In\n",
      " Pr oceedings of\n",
      " the 40th annual meeting on association for compu-\n",
      " tational linguistics\n",
      " , pages 311Œ318. Association for\n",
      " Computational Linguistics.\n",
      " Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli,\n",
      " and W ojciech Zaremba. 2015. Sequence le v el train-\n",
      " ing with recurrent neural netw orks.\n",
      " arXiv pr eprint\n",
      " arXiv:1511.06732\n",
      " .\n",
      " Rico Sennrich, Barry Haddo w , and Ale xandra Birch.\n",
      " 2016.\n",
      " Neural machine translation of rare w ords\n",
      " with subw ord units\n",
      " . In\n",
      " Pr oceedings of the 54th An-\n",
      " nual Meeting of the Ass ociation for Computational\n",
      " Linguistics (V olume 1: Long P aper s)\n",
      " , pages 1715Œ\n",
      " 1725, Berlin, German y . Association for Computa-\n",
      " tional Linguistics.\n",
      " Chenze Shao, Xilin Chen, and Y ang Feng. 2018.\n",
      " Greedy search with probabilistic n-gram matching\n",
      " for neural machine translation. In\n",
      " Pr oceedings of\n",
      " the 2018 Confer ence on Empirical Methods in Nat-\n",
      " ur al Langua g e Pr ocessing\n",
      " , pages 4778Œ4784.\n",
      " Shiqi Shen, Y ong Cheng, Zhongjun He, W ei He, Hua\n",
      " W u, Maosong Sun, and Y ang Liu. 2016. Minimum\n",
      " risk training for neural machine translation. In\n",
      " Pr o-\n",
      " ceedings of the 54th Annual Meeting of the Associa-\n",
      " tion for Computational Linguistics (V olume 1: Long\n",
      " P aper s)\n",
      " , v olume 1, pages 1683Œ1692.\n",
      " Ilya Sutsk e v er , Oriol V in yals, and Quoc V Le. 2014.\n",
      " Sequence to sequence learning with neural net-\n",
      " w orks\n",
      " . In Z. Ghahramani, M. W elling, C. Cortes,\n",
      " N. D. La wrence, and K. Q. W einber ger , editors,\n",
      " Ad-\n",
      " vances in Neur al Information Pr ocessing Systems\n",
      " 27\n",
      " , pages 3104Œ3112. Curran Associates, Inc.\n",
      " Zhaopeng T u, Zhengdong Lu, Y ang Liu, Xiaohua Liu,\n",
      " and Hang Li. 2016. Modeling co v erage for neural\n",
      " machine translation. In\n",
      " Pr oceedings of A CL\n",
      " .\n",
      " Ashish V asw ani, Noam Shazeer , Niki P armar , Jak ob\n",
      " Uszk oreit, Llion Jones, Aidan N Gomez, ukasz\n",
      " Kaiser , and Illia Polosukhin. 2017.\n",
      " Attention is all\n",
      " you need\n",
      " . In I. Guyon, U. V . Luxb ur g, S. Bengio,\n",
      " H. W allach, R. Fer gus, S. V ishw anathan, and R. Gar -\n",
      " nett, editors ,\n",
      " Advances in Neur al Information Pr o-\n",
      " cessing Systems 30\n",
      " , pages 5998Œ6008. Curran As-\n",
      " sociates, Inc.\n",
      " Arun V enkatraman, Mar tial Hebert, and J. Andre w\n",
      " Bagnell. 2015.\n",
      " Impro ving multi-step prediction of\n",
      " learned time series models\n",
      " . In\n",
      " Pr oceedings of the\n",
      " T wenty-Ninth AAAI Confer ence on Intelli-\n",
      " g ence\n",
      " , AAAI'15, pages 3024Œ3030. AAAI Press.\n",
      " Y onghui W u, Mik e Schuster , Zhifeng C h e n, Quoc V\n",
      " Le, Mohammad Norouzi, W olfg ang Machere y ,\n",
      " Maxim Krikun, Y uan Cao, Qin Gao, Klaus\n",
      " Machere y , et al. 2016. Google' s neural ma-\n",
      " chine translation system: Bridging the g ap between\n",
      " human and machine translation.\n",
      " arXiv pr eprint\n",
      " arXiv:1609.08144\n",
      " .\n",
      " Matthe w D Zeiler . 2012. Adadelta: an adapti v e learn-\n",
      " ing rate method.\n",
      " arXiv pr eprint arXiv:1212.5701\n",
      " .\n",
      " Jinchao Zhang, Mi ngxu a n W ang, Qun Liu, and Jie\n",
      " Zhou. 2017. Incorporating w ord reordering kno wl-\n",
      " edge into atte n t ion-based neural machine transla-\n",
      " tion. In\n",
      " Pr oceedings of A CL\n",
      " .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the modules using a pdf file\n",
    "#print(loadPdf(\"Bridging_The_Gap_Between_Training_&_Inference_For_Neural_Machine_Translation.pdf\"))\n",
    "#print(getPdfText(\"Bridging_The_Gap_Between_Training_&_Inference_For_Neural_Machine_Translation.pdf\", 10))\n",
    "print(makeTxt2Pdf(\"Bridging_The_Gap_Between_Training_&_Inference_For_Neural_Machine_Translation.pdf\", 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the text file\n",
    "with open(\"my_file.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_data = text.replace('\\t','\\n').split('\\n')\n",
    "#replaced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to string\n",
    "string_data = \" \".join(map(str, replaced_data))\n",
    "#print(string_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numbers\n",
    "import re\n",
    "nonnumbered_text = re.sub(r'\\d+', '', string_data)\n",
    "#print(nonnumbered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = set(string.punctuation)\n",
    "tokenized_text = word_tokenize(nonnumbered_text)\n",
    "#print(tokenized_text)\n",
    "filtered_text = []\n",
    "for i in tokenized_text:\n",
    "    if i not in s:\n",
    "        filtered_text.append(i)\n",
    "#print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download(\"stopwords\")\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "stopped_text = [word.lower() for word in filtered_text if word.lower() not in stop]\n",
    "#print(stopped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nameparser.parser import HumanName\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_list = []\n",
    "person_names=person_list\n",
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #avoid grabbing lone surnames\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "#     print (person_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to string\n",
    "string_data1 = \" \".join(map(str, stopped_text))\n",
    "#print(string_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D A', 'Systems Ar', 'De T', 'Dzmitry Bahdanau', 'Samy Bengi', 'Noam Shazeer', 'Fethi Boug', 'Holger Schwenk', 'Ann Arbor', 'Jonas Gehring', 'Emil Julius Gumbel', 'Jinchao Zhang', 'Sumit Chopra', 'Barry Haddo', 'Chenze Shao', 'Shiqi Shen', 'Ilya Sutsk', 'Xiaohua Liu', 'Llion Jones', 'Illia Polosukhin', 'Qin Gao', 'Klaus Machere', 'Qun Liu']\n"
     ]
    }
   ],
   "source": [
    "names = get_human_names(string_data1)\n",
    "for person in person_list:\n",
    "    person_split = person.split(\" \")\n",
    "    for name in person_split:\n",
    "        if wordnet.synsets(name):\n",
    "            if(name in person):\n",
    "                person_names.remove(person)\n",
    "                break\n",
    "\n",
    "print(person_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "import nltk\n",
    "#nltk.download()\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in nltk.sent_tokenize(string_data1):\n",
    "      for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                  print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NERTagger' from 'nltk.tag.stanford' (C:\\Users\\Abs_Sayem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tag\\stanford.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-34d6059754dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstanford\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNERTagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNERTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stanford-ner/all.3class.distsim.crf.ser.gz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stanford-ner/stanford-ner.jar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#text = \"\"\"YOUR TEXT GOES HERE\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'NERTagger' from 'nltk.tag.stanford' (C:\\Users\\Abs_Sayem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tag\\stanford.py)"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import NERTagger\n",
    "st = NERTagger('stanford-ner/all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')\n",
    "#text = \"\"\"YOUR TEXT GOES HERE\"\"\"\n",
    "\n",
    "for sent in nltk.sent_tokenize(string_data1):\n",
    "    tokens = nltk.tokenize.word_tokenize(sent)\n",
    "    tags = st.tag(tokens)\n",
    "    for tag in tags:\n",
    "        if tag[1]=='PERSON':\n",
    "            print(tag)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ad1a662050c36948a62a2159e528a078f0bb94ec7adadaee9eccaf8b42424e4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
