{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Info from PDF\n",
    "*PDFs are a proprietory format of adobe and it is not easy when it comes to automating the process of extracting information from pdf file. Our aim is to extract required information from any pdf file.*\n",
    "\n",
    "*In this tutorial we will use some outstanding python libraries to do our work.*\n",
    "\n",
    "*> *****PyPDF2:***** to convert simple, text-based pdf files into text readable*\n",
    "\n",
    "*> *****textract:***** to convert non-trival, scanned pdf files into text readable*\n",
    "\n",
    "*> *****NLTK:***** to clean and convert phrases into keywords*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pypdf2 - \"pip install pypdf2\"\n",
    "# Install textract - \"pip install textract\"\n",
    "# Install NLTK - \"pip install nltk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyPDF2 and its modules\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pdf file\n",
    "# It will return an object\n",
    "def loadPdf(file_name):\n",
    "    #import PyPDF2\n",
    "    #from PyPDF2 import PdfFileReader\n",
    "    \n",
    "    my_file = PdfFileReader(f\"{file_name}\")\n",
    "    return(my_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Text from pdf\n",
    "def getPdfText(file_name, page_range):\n",
    "    #import PyPDF2\n",
    "    #from PyPDF2 import PdfFileReader\n",
    "    \n",
    "    my_file = PdfFileReader(f\"{file_name}\")\n",
    "    no_page = my_file.getNumPages()\n",
    "    if(page_range <= no_page):\n",
    "        page_num = page_range\n",
    "    else:\n",
    "        page_num = no_page\n",
    "    file_text = \"\"\n",
    "    for i in range(page_num):\n",
    "        file_text += my_file.getPage(i).extractText()\n",
    "    return(file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a text file from pdf file\n",
    "def makeTxt2Pdf(file_name, page_range):\n",
    "    #import PyPDF2\n",
    "    #from PyPDF2 import PdfFileReader\n",
    "    \n",
    "    my_file = PdfFileReader(f\"{file_name}\")\n",
    "    no_page = my_file.getNumPages()\n",
    "    if(page_range <= no_page):\n",
    "        page_num = page_range\n",
    "    else:\n",
    "        page_num = no_page\n",
    "    file_text = \"\"\n",
    "    for i in range(page_num):\n",
    "        file_text += my_file.getPage(i).extractText()\n",
    "    # making txt file\n",
    "    with open(\"text_file.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the modules using a pdf file\n",
    "#print(loadPdf(\"Bridging_The_Gap_Between_Training_&_Inference_For_Neural_Machine_Translation.pdf\"))\n",
    "text = getPdfText(\"Bridging_The_Gap_Between_Training_&_Inference_For_Neural_Machine_Translation.pdf\", 10)\n",
    "#print(makeTxt2Pdf(\"Bridging_The_Gap_Between_Training_&_Inference_For_Neural_Machine_Translation.pdf\", 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing\n",
    "*Now we will preprocess the whole text. Preprocess contains the steps-*\n",
    "\n",
    "*> *****Remove Spaces:***** removing the tabs or long white spaces*\n",
    "\n",
    "*> *****Remove Punctuation:***** removing all punctuations like- .,?:;'\" etc*\n",
    "\n",
    "*> *****Remove Stopwords:***** removing the words those aren't so essential for meaning. We will remove the default stopwords defined in nltk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = \"This is a text, which contains '$500'   as its context. Let's   have a look what happend!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "def preprocess(text):\n",
    "    # Removing spaces\n",
    "    import re\n",
    "    #non_spaced_text = text.replace('\\t','\\n').split('\\n')\n",
    "    non_spaced_text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    # Removing punctuation\n",
    "    #  Before that we need to convert the splited text to string\n",
    "    string_text = \"\".join(map(str, non_spaced_text))\n",
    "    # Remove punctuation\n",
    "    premier_text = re.sub(r'[^\\w\\s]', \"\", string_text)   # it will replace non-alphanumeric characters or whitespae\n",
    "    \n",
    "    import string\n",
    "    punc_set = set(string.punctuation)\n",
    "    punctuated_text = []\n",
    "    for i in premier_text:\n",
    "        if i not in punc_set:\n",
    "            punctuated_text.append(i)\n",
    "    middle_string_text = \"\".join(map(str, punctuated_text))\n",
    "    \n",
    "    # Remove Stopwords\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_set = set(stopwords.words('english'))\n",
    "    stopped_text = [word.lower() for word in middle_string_text.split() if word.lower() not in stop_set]\n",
    "\n",
    "    # Convert to string\n",
    "    final_string_text = \" \".join(stopped_text)\n",
    "    \n",
    "    return(final_string_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = preprocess(text)\n",
    "#print(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading the file, now our target is to extract any information from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 1 2 3 4 5 1 2 3 4 5 1 2014 2015 2019 2017 2017 2015 2015 17 2019 2 2015 1 1 2014 1 1 1 2 1 3 0 1 0 4 1 5 1 1 6 1 7 8 9 3 1 1 1 1 1 2 1 1 1 3 1 1 6 7 1 31 1 1 1 2002 2 2016 2004 1 1 9 2 1 1954 2014 1 8 3 1 10 1 1 11 1 1 12 0 1 0 1 1 1 1 1 1 13 3 1 1 1 1 14 6 1 1 1 1 32 1 1 1 1 1 1 1 1 2015 0 15 33 1 6 7 8 9 1 1 16 4 2015 2015 2015 2016 2018 5 51 1 2002 878 2003 2004 2005 2006 919 1788 1082 1664 2 3003 2737 2002 2005 30 97 7 99 3 2016 37 39418 40274 50 128 512 10 0 1 0 1 80 2012 0 95 1 6 0 5 1 2 2016 33 69 38 05 35 01 34 83 35 40 2016 37 41 39 87 37 45 36 80 37 88 2017 37 93 40 40 36 81 35 77 37 73 37 93 40 53 36 65 35 80 37 73 38 82 41 68 37 28 37 98 38 94 38 70 40 81 37 59 38 38 38 87 4040 4263 3887 3844 4009 46 89 47 88 47 40 46 66 47 21 47 42 48 34 47 89 47 34 47 75 4831 4940 4872 4845 4872 1 001 3 52 2 6 1 1 1 17 2 18 3 1 1 2015 15 2015 1 3 0 5 11 12 15 3 53 1 2016 2016 2017 1 2016 2 2016 2017 2015 2015 1 2 36 37 73 38 94 39 50 39 56 4009 2 06 1 2 1 0 54 1 0 54 2 1 21 0 62 0 56 0 53 4 5 55 4 5 7 7 4 5 6 6 4 0 5 56 7 7 10 20 40 50 70 80 57 1 4 5 6 7 24 87 2 18 28 266 18 391 18 391 28 266 65 06 58 25 82 26 50 26 76 2741 27 34 28 05 27 98 2865 3 001 3 1 59 1 31 6 61662077 61876174 2015 2015 2015 28 2014 2014 1734 2005 05 2017 70 1954 33 2004 2014 27 2019 2002 2015 2016 1 1725 2018 2018 2016 1 1 2014 27 2016 2017 30 2015 2016 2012 2017\n"
     ]
    }
   ],
   "source": [
    "# Get any number\n",
    "def getNumber(text):\n",
    "    all_numbers = []\n",
    "    for num in processed_text.split():\n",
    "        if(num.isdigit()):\n",
    "            all_numbers.append(num)\n",
    "    numbers = \" \".join(all_numbers)\n",
    "    print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Price(amount=Decimal('12'), currency='eur')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find price value\n",
    "# we will use price-parser to get price values. to install it use - \"pip install price-parser\"\n",
    "from price_parser import Price\n",
    "price = Price.fromstring(processed_text)\n",
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nameparser.parser import HumanName\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_list = []\n",
    "person_names=person_list\n",
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #avoid grabbing lone surnames\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "#     print (person_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "names = get_human_names(processed_text)\n",
    "for person in person_list:\n",
    "    person_split = person.split(\" \")\n",
    "    for name in person_split:\n",
    "        if wordnet.synsets(name):\n",
    "            if(name in person):\n",
    "                person_names.remove(person)\n",
    "                break\n",
    "\n",
    "\n",
    "print(person_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "import nltk\n",
    "#nltk.download()\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in nltk.sent_tokenize(string_data1):\n",
    "      for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                  print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NERTagger' from 'nltk.tag.stanford' (C:\\Users\\Abs_Sayem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tag\\stanford.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-34d6059754dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstanford\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNERTagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNERTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stanford-ner/all.3class.distsim.crf.ser.gz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stanford-ner/stanford-ner.jar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#text = \"\"\"YOUR TEXT GOES HERE\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'NERTagger' from 'nltk.tag.stanford' (C:\\Users\\Abs_Sayem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tag\\stanford.py)"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import NERTagger\n",
    "st = NERTagger('stanford-ner/all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')\n",
    "#text = \"\"\"YOUR TEXT GOES HERE\"\"\"\n",
    "\n",
    "for sent in nltk.sent_tokenize(string_data1):\n",
    "    tokens = nltk.tokenize.word_tokenize(sent)\n",
    "    tags = st.tag(tokens)\n",
    "    for tag in tags:\n",
    "        if tag[1]=='PERSON':\n",
    "            print(tag)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ad1a662050c36948a62a2159e528a078f0bb94ec7adadaee9eccaf8b42424e4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
