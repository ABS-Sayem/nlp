{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Info from PDF\n",
    "*PDFs are a proprietory format of adobe and it is not easy when it comes to automating the process of extracting information from pdf file. Our aim is to extract required information from any pdf file.*\n",
    "\n",
    "*In this tutorial we will use some outstanding python libraries to do our work.*\n",
    "\n",
    "*> *****PyPDF2:***** to convert simple, text-based pdf files into text readable*\n",
    "\n",
    "*> *****textract:***** to convert non-trival, scanned pdf files into text readable*\n",
    "\n",
    "*> *****NLTK:***** to clean and convert phrases into keywords*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pypdf2 - \"pip install pypdf2\"\n",
    "# Install textract - \"pip install textract\"\n",
    "# Install NLTK - \"pip install nltk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyPDF2 and its modules\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pdf file\n",
    "# It will return an object\n",
    "def loadPdf(file_name):\n",
    "    #import PyPDF2\n",
    "    #from PyPDF2 import PdfFileReader\n",
    "    \n",
    "    my_file = PdfFileReader(f\"{file_name}\")\n",
    "    return(my_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Text from pdf\n",
    "def getPdfText(file_name, page_range):\n",
    "    #import PyPDF2\n",
    "    #from PyPDF2 import PdfFileReader\n",
    "    \n",
    "    my_file = PdfFileReader(f\"{file_name}\")\n",
    "    no_page = my_file.getNumPages()\n",
    "    if(page_range <= no_page):\n",
    "        page_num = page_range\n",
    "    else:\n",
    "        page_num = no_page\n",
    "    file_text = \"\"\n",
    "    for i in range(page_num):\n",
    "        file_text += my_file.getPage(i).extractText()\n",
    "    return(file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a text file from pdf file\n",
    "def makeTxt2Pdf(file_name, page_range):\n",
    "    #import PyPDF2\n",
    "    #from PyPDF2 import PdfFileReader\n",
    "    \n",
    "    my_file = PdfFileReader(f\"{file_name}\")\n",
    "    no_page = my_file.getNumPages()\n",
    "    if(page_range <= no_page):\n",
    "        page_num = page_range\n",
    "    else:\n",
    "        page_num = no_page\n",
    "    file_text = \"\"\n",
    "    for i in range(page_num):\n",
    "        file_text += my_file.getPage(i).extractText()\n",
    "    # making txt file\n",
    "    with open(\"text_file.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the modules using a pdf file\n",
    "#print(loadPdf(\"Bridging_The_Gap_Between_Training_&_Inference_For_Neural_Machine_Translation.pdf\"))\n",
    "text = getPdfText(\"Bridging_The_Gap_Between_Training_&_Inference_For_Neural_Machine_Translation.pdf\", 10)\n",
    "#print(makeTxt2Pdf(\"Bridging_The_Gap_Between_Training_&_Inference_For_Neural_Machine_Translation.pdf\", 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing\n",
    "*Now we will preprocess the whole text.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Remove numbers: to remove numbers we need to convert the object data to string data.\\nstring_text = \" \".join(map(str, non_spaced_text))\\n# Remove numbers\\nimport re\\nnon_numbered_text = re.sub(r\\'\\\\d+\\', \\'\\', string_text)\\n#print(non_numbered_text)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove long spaces\n",
    "non_spaced_text = text.replace('\\t','\\n').split('\\n')\n",
    "#print(non_spaced_text)\n",
    "'''\n",
    "# Remove numbers: to remove numbers we need to convert the object data to string data.\n",
    "string_text = \" \".join(map(str, non_spaced_text))\n",
    "# Remove numbers\n",
    "import re\n",
    "non_numbered_text = re.sub(r'\\d+', '', string_text)\n",
    "#print(non_numbered_text)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Punctuation: Before that we need to convert object data to string data\n",
    "# Convert to string\n",
    "string_text = \" \".join(map(str, non_spaced_text))\n",
    "# Remove punctuation\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = set(string.punctuation)\n",
    "tokenized_text = word_tokenize(string_text)\n",
    "#print(tokenized_text)\n",
    "filtered_text = []\n",
    "for i in tokenized_text:\n",
    "    if i not in s:\n",
    "        filtered_text.append(i)\n",
    "#filtered_text1 = \" \".join(filtered_text)\n",
    "#print(filtered_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download(\"stopwords\")\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "stopped_text = [word.lower() for word in filtered_text if word.lower() not in stop]\n",
    "#print(stopped_text)\n",
    "stopped_text1 = \" \".join(stopped_text)\n",
    "#print(stopped_text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading the file, now our target is to extract any information from the text. For this we wil use Regular Expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nameparser.parser import HumanName\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_list = []\n",
    "person_names=person_list\n",
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #avoid grabbing lone surnames\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "#     print (person_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "names = get_human_names(stopped_text1)\n",
    "for person in person_list:\n",
    "    person_split = person.split(\" \")\n",
    "    for name in person_split:\n",
    "        if wordnet.synsets(name):\n",
    "            if(name in person):\n",
    "                person_names.remove(person)\n",
    "                break\n",
    "\n",
    "print(person_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "import nltk\n",
    "#nltk.download()\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in nltk.sent_tokenize(string_data1):\n",
    "      for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                  print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NERTagger' from 'nltk.tag.stanford' (C:\\Users\\Abs_Sayem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tag\\stanford.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-34d6059754dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstanford\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNERTagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNERTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stanford-ner/all.3class.distsim.crf.ser.gz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stanford-ner/stanford-ner.jar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#text = \"\"\"YOUR TEXT GOES HERE\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'NERTagger' from 'nltk.tag.stanford' (C:\\Users\\Abs_Sayem\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tag\\stanford.py)"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import NERTagger\n",
    "st = NERTagger('stanford-ner/all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')\n",
    "#text = \"\"\"YOUR TEXT GOES HERE\"\"\"\n",
    "\n",
    "for sent in nltk.sent_tokenize(string_data1):\n",
    "    tokens = nltk.tokenize.word_tokenize(sent)\n",
    "    tags = st.tag(tokens)\n",
    "    for tag in tags:\n",
    "        if tag[1]=='PERSON':\n",
    "            print(tag)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ad1a662050c36948a62a2159e528a078f0bb94ec7adadaee9eccaf8b42424e4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
