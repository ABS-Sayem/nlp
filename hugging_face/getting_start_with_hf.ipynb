{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transformers, Pipeline, Tokenizer, Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transformers**\n",
    "**Install Transformers**\n",
    "* `pip install transformers` will install the huggingface transformers library.\n",
    "* We can use the transformers library on-top-of `PyTorch` or `Tensorflow`. So, we need to install any of the two first.\n",
    "* `pip install tensorflow` for tensorflow and `pip install torch` for pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Pipeline**\n",
    "**`What a pipeline do?`:** A pipeline basically do three things:\n",
    "* `preprocess` text (in this case- by applying a tokenizer)\n",
    "* `fit the text to model`\n",
    "* `postprocessing` the output (in this case- show us the sentiment and the score)\n",
    "> The things can be different for different tasks. For more about transformer pipeline check [here](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example-1: Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9715973138809204}]\n"
     ]
    }
   ],
   "source": [
    "# Import pipeline\n",
    "# huggingface has pipelines for various tasks. You can check here. (https://huggingface.co/models)\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create an object: we need to create an object for a task. We won't choice any model, so it will choice a default model on-behalf.\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Obtain sentiment using the classifier\n",
    "sentiment = classifier(\"I want to learn transformers in-depth\")\n",
    "\n",
    "# See the sentiment\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example-2: Text Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abs_Sayem\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\generation\\utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The text generator libaray of huggingface consists of the following options:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThese are the options you just need to define yourself. Each option you choose creates an interesting dynamic layer:\\n\\n\\n'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Define the generator: we will choice a model here with the task object.\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "# Define the generator parameters and generate text\n",
    "sentance_portion = \"The text generator libaray of huggingface consists of\"  # We will complete sentence using the generator\n",
    "generated_sentence = generator(sentance_portion, \n",
    "                               max_length=50,   # default 20\n",
    "                               num_return_sequences=1)\n",
    "# We can modify the generation parameters. See here (https://huggingface.co/docs/transformers/main_classes/text_generation) for details.\n",
    "\n",
    "# See the generated sentence\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example-3: Zero Shot Classification** Here, we will classify a sentence with given classes, ie- will provide the classes too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'I am happy because I have been learning.', 'labels': ['Education', 'Travel', 'Politics'], 'scores': [0.48755040764808655, 0.3468903601169586, 0.16555924713611603]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Define the classifier\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "# Obtain class of a sentence using the classifier\n",
    "appox_class = classifier(\n",
    "    \"I am happy because I have been learning.\",\n",
    "    candidate_labels = [\"Education\", \"Politics\", \"Travel\"]\n",
    ")\n",
    "\n",
    "# See the result\n",
    "print(appox_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`For more about transformers pipeline, check` [here](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tokenizer**\n",
    "\n",
    "Let's deep dive into the pipeline and try to understand the mechanism inside a pipeline. Here, we will see the tokenization and model details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.8353028297424316}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification      # Generic Class - General purpose\n",
    "from transformers import BertTokenizer, BertModel       # Specific Class - Specialized purpose\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "# Because we don't specify any model, it will use 'distilbert-base-uncased-finetuned-sst-2-english' model by-default.\n",
    "\n",
    "sentiment = classifier(\"A pipeline basically perform three things.\")\n",
    "\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`We can specify the model and tokenizer name explicitly.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.8353028297424316}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification      # Generic Class - General purpose\n",
    "from transformers import BertTokenizer, BertModel       # Specific Class - Specialized purpose\n",
    "\n",
    "# Specify the Tokenizer and Model\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Specify the tokenizer and model to the classifier\n",
    "classifier = pipeline(\"sentiment-analysis\", tokenizer=tokenizer, model=model)\n",
    "\n",
    "sentiment = classifier(\"A pipeline basically perform three things.\")\n",
    "\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tokenizer In-Depth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1037, 13117, 10468, 4685, 2093, 2477, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Select the model and tokenizer\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize a Text\n",
    "sentence = \"A pipeline basically perform three things.\"\n",
    "tokens = tokenizer(sentence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Here, we see a list of `token_ids` and `attention_mask`. `Token_Ids` are the represented form of the tokens (words), and `Attention_Mask` are the identification of which token attention layer should focused on or ignore.**\n",
    "###### **We can see the further details of a tokenizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['a', 'pipeline', 'basically', 'perform', 'three', 'things', '.']\n",
      "Ids   : [1037, 13117, 10468, 4685, 2093, 2477, 1012]\n",
      "String: a pipeline basically perform three things.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Select the model and tokenizer\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# A sentence string\n",
    "sentence = \"A pipeline basically perform three things.\"\n",
    "\n",
    "# Tokens\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "# Ids\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"Ids   : {ids}\")\n",
    "# Back to String\n",
    "string = tokenizer.decode(ids)\n",
    "print(f\"String: {string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine all the above with PyTorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Classifier in Default Setting---------\n",
      "Result: [{'label': 'POSITIVE', 'score': 0.9951896667480469}, {'label': 'NEGATIVE', 'score': 0.5991277694702148}]\n",
      "\n",
      "Using Classifier in Customized Setting-------------\n",
      "Tokens: {'input_ids': tensor([[  101,  2023,  2944,  2003,  1037,  2986,  1011,  8694, 26520,  1997,\n",
      "          4487, 16643, 23373,  1011,  2918,  1011,  4895, 28969,  1010,  2986,\n",
      "          1011, 15757,  2006,  7020,  2102,  1011,  1016,  1012,   102],\n",
      "        [  101,  2023,  2944,  6561,  2019, 10640,  1997,  6205,  1012,  1017,\n",
      "          2006,  1996, 16475,  2275,  1012,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]])}\n",
      "Output: SequenceClassifierOutput(loss=None, logits=tensor([[-2.6458,  2.6863],\n",
      "        [ 0.2928, -0.1090]]), hidden_states=None, attentions=None)\n",
      "Pred Results: tensor([[0.0048, 0.9952],\n",
      "        [0.5991, 0.4009]])\n",
      "Labels: tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as fnn\n",
    "\n",
    "# Specify the Tokenizer and Model\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Specify the tokenizer and model to the classifier\n",
    "classifier = pipeline(\"sentiment-analysis\", tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Train Text\n",
    "train_text = [\"This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2.\", \n",
    "              \"This model reaches an accuracy of 91.3 on the dev set.\"]\n",
    "\n",
    "# Predict Sentiments with Classifier Pipeline (Default Setting)\n",
    "print('Using Classifier in Default Setting---------')\n",
    "result = classifier(train_text)\n",
    "print(f\"Result: {result}\")\n",
    "\n",
    "\n",
    "# Predict Sentiments with Classifier Pipeline (Customized Setting)\n",
    "print('\\nUsing Classifier in Customized Setting-------------')\n",
    "\n",
    "# Tokens\n",
    "tokens = tokenizer(train_text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Define Infer\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)        # Unpacked the dictionary (of tokens)\n",
    "    print(f\"Output: {outputs}\")\n",
    "    pred_results = fnn.softmax(outputs.logits, dim=1)        # Predict the probability\n",
    "    print(f\"Pred Results: {pred_results}\")\n",
    "    labels = torch.argmax(pred_results, dim=1)      # Get the labels\n",
    "    print(f\"Labels: {labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine all the above with Tensorflow** `[Will be done later]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow\n",
    "#import tensorflow.\n",
    "\n",
    "# # Specify the Tokenizer and Model\n",
    "# model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# # Specify the tokenizer and model to the classifier\n",
    "# classifier = pipeline(\"sentiment-analysis\", tokenizer=tokenizer, model=model)\n",
    "\n",
    "# # Train Text\n",
    "# train_text = [\"This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2.\", \n",
    "#               \"This model reaches an accuracy of 91.3 on the dev set.\"]\n",
    "\n",
    "# # Predict Sentiments with Classifier Pipeline (Default Setting)\n",
    "# print('Using Classifier in Default Setting---------')\n",
    "# result = classifier(train_text)\n",
    "# print(result)\n",
    "\n",
    "\n",
    "# # Predict Sentiments with Classifier Pipeline (Customized Setting)\n",
    "# tokens = tokenizer(train_text, padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
    "\n",
    "# # Define Infer\n",
    "# print('\\nUsing Classifier in Customized Setting-------------')\n",
    "# with torch.no_grad():\n",
    "#     output = model(**tokens)        # Unpacked the dictionary (of tokens)\n",
    "#     print(f\"Output: {output}\")\n",
    "#     pred_results = fnn.softmax(output.logits, dim=1)    # Predict the probability\n",
    "#     print(f\"Pred Results: {pred_results}\")\n",
    "#     labels = torch.argmax(pred_results, dim=1)      # Get the labels\n",
    "#     print(f\"Labels: {labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **FineTune**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
